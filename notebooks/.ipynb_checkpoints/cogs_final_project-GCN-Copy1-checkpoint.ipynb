{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch_geometric\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.insert(0, '../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import json\n",
    "import argparse\n",
    "# import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Function\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from torchvision.datasets import CocoDetection\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.insert(0, '../src/data/cocoapi/PythonAPI')\n",
    "from pycocotools.coco import COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/jlevy44/WSI-GTFE/blob/master/notebooks/3_fit_gnn_model.ipynb\n",
    "import torch, torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv, GATConv, DeepGraphInfomax, SAGEConv\n",
    "from torch_geometric.nn import DenseGraphConv\n",
    "from torch_geometric.utils import to_dense_batch, to_dense_adj, dense_to_sparse\n",
    "from torch_geometric.nn import GINEConv\n",
    "from torch_geometric.utils import dropout_adj\n",
    "\n",
    "\n",
    "class GCNNet(torch.nn.Module):\n",
    "    def __init__(self, inp_dim, out_dim, hidden_topology=[32,64,128,128], p=0.2, p2=0.0, drop_each=True):\n",
    "        super(GCNNet, self).__init__()\n",
    "        self.out_dim=out_dim\n",
    "        self.convs = nn.ModuleList([GATConv(inp_dim, hidden_topology[0])]+[GATConv(hidden_topology[i],hidden_topology[i+1]) for i in range(len(hidden_topology[:-1]))])\n",
    "        self.drop_edge = lambda edge_index: dropout_adj(edge_index,p=p2)[0]\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.fc = nn.Linear(hidden_topology[-1], out_dim)\n",
    "        self.drop_each=drop_each\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr=None, return_attention=False):\n",
    "        attention_weights=[]\n",
    "        for conv in self.convs:\n",
    "            if self.drop_each and self.training: edge_index=self.drop_edge(edge_index)\n",
    "            x, attention = conv(x, edge_index, edge_attr,return_attention_weights=True)\n",
    "            x = F.relu(x)\n",
    "            attention_weights.append(attention)\n",
    "        if self.training:\n",
    "            x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        if return_attention: return x, attention_weights\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCNNet(\n",
       "  (convs): ModuleList(\n",
       "    (0): GATConv(2048, 32, heads=1)\n",
       "    (1): GATConv(32, 32, heads=1)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (fc): Linear(in_features=32, out_features=80, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gcn.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (0): Linear(in_features=2048, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (1): Linear(in_features=32, out_features=80, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! jupyter nbextension enable --py widgetsnbextension\n",
    "# ! python -c \"from torchvision import models; model = models.resnet50(pretrained=True)\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! git clone https://github.com/cocodataset/cocoapi.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class cocoDataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, root, annotation, transforms=None):\n",
    "#         self.root = root\n",
    "#         self.transforms = transforms\n",
    "#         self.coco = COCO(annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://medium.com/fullstackai/how-to-train-an-object-detector-with-your-own-coco-dataset-in-pytorch-319e7090da5\n",
    "class cocoDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, annotation, transforms=None, target_transform=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        self.coco = COCO(annotation)\n",
    "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Coco File\n",
    "        coco = self.coco\n",
    "        # Image ID\n",
    "        img_id = self.ids[index]\n",
    "        # List: get annotation id for img from coco\n",
    "        ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "        # Dictionary: coco annotations for an image\n",
    "        coco_annotation = coco.loadAnns(ann_ids)\n",
    "        # path for input image\n",
    "        path = coco.loadImgs(img_id)[0]['file_name']\n",
    "        # open the input image with PIL\n",
    "        img = Image.open(os.path.join(self.root, path)).convert(\"RGB\")\n",
    "        \n",
    "        # number of objects in the image\n",
    "        num_objs = len(coco_annotation)\n",
    "        \n",
    "        # Extracting bounding boxes for objects\n",
    "        # In coco format, bbox = [xmin, ymin, xmax, ymax]\n",
    "        # In pytorch, input should be [xmin, ymin, xmax, ymax]\n",
    "        boxes = []\n",
    "        # Labels for each objet\n",
    "        labels = []\n",
    "        # Area of each bounding box\n",
    "        areas = []\n",
    "        # IsCrowd: whether or not the object is a crowd of objects\n",
    "        iscrowd = []\n",
    "        xy=[]\n",
    "        subimages=[]\n",
    "        for i in range(num_objs):\n",
    "            xmin = coco_annotation[i]['bbox'][0]\n",
    "            ymin = coco_annotation[i]['bbox'][1]\n",
    "            xmax = xmin + coco_annotation[i]['bbox'][2]\n",
    "            ymax = ymin + coco_annotation[i]['bbox'][3]\n",
    "            bbox=np.array([xmin, ymin, xmax, ymax]).astype(int)\n",
    "            boxes.append(bbox)\n",
    "            labels.append(coco_annotation[i]['category_id'])\n",
    "            areas.append(coco_annotation[i]['area'])\n",
    "            iscrowd.append(coco_annotation[i]['iscrowd'])\n",
    "            xy.append([xmin+(xmax-xmin)/2,ymin+(ymax-ymin)/2])\n",
    "            subimages.append(img.crop(bbox))\n",
    "            \n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        areas = torch.as_tensor(areas, dtype=torch.float32)\n",
    "        iscrowd = torch.as_tensor(iscrowd, dtype=torch.float32)\n",
    "        \n",
    "        # Annotation is in dictionary format\n",
    "        my_annotation = {}\n",
    "        my_annotation['boxes'] = boxes\n",
    "        my_annotation['labels'] = labels\n",
    "        my_annotation['image_id'] = img_id\n",
    "        my_annotation['area'] = areas\n",
    "        my_annotation['iscrowd'] = iscrowd\n",
    "        \n",
    "        imgs=[img]\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            imgs = [self.transforms(img) for img in subimages]\n",
    "        if self.target_transform is not None:\n",
    "            labels = [self.target_transform(label) for label in labels]\n",
    "#         if self.\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "            \n",
    "        imgs=torch.stack(imgs)\n",
    "        \n",
    "        xy=torch.tensor(xy)\n",
    "            \n",
    "        return imgs, xy, torch.LongTensor([index]*len(labels)), torch.LongTensor(labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: add target transform to convert labels to proper labels using id_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(annFile):\n",
    "    # import json\n",
    "    with open(annFile, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    coco = COCO(annFile)\n",
    "    # _, data['categories'] = fix_ids(data)\n",
    "    return coco, data\n",
    "\n",
    "def fix_ids(data):\n",
    "    '''\n",
    "    Takes in 'categories' key of a COCO dataset, returns new IDs for those categories (properly mapped from 0 to len(categories)-1)\n",
    "    '''\n",
    "    categories = data['categories']\n",
    "    id_dict = {}\n",
    "    for idx, cat in enumerate(categories):\n",
    "        id_dict[cat['id']] = idx\n",
    "        cat['id'] = idx\n",
    "    return id_dict, categories\n",
    "\n",
    "def drop_null_annotations(coco, data, dataDir, dataType, annFile, overwrite, map_ids=True, save=True, tmpDataDir='data/temp'):\n",
    "    \"\"\"\n",
    "    Takes in a json.load(f) of an annotation file, finds all image ids without an annotation, then drops those images from the file.\n",
    "    Writes to a new json file without those images.\n",
    "    \"\"\"\n",
    "    # Writes new data to a cleaned json instances file\n",
    "    fname = tmpDataDir+f\"/annotations/clean_instances_{dataType}.json\"\n",
    "    if os.path.isfile(fname) and overwrite==False: print(fname, 'already exists')\n",
    "    elif os.path.isfile(fname) and overwrite==True: print(f'{fname} already exists, overwriting anyways b/c overwrite=True')\n",
    "        # if map_ids:\n",
    "        #     new_data = data.copy()\n",
    "        #     id_dict = {}\n",
    "        #     for i, cat in enumerate(new_data['categories']):\n",
    "        #         id_dict[cat['id']] = i\n",
    "        #         cat['id'] = i\n",
    "\n",
    "        # return id_dict\n",
    "\n",
    "\n",
    "    images_pd = pd.Series(data['images'])\n",
    "    new_images_pd = images_pd.copy()\n",
    "    new_data = data.copy()\n",
    "    new_data['images'] = \\\n",
    "        new_images_pd.loc[~images_pd.apply(lambda x: len(coco.getAnnIds(x['id']))==0)].tolist()\n",
    "    # sets new_data['images'] to only the list of images with one or more annotations\n",
    "#     if os.path.isdir(tmpDataF+\"/annotations\") == False: os.mkdir(dataDir+\"/annotations\")\n",
    "    if map_ids: id_dict, _ = fix_ids(new_data)\n",
    "    print(os.getcwd())\n",
    "\n",
    "    # print(fname)\n",
    "    if save:\n",
    "        try:\n",
    "            open(fname, 'w')\n",
    "        except FileNotFoundError:\n",
    "            os.mkdir(tmpDataDir+'/annotations')\n",
    "            print('annotation directory created')\n",
    "        with open(fname, 'w') as f:\n",
    "            json.dump(new_data, f)\n",
    "    else:\n",
    "        print(\"file saving skipped\")\n",
    "    print(\"Start images:\", len(data['images']))\n",
    "    print(\"Images remaining:\",len(new_data['images']))\n",
    "    print(\"Number of images with no annotations:\",len(data['images'])-len(new_data['images']))\n",
    "    if map_ids: return id_dict\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_transform():\n",
    "    custom_transforms = []\n",
    "    custom_transforms.append(torchvision.transforms.Resize(size=(128, 128)))\n",
    "    custom_transforms.append(torchvision.transforms.ToTensor())\n",
    "    return torchvision.transforms.Compose(custom_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.listdir('/datasets/COCO-2017/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = '/datasets/COCO-2017/train2017'\n",
    "val_dir = '/datasets/COCO-2017/val2017'\n",
    "\n",
    "train_ann = '/datasets/COCO-2017/anno2017/instances_train2017.json'\n",
    "val_ann = '/datasets/COCO-2017/anno2017/instances_val2017.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.63s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "val_coco, val_data = load_data(val_ann)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jdlevy/COGS_185/cogs_185_final_project/notebooks'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/temp/annotations/clean_instances_val2017.json already exists\n",
      "/home/jdlevy/COGS_185/cogs_185_final_project/notebooks\n",
      "Start images: 5000\n",
      "Images remaining: 4952\n",
      "Number of images with no annotations: 48\n"
     ]
    }
   ],
   "source": [
    "id_dict = drop_null_annotations(coco=val_coco, data=val_data, dataDir='/datasets/COCO-2017', dataType='val2017',\n",
    "                      annFile=val_ann, overwrite=False, tmpDataDir='../data/temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "del val_coco, val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_ids(target):\n",
    "    return id_dict[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %%capture\n",
    "# train_coco, train_data = load_data(train_ann)\n",
    "# %time drop_null_annotations(coco=train_coco, data=train_data, dataDir='/datasets/COCO-2017', dataType='train2017',\\\n",
    "#                             annFile=train_ann, overwrite=True, tmpDataDir='../data/temp');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del train_coco\n",
    "# del train_data\n",
    "\n",
    "# del val_coco\n",
    "# del val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train_ann = '../data/temp/annotations/clean_instances_train2017.json'\n",
    "clean_val_ann = '../data/temp/annotations/clean_instances_val2017.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=17.53s)\n",
      "creating index...\n",
      "index created!\n",
      "CPU times: user 16.4 s, sys: 2.38 s, total: 18.8 s\n",
      "Wall time: 18.7 s\n",
      "loading annotations into memory...\n",
      "Done (t=0.57s)\n",
      "creating index...\n",
      "index created!\n",
      "CPU times: user 532 ms, sys: 91.3 ms, total: 623 ms\n",
      "Wall time: 621 ms\n"
     ]
    }
   ],
   "source": [
    "%time train_set = cocoDataset(root=train_dir,\\\n",
    "                      annotation=clean_train_ann,\\\n",
    "                      transforms=get_transform(),\\\n",
    "                      target_transform=convert_ids)\n",
    "\n",
    "%time val_set = cocoDataset(root=val_dir,\\\n",
    "                      annotation=clean_val_ann,\\\n",
    "                      transforms=get_transform(),\\\n",
    "                      target_transform=convert_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    imgs=torch.cat([item[0] for item in batch],dim=0)\n",
    "    xy=torch.cat([item[1] for item in batch],dim=0)\n",
    "    idx=torch.cat([item[2] for item in batch]).flatten()\n",
    "    y=torch.cat([item[3] for item in batch]).flatten()\n",
    "    return [imgs,xy,idx,y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Params cell\n",
    "num_epoch = 10\n",
    "fname = f'graph_80_{num_epoch}_resnet50'\n",
    "train_batch_size = 12\n",
    "k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_set,\n",
    "                                          batch_size=train_batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=8,\n",
    "                                          collate_fn=collate_fn)\n",
    "\n",
    "valloader = torch.utils.data.DataLoader(val_set,\n",
    "                                        batch_size=train_batch_size,\n",
    "                                        shuffle=True,\n",
    "                                        num_workers=8,\n",
    "                                        collate_fn=collate_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del train_set, val_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our model for GCN embeddings\n",
    "model = models.resnet50(pretrained=False)\n",
    "state_dict=torch.load(\"../src/models/resnet50-19c8e357.pth\")\n",
    "model.load_state_dict(state_dict)\n",
    "model.fc = nn.Linear(2048, 80)\n",
    "model.fc=nn.Flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "import tqdm\n",
    "from torch_cluster import knn_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9773it [12:38, 12.89it/s]                            \n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import Data\n",
    "import tqdm\n",
    "from torch_cluster import knn_graph\n",
    "\n",
    "graphs=[]\n",
    "Idx=[]\n",
    "Z=[]\n",
    "XY=[]\n",
    "Y=[]\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for i,(imgs,xy,idx,y) in tqdm.tqdm(enumerate(trainloader),total=len(trainloader.dataset)//trainloader.batch_size):\n",
    "#         print(imgs.shape,xy.shape,y.shape)\n",
    "#         edge_index=knn_graph(xy,k=5)\n",
    "        imgs=imgs.cuda()\n",
    "        z=model(imgs).cpu()\n",
    "        Z.append(z)\n",
    "        XY.append(xy)\n",
    "        Y.append(y)\n",
    "        Idx.append(idx)\n",
    "        \n",
    "#         graphs.append(Data(x=z,pos=xy,edge_index=edge_index,y=y))\n",
    "    #     print(graphs[i].x.shape)\n",
    "#         if i==100: break\n",
    "        del imgs, z, xy, y, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "del trainloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z=torch.cat(Z,0)\n",
    "XY=torch.cat(XY,0)\n",
    "Y=torch.cat(Y).flatten()\n",
    "Idx=torch.cat(Idx).flatten().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 117266/117266 [16:43<00:00, 116.80it/s]\n"
     ]
    }
   ],
   "source": [
    "train_graphs=[]\n",
    "\n",
    "for idx in tqdm.tqdm(np.unique(Idx)):\n",
    "    select=(Idx==idx)\n",
    "    z=Z[select]\n",
    "    xy=XY[select]\n",
    "    y=Y[select]\n",
    "    edge_index=knn_graph(xy,k=k)\n",
    "    train_graphs.append(Data(x=z,pos=xy,edge_index=edge_index,y=y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "413it [00:34, 12.04it/s]                         \n"
     ]
    }
   ],
   "source": [
    "val_graphs=[]\n",
    "Idx=[]\n",
    "Z=[]\n",
    "XY=[]\n",
    "Y=[]\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for i,(imgs,xy,idx,y) in tqdm.tqdm(enumerate(valloader),total=len(valloader.dataset)//valloader.batch_size):\n",
    "#         print(imgs.shape,xy.shape,y.shape)\n",
    "#         edge_index=knn_graph(xy,k=5)\n",
    "        imgs=imgs.cuda()\n",
    "        z=model(imgs).cpu() # the 2048-dimensional embeddings provided by resnet\n",
    "        Z.append(z)\n",
    "        XY.append(xy)\n",
    "        Y.append(y)\n",
    "        Idx.append(idx)\n",
    "        \n",
    "#         graphs.append(Data(x=z,pos=xy,edge_index=edge_index,y=y))\n",
    "    #     print(graphs[i].x.shape)\n",
    "#         if i==100: break\n",
    "        del imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z=torch.cat(Z,0)\n",
    "XY=torch.cat(XY,0)\n",
    "Y=torch.cat(Y).flatten()\n",
    "Idx=torch.cat(Idx).flatten().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4952/4952 [00:13<00:00, 375.93it/s]\n"
     ]
    }
   ],
   "source": [
    "for idx in tqdm.tqdm(np.unique(Idx)):\n",
    "    select=(Idx==idx)\n",
    "    z=Z[select]\n",
    "    xy=XY[select]\n",
    "    y=Y[select]\n",
    "    edge_index=knn_graph(xy,k=k)\n",
    "    val_graphs.append(Data(x=z,pos=xy,edge_index=edge_index,y=y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "del valloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COCO or resnet50 graph embeddings? (coco/resnet50)resnet50\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mod_select = input('COCO or resnet50 graph embeddings? (coco/resnet50) ')\n",
    "# if mod_select == 'coco':\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are you sure you want to overwrite ../data/temp/resnet50_train_graphs_k_5.pkl? ([y],n)\n",
      "CPU times: user 32.7 s, sys: 14.6 s, total: 47.3 s\n",
      "Wall time: 55.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "### For writing new train graphs\n",
    "pickle_name = f'../data/temp/{mod_select}_train_graphs_k_{k}.pkl'\n",
    "ans = input(f'Are you sure you want to overwrite {pickle_name}? ([y],n)')\n",
    "if ans == 'y' or ans=='':\n",
    "    with open(pickle_name, 'wb') as f:\n",
    "    #     val_graphs = pickle.load(f)\n",
    "        pickle.dump(train_graphs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are you sure you want to overwrite ../data/temp/resnet50_val_graphs_k_5.pkl? ([y],n)\n",
      "CPU times: user 1.17 s, sys: 532 ms, total: 1.7 s\n",
      "Wall time: 11.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "### For writing new val graphs\n",
    "val_pickle_name = f'../data/temp/{mod_select}_val_graphs_k_{k}.pkl'\n",
    "\n",
    "ans = input(f'Are you sure you want to overwrite {val_pickle_name}? ([y],n)')\n",
    "if ans == 'y' or ans =='':\n",
    "    with open(f'../data/temp/val_graphs_k_{k}.pkl', 'wb') as f:\n",
    "    #     val_graphs = pickle.load(f)\n",
    "        pickle.dump(val_graphs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 43.4 s, sys: 15.8 s, total: 59.2 s\n",
      "Wall time: 59.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "### For reading train_graphs\n",
    "with open(f'../data/temp/{mod_select}_train_graphs_k_{k}.pkl', 'rb') as f:\n",
    "    train_graphs = pickle.load(f)\n",
    "#     pickle.dump(val_graphs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.83 s, sys: 412 ms, total: 2.24 s\n",
      "Wall time: 2.23 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "### For reading val_graphs\n",
    "with open(f'../data/temp/{mod_select}_val_graphs_k_{k}.pkl', 'rb') as f:\n",
    "    val_graphs = pickle.load(f)\n",
    "#     pickle.dump(val_graphs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import DataLoader as TG_DataLoader\n",
    "# https://pytorch.org/docs/stable/data.html#torch.utils.data.Subset\n",
    "# Batch size should be the same as batch size for dataloader above\n",
    "graph_trainloader = TG_DataLoader(train_graphs, batch_size=train_batch_size, shuffle=True)\n",
    "\n",
    "graph_valloader=TG_DataLoader(val_graphs,batch_size=train_batch_size,shuffle=True)#[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gcn=GCNNet(2048,80, [32]*2)\n",
    "model_mlp=nn.Sequential(nn.Sequential(nn.Linear(2048,32),nn.ReLU(),nn.Dropout(p=0.3)),nn.Linear(32,80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_gcn.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there are GPUs, choose the first one for computing. Otherwise use CPU.\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(device)\n",
    "# If 'cuda:0' is printed, it means GPU is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_gcn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'resnet50_graph_80_10_3'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Params Cell\n",
    "k = 3\n",
    "epochs = 10\n",
    "fname = f'{mod_select}_graph_80_{epochs}_{k}'\n",
    "batch_size=12\n",
    "fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: implement edge index as part of dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "opt = optim.SGD(model_gcn.parameters(), lr=0.0001, momentum=0.9)\n",
    "avg_losses = []\n",
    "print_freq = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 0, i: 99] avg mini-batch loss: 4.295726537704468\n",
      "[epoch: 0, i: 199] avg mini-batch loss: 3.933127408027649\n",
      "[epoch: 0, i: 299] avg mini-batch loss: 3.8372894668579103\n",
      "[epoch: 0, i: 399] avg mini-batch loss: 3.76384491443634\n",
      "[epoch: 0, i: 499] avg mini-batch loss: 3.7350661659240725\n",
      "[epoch: 0, i: 599] avg mini-batch loss: 3.6771505641937257\n",
      "[epoch: 0, i: 699] avg mini-batch loss: 3.5944129276275634\n",
      "[epoch: 0, i: 799] avg mini-batch loss: 3.6781405782699585\n",
      "[epoch: 0, i: 899] avg mini-batch loss: 3.598031578063965\n",
      "[epoch: 0, i: 999] avg mini-batch loss: 3.5866683602333067\n",
      "[epoch: 0, i: 1099] avg mini-batch loss: 3.5426287269592285\n",
      "[epoch: 0, i: 1199] avg mini-batch loss: 3.5633253765106203\n",
      "[epoch: 0, i: 1299] avg mini-batch loss: 3.4951982736587524\n",
      "[epoch: 0, i: 1399] avg mini-batch loss: 3.4962286138534546\n",
      "[epoch: 0, i: 1499] avg mini-batch loss: 3.431639168262482\n",
      "[epoch: 0, i: 1599] avg mini-batch loss: 3.4474546265602113\n",
      "[epoch: 0, i: 1699] avg mini-batch loss: 3.4975180888175963\n",
      "[epoch: 0, i: 1799] avg mini-batch loss: 3.4607158422470095\n",
      "[epoch: 0, i: 1899] avg mini-batch loss: 3.471060378551483\n",
      "[epoch: 0, i: 1999] avg mini-batch loss: 3.4267765641212464\n",
      "[epoch: 0, i: 2099] avg mini-batch loss: 3.429238669872284\n",
      "[epoch: 0, i: 2199] avg mini-batch loss: 3.487053635120392\n",
      "[epoch: 0, i: 2299] avg mini-batch loss: 3.419043278694153\n",
      "[epoch: 0, i: 2399] avg mini-batch loss: 3.3879836082458494\n",
      "[epoch: 0, i: 2499] avg mini-batch loss: 3.459506115913391\n",
      "[epoch: 0, i: 2599] avg mini-batch loss: 3.4177175092697145\n",
      "[epoch: 0, i: 2699] avg mini-batch loss: 3.4622833061218263\n",
      "[epoch: 0, i: 2799] avg mini-batch loss: 3.3695985770225523\n",
      "[epoch: 0, i: 2899] avg mini-batch loss: 3.359615633487701\n",
      "[epoch: 0, i: 2999] avg mini-batch loss: 3.4326825070381166\n",
      "[epoch: 0, i: 3099] avg mini-batch loss: 3.453060266971588\n",
      "[epoch: 0, i: 3199] avg mini-batch loss: 3.376210694313049\n",
      "[epoch: 0, i: 3299] avg mini-batch loss: 3.446442890167236\n",
      "[epoch: 0, i: 3399] avg mini-batch loss: 3.4194921541213987\n",
      "[epoch: 0, i: 3499] avg mini-batch loss: 3.327182469367981\n",
      "[epoch: 0, i: 3599] avg mini-batch loss: 3.392585608959198\n",
      "[epoch: 0, i: 3699] avg mini-batch loss: 3.3815241694450378\n",
      "[epoch: 0, i: 3799] avg mini-batch loss: 3.295775260925293\n",
      "[epoch: 0, i: 3899] avg mini-batch loss: 3.379499099254608\n",
      "[epoch: 0, i: 3999] avg mini-batch loss: 3.389089946746826\n",
      "[epoch: 0, i: 4099] avg mini-batch loss: 3.354984128475189\n",
      "[epoch: 0, i: 4199] avg mini-batch loss: 3.2666798400878907\n",
      "[epoch: 0, i: 4299] avg mini-batch loss: 3.3255565619468688\n",
      "[epoch: 0, i: 4399] avg mini-batch loss: 3.3164543318748474\n",
      "[epoch: 0, i: 4499] avg mini-batch loss: 3.3679638576507567\n",
      "[epoch: 0, i: 4599] avg mini-batch loss: 3.331385192871094\n",
      "[epoch: 0, i: 4699] avg mini-batch loss: 3.291230731010437\n",
      "[epoch: 0, i: 4799] avg mini-batch loss: 3.3462059926986694\n",
      "[epoch: 0, i: 4899] avg mini-batch loss: 3.2976921725273134\n",
      "[epoch: 0, i: 4999] avg mini-batch loss: 3.278917829990387\n",
      "[epoch: 0, i: 5099] avg mini-batch loss: 3.24485880613327\n",
      "[epoch: 0, i: 5199] avg mini-batch loss: 3.2558995056152344\n",
      "[epoch: 0, i: 5299] avg mini-batch loss: 3.240985321998596\n",
      "[epoch: 0, i: 5399] avg mini-batch loss: 3.2457579922676087\n",
      "[epoch: 0, i: 5499] avg mini-batch loss: 3.2406619238853454\n",
      "[epoch: 0, i: 5599] avg mini-batch loss: 3.203135759830475\n",
      "[epoch: 0, i: 5699] avg mini-batch loss: 3.2066339659690857\n",
      "[epoch: 0, i: 5799] avg mini-batch loss: 3.211717767715454\n",
      "[epoch: 0, i: 5899] avg mini-batch loss: 3.2049686431884767\n",
      "[epoch: 0, i: 5999] avg mini-batch loss: 3.2761197805404665\n",
      "[epoch: 0, i: 6099] avg mini-batch loss: 3.218633074760437\n",
      "[epoch: 0, i: 6199] avg mini-batch loss: 3.2559142446517946\n",
      "[epoch: 0, i: 6299] avg mini-batch loss: 3.1996174001693727\n",
      "[epoch: 0, i: 6399] avg mini-batch loss: 3.31457448720932\n",
      "[epoch: 0, i: 6499] avg mini-batch loss: 3.2734403824806213\n",
      "[epoch: 0, i: 6599] avg mini-batch loss: 3.206611728668213\n",
      "[epoch: 0, i: 6699] avg mini-batch loss: 3.2425623321533203\n",
      "[epoch: 0, i: 6799] avg mini-batch loss: 3.284547827243805\n",
      "[epoch: 0, i: 6899] avg mini-batch loss: 3.2167521691322327\n",
      "[epoch: 0, i: 6999] avg mini-batch loss: 3.2952275824546815\n",
      "[epoch: 0, i: 7099] avg mini-batch loss: 3.2205956864356993\n",
      "[epoch: 0, i: 7199] avg mini-batch loss: 3.1853248286247253\n",
      "[epoch: 0, i: 7299] avg mini-batch loss: 3.1907904601097106\n",
      "[epoch: 0, i: 7399] avg mini-batch loss: 3.1805206894874574\n",
      "[epoch: 0, i: 7499] avg mini-batch loss: 3.2022582030296327\n",
      "[epoch: 0, i: 7599] avg mini-batch loss: 3.2103124141693113\n",
      "[epoch: 0, i: 7699] avg mini-batch loss: 3.234763829708099\n",
      "[epoch: 0, i: 7799] avg mini-batch loss: 3.195992438793182\n",
      "[epoch: 0, i: 7899] avg mini-batch loss: 3.1633079624176026\n",
      "[epoch: 0, i: 7999] avg mini-batch loss: 3.198314092159271\n",
      "[epoch: 0, i: 8099] avg mini-batch loss: 3.185040762424469\n",
      "[epoch: 0, i: 8199] avg mini-batch loss: 3.2130974078178407\n",
      "[epoch: 0, i: 8299] avg mini-batch loss: 3.2275322532653807\n",
      "[epoch: 0, i: 8399] avg mini-batch loss: 3.1111452913284303\n",
      "[epoch: 0, i: 8499] avg mini-batch loss: 3.148693504333496\n",
      "[epoch: 0, i: 8599] avg mini-batch loss: 3.153751153945923\n",
      "[epoch: 0, i: 8699] avg mini-batch loss: 3.161583924293518\n",
      "[epoch: 0, i: 8799] avg mini-batch loss: 3.058741545677185\n",
      "[epoch: 0, i: 8899] avg mini-batch loss: 3.1501500153541566\n",
      "[epoch: 0, i: 8999] avg mini-batch loss: 3.0961904096603394\n",
      "[epoch: 0, i: 9099] avg mini-batch loss: 3.1647850608825685\n",
      "[epoch: 0, i: 9199] avg mini-batch loss: 3.1128232669830322\n",
      "[epoch: 0, i: 9299] avg mini-batch loss: 3.1127575182914735\n",
      "[epoch: 0, i: 9399] avg mini-batch loss: 3.1123212027549743\n",
      "[epoch: 0, i: 9499] avg mini-batch loss: 3.1264142036437987\n",
      "[epoch: 0, i: 9599] avg mini-batch loss: 3.1057215332984924\n",
      "[epoch: 0, i: 9699] avg mini-batch loss: 3.116266341209412\n",
      "[epoch: 1, i: 99] avg mini-batch loss: 3.1009517407417295\n",
      "[epoch: 1, i: 199] avg mini-batch loss: 3.12630277633667\n",
      "[epoch: 1, i: 299] avg mini-batch loss: 3.0975173234939577\n",
      "[epoch: 1, i: 399] avg mini-batch loss: 3.1281229662895202\n",
      "[epoch: 1, i: 499] avg mini-batch loss: 3.0612776184082033\n",
      "[epoch: 1, i: 599] avg mini-batch loss: 3.084987325668335\n",
      "[epoch: 1, i: 699] avg mini-batch loss: 3.042518231868744\n",
      "[epoch: 1, i: 799] avg mini-batch loss: 3.07704146027565\n",
      "[epoch: 1, i: 899] avg mini-batch loss: 3.0346364521980287\n",
      "[epoch: 1, i: 999] avg mini-batch loss: 3.0682167840003967\n",
      "[epoch: 1, i: 1099] avg mini-batch loss: 3.094024670124054\n",
      "[epoch: 1, i: 1199] avg mini-batch loss: 3.0431931376457215\n",
      "[epoch: 1, i: 1299] avg mini-batch loss: 3.040480191707611\n",
      "[epoch: 1, i: 1399] avg mini-batch loss: 3.0732742941379545\n",
      "[epoch: 1, i: 1499] avg mini-batch loss: 3.092306652069092\n",
      "[epoch: 1, i: 1599] avg mini-batch loss: 3.052896776199341\n",
      "[epoch: 1, i: 1699] avg mini-batch loss: 3.034253251552582\n",
      "[epoch: 1, i: 1799] avg mini-batch loss: 3.068308594226837\n",
      "[epoch: 1, i: 1899] avg mini-batch loss: 3.070156903266907\n",
      "[epoch: 1, i: 1999] avg mini-batch loss: 2.9813702726364135\n",
      "[epoch: 1, i: 2099] avg mini-batch loss: 3.0096508121490477\n",
      "[epoch: 1, i: 2199] avg mini-batch loss: 3.031567474603653\n",
      "[epoch: 1, i: 2299] avg mini-batch loss: 3.0607422852516173\n",
      "[epoch: 1, i: 2399] avg mini-batch loss: 3.0167999267578125\n",
      "[epoch: 1, i: 2499] avg mini-batch loss: 3.0283147954940794\n",
      "[epoch: 1, i: 2599] avg mini-batch loss: 2.98312922000885\n",
      "[epoch: 1, i: 2699] avg mini-batch loss: 3.023729965686798\n",
      "[epoch: 1, i: 2799] avg mini-batch loss: 3.0069147324562073\n",
      "[epoch: 1, i: 2899] avg mini-batch loss: 2.9810413575172423\n",
      "[epoch: 1, i: 2999] avg mini-batch loss: 2.9801057529449464\n",
      "[epoch: 1, i: 3099] avg mini-batch loss: 2.9863352465629576\n",
      "[epoch: 1, i: 3199] avg mini-batch loss: 2.903770388364792\n",
      "[epoch: 1, i: 3299] avg mini-batch loss: 2.960961525440216\n",
      "[epoch: 1, i: 3399] avg mini-batch loss: 2.9164778387546537\n",
      "[epoch: 1, i: 3499] avg mini-batch loss: 3.0106273984909055\n",
      "[epoch: 1, i: 3599] avg mini-batch loss: 3.0184749031066893\n",
      "[epoch: 1, i: 3699] avg mini-batch loss: 2.974705307483673\n",
      "[epoch: 1, i: 3799] avg mini-batch loss: 2.8977794289588927\n",
      "[epoch: 1, i: 3899] avg mini-batch loss: 2.954616105556488\n",
      "[epoch: 1, i: 3999] avg mini-batch loss: 2.952152659893036\n",
      "[epoch: 1, i: 4099] avg mini-batch loss: 2.898328125476837\n",
      "[epoch: 1, i: 4199] avg mini-batch loss: 2.9469327092170716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 1, i: 4299] avg mini-batch loss: 2.964483528137207\n",
      "[epoch: 1, i: 4399] avg mini-batch loss: 3.0205546498298643\n",
      "[epoch: 1, i: 4499] avg mini-batch loss: 2.935910210609436\n",
      "[epoch: 1, i: 4599] avg mini-batch loss: 2.903560700416565\n",
      "[epoch: 1, i: 4699] avg mini-batch loss: 2.940667595863342\n",
      "[epoch: 1, i: 4799] avg mini-batch loss: 2.91302551984787\n",
      "[epoch: 1, i: 4899] avg mini-batch loss: 2.962537581920624\n",
      "[epoch: 1, i: 4999] avg mini-batch loss: 2.895660493373871\n",
      "[epoch: 1, i: 5099] avg mini-batch loss: 2.976808066368103\n",
      "[epoch: 1, i: 5199] avg mini-batch loss: 2.9110999596118927\n",
      "[epoch: 1, i: 5299] avg mini-batch loss: 2.8825952506065367\n",
      "[epoch: 1, i: 5399] avg mini-batch loss: 2.9123137307167055\n",
      "[epoch: 1, i: 5499] avg mini-batch loss: 2.904320688247681\n",
      "[epoch: 1, i: 5599] avg mini-batch loss: 2.8773550176620484\n",
      "[epoch: 1, i: 5699] avg mini-batch loss: 2.914371967315674\n",
      "[epoch: 1, i: 5799] avg mini-batch loss: 2.9443161916732787\n",
      "[epoch: 1, i: 5899] avg mini-batch loss: 2.8554457914829254\n",
      "[epoch: 1, i: 5999] avg mini-batch loss: 2.879143959283829\n",
      "[epoch: 1, i: 6099] avg mini-batch loss: 2.901215295791626\n",
      "[epoch: 1, i: 6199] avg mini-batch loss: 2.87852429151535\n",
      "[epoch: 1, i: 6299] avg mini-batch loss: 2.8340687477588653\n",
      "[epoch: 1, i: 6399] avg mini-batch loss: 2.857949948310852\n",
      "[epoch: 1, i: 6499] avg mini-batch loss: 2.892685832977295\n",
      "[epoch: 1, i: 6599] avg mini-batch loss: 2.8054152441024782\n",
      "[epoch: 1, i: 6699] avg mini-batch loss: 2.8615425455570223\n",
      "[epoch: 1, i: 6799] avg mini-batch loss: 2.8956944847106936\n",
      "[epoch: 1, i: 6899] avg mini-batch loss: 2.7918020272254944\n",
      "[epoch: 1, i: 6999] avg mini-batch loss: 2.854188220500946\n",
      "[epoch: 1, i: 7099] avg mini-batch loss: 2.8583522510528563\n",
      "[epoch: 1, i: 7199] avg mini-batch loss: 2.8541147017478945\n",
      "[epoch: 1, i: 7299] avg mini-batch loss: 2.8141910457611083\n",
      "[epoch: 1, i: 7399] avg mini-batch loss: 2.8113386392593385\n",
      "[epoch: 1, i: 7499] avg mini-batch loss: 2.7973016238212587\n",
      "[epoch: 1, i: 7599] avg mini-batch loss: 2.8035169756412506\n",
      "[epoch: 1, i: 7699] avg mini-batch loss: 2.805401382446289\n",
      "[epoch: 1, i: 7799] avg mini-batch loss: 2.799214606285095\n",
      "[epoch: 1, i: 7899] avg mini-batch loss: 2.8319101977348327\n",
      "[epoch: 1, i: 7999] avg mini-batch loss: 2.7640417766571046\n",
      "[epoch: 1, i: 8099] avg mini-batch loss: 2.8061263847351072\n",
      "[epoch: 1, i: 8199] avg mini-batch loss: 2.8385020303726196\n",
      "[epoch: 1, i: 8299] avg mini-batch loss: 2.847425720691681\n",
      "[epoch: 1, i: 8399] avg mini-batch loss: 2.798937246799469\n",
      "[epoch: 1, i: 8499] avg mini-batch loss: 2.8080899214744566\n",
      "[epoch: 1, i: 8599] avg mini-batch loss: 2.7853189182281493\n",
      "[epoch: 1, i: 8699] avg mini-batch loss: 2.754325249195099\n",
      "[epoch: 1, i: 8799] avg mini-batch loss: 2.8080706214904785\n",
      "[epoch: 1, i: 8899] avg mini-batch loss: 2.8354717254638673\n",
      "[epoch: 1, i: 8999] avg mini-batch loss: 2.7493686127662658\n",
      "[epoch: 1, i: 9099] avg mini-batch loss: 2.795888541936874\n",
      "[epoch: 1, i: 9199] avg mini-batch loss: 2.8042462611198427\n",
      "[epoch: 1, i: 9299] avg mini-batch loss: 2.732253544330597\n",
      "[epoch: 1, i: 9399] avg mini-batch loss: 2.695783998966217\n",
      "[epoch: 1, i: 9499] avg mini-batch loss: 2.8177101588249207\n",
      "[epoch: 1, i: 9599] avg mini-batch loss: 2.7609234178066253\n",
      "[epoch: 1, i: 9699] avg mini-batch loss: 2.764128203392029\n",
      "[epoch: 2, i: 99] avg mini-batch loss: 2.7953457975387574\n",
      "[epoch: 2, i: 199] avg mini-batch loss: 2.796196768283844\n",
      "[epoch: 2, i: 299] avg mini-batch loss: 2.7834770274162293\n",
      "[epoch: 2, i: 399] avg mini-batch loss: 2.749694226980209\n",
      "[epoch: 2, i: 499] avg mini-batch loss: 2.7032637357711793\n",
      "[epoch: 2, i: 599] avg mini-batch loss: 2.713120769262314\n",
      "[epoch: 2, i: 699] avg mini-batch loss: 2.737568271160126\n",
      "[epoch: 2, i: 799] avg mini-batch loss: 2.7263142347335814\n",
      "[epoch: 2, i: 899] avg mini-batch loss: 2.7391935849189757\n",
      "[epoch: 2, i: 999] avg mini-batch loss: 2.7757241237163544\n",
      "[epoch: 2, i: 1099] avg mini-batch loss: 2.8120921552181244\n",
      "[epoch: 2, i: 1199] avg mini-batch loss: 2.750062190294266\n",
      "[epoch: 2, i: 1299] avg mini-batch loss: 2.7510063886642455\n",
      "[epoch: 2, i: 1399] avg mini-batch loss: 2.655735095739365\n",
      "[epoch: 2, i: 1499] avg mini-batch loss: 2.7987298226356505\n",
      "[epoch: 2, i: 1599] avg mini-batch loss: 2.7297447311878202\n",
      "[epoch: 2, i: 1699] avg mini-batch loss: 2.71665465593338\n",
      "[epoch: 2, i: 1799] avg mini-batch loss: 2.743765318393707\n",
      "[epoch: 2, i: 1899] avg mini-batch loss: 2.701393802165985\n",
      "[epoch: 2, i: 1999] avg mini-batch loss: 2.737500783205032\n",
      "[epoch: 2, i: 2099] avg mini-batch loss: 2.702962316274643\n",
      "[epoch: 2, i: 2199] avg mini-batch loss: 2.7679635000228884\n",
      "[epoch: 2, i: 2299] avg mini-batch loss: 2.6961081540584564\n",
      "[epoch: 2, i: 2399] avg mini-batch loss: 2.707894333600998\n",
      "[epoch: 2, i: 2499] avg mini-batch loss: 2.7579335510730743\n",
      "[epoch: 2, i: 2599] avg mini-batch loss: 2.699052596092224\n",
      "[epoch: 2, i: 2699] avg mini-batch loss: 2.6604468631744385\n",
      "[epoch: 2, i: 2799] avg mini-batch loss: 2.610251865386963\n",
      "[epoch: 2, i: 2899] avg mini-batch loss: 2.6776510536670686\n",
      "[epoch: 2, i: 2999] avg mini-batch loss: 2.6760223829746246\n",
      "[epoch: 2, i: 3099] avg mini-batch loss: 2.694721882343292\n",
      "[epoch: 2, i: 3199] avg mini-batch loss: 2.715321123600006\n",
      "[epoch: 2, i: 3299] avg mini-batch loss: 2.6130994367599487\n",
      "[epoch: 2, i: 3399] avg mini-batch loss: 2.6437247729301454\n",
      "[epoch: 2, i: 3499] avg mini-batch loss: 2.6184762120246887\n",
      "[epoch: 2, i: 3599] avg mini-batch loss: 2.685138908624649\n",
      "[epoch: 2, i: 3699] avg mini-batch loss: 2.679064486026764\n",
      "[epoch: 2, i: 3799] avg mini-batch loss: 2.704359951019287\n",
      "[epoch: 2, i: 3899] avg mini-batch loss: 2.7228309619426727\n",
      "[epoch: 2, i: 3999] avg mini-batch loss: 2.746200203895569\n",
      "[epoch: 2, i: 4099] avg mini-batch loss: 2.6676395964622497\n",
      "[epoch: 2, i: 4199] avg mini-batch loss: 2.62271710395813\n",
      "[epoch: 2, i: 4299] avg mini-batch loss: 2.6764838564395905\n",
      "[epoch: 2, i: 4399] avg mini-batch loss: 2.665486820936203\n",
      "[epoch: 2, i: 4499] avg mini-batch loss: 2.6570433735847474\n",
      "[epoch: 2, i: 4599] avg mini-batch loss: 2.6376107251644134\n",
      "[epoch: 2, i: 4699] avg mini-batch loss: 2.6029156708717345\n",
      "[epoch: 2, i: 4799] avg mini-batch loss: 2.6786033153533935\n",
      "[epoch: 2, i: 4899] avg mini-batch loss: 2.691534165143967\n",
      "[epoch: 2, i: 4999] avg mini-batch loss: 2.650987830162048\n",
      "[epoch: 2, i: 5099] avg mini-batch loss: 2.6601921117305753\n",
      "[epoch: 2, i: 5199] avg mini-batch loss: 2.6405267727375032\n",
      "[epoch: 2, i: 5299] avg mini-batch loss: 2.6208058583736418\n",
      "[epoch: 2, i: 5399] avg mini-batch loss: 2.651638686656952\n",
      "[epoch: 2, i: 5499] avg mini-batch loss: 2.694521815776825\n",
      "[epoch: 2, i: 5599] avg mini-batch loss: 2.603648067712784\n",
      "[epoch: 2, i: 5699] avg mini-batch loss: 2.661505779027939\n",
      "[epoch: 2, i: 5799] avg mini-batch loss: 2.6113070809841155\n",
      "[epoch: 2, i: 5899] avg mini-batch loss: 2.6294496941566465\n",
      "[epoch: 2, i: 5999] avg mini-batch loss: 2.6820608556270598\n",
      "[epoch: 2, i: 6099] avg mini-batch loss: 2.618626708984375\n",
      "[epoch: 2, i: 6199] avg mini-batch loss: 2.680565814971924\n",
      "[epoch: 2, i: 6299] avg mini-batch loss: 2.6484101271629332\n",
      "[epoch: 2, i: 6399] avg mini-batch loss: 2.5799028706550597\n",
      "[epoch: 2, i: 6499] avg mini-batch loss: 2.597257822751999\n",
      "[epoch: 2, i: 6599] avg mini-batch loss: 2.614997171163559\n",
      "[epoch: 2, i: 6699] avg mini-batch loss: 2.6489905881881715\n",
      "[epoch: 2, i: 6799] avg mini-batch loss: 2.6499333715438844\n",
      "[epoch: 2, i: 6899] avg mini-batch loss: 2.6265686571598055\n",
      "[epoch: 2, i: 6999] avg mini-batch loss: 2.6132695269584656\n",
      "[epoch: 2, i: 7099] avg mini-batch loss: 2.6070854580402374\n",
      "[epoch: 2, i: 7199] avg mini-batch loss: 2.5815085983276367\n",
      "[epoch: 2, i: 7299] avg mini-batch loss: 2.6041160082817076\n",
      "[epoch: 2, i: 7399] avg mini-batch loss: 2.6017159032821655\n",
      "[epoch: 2, i: 7499] avg mini-batch loss: 2.5855846011638643\n",
      "[epoch: 2, i: 7599] avg mini-batch loss: 2.650627157688141\n",
      "[epoch: 2, i: 7699] avg mini-batch loss: 2.588944066762924\n",
      "[epoch: 2, i: 7799] avg mini-batch loss: 2.6290521943569183\n",
      "[epoch: 2, i: 7899] avg mini-batch loss: 2.608702541589737\n",
      "[epoch: 2, i: 7999] avg mini-batch loss: 2.556652059555054\n",
      "[epoch: 2, i: 8099] avg mini-batch loss: 2.622038793563843\n",
      "[epoch: 2, i: 8199] avg mini-batch loss: 2.6158932518959044\n",
      "[epoch: 2, i: 8299] avg mini-batch loss: 2.6323691487312315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 2, i: 8399] avg mini-batch loss: 2.6329364621639253\n",
      "[epoch: 2, i: 8499] avg mini-batch loss: 2.556089026927948\n",
      "[epoch: 2, i: 8599] avg mini-batch loss: 2.644288957118988\n",
      "[epoch: 2, i: 8699] avg mini-batch loss: 2.621056158542633\n",
      "[epoch: 2, i: 8799] avg mini-batch loss: 2.6299956798553468\n",
      "[epoch: 2, i: 8899] avg mini-batch loss: 2.6203514432907102\n",
      "[epoch: 2, i: 8999] avg mini-batch loss: 2.5119445073604583\n",
      "[epoch: 2, i: 9099] avg mini-batch loss: 2.5613509225845337\n",
      "[epoch: 2, i: 9199] avg mini-batch loss: 2.5900547921657564\n",
      "[epoch: 2, i: 9299] avg mini-batch loss: 2.625772867202759\n",
      "[epoch: 2, i: 9399] avg mini-batch loss: 2.5845332300662993\n",
      "[epoch: 2, i: 9499] avg mini-batch loss: 2.5618958640098572\n",
      "[epoch: 2, i: 9599] avg mini-batch loss: 2.57918670296669\n",
      "[epoch: 2, i: 9699] avg mini-batch loss: 2.554952073097229\n",
      "[epoch: 3, i: 99] avg mini-batch loss: 2.565951261520386\n",
      "[epoch: 3, i: 199] avg mini-batch loss: 2.573338257074356\n",
      "[epoch: 3, i: 299] avg mini-batch loss: 2.5728598964214324\n",
      "[epoch: 3, i: 399] avg mini-batch loss: 2.6516767358779907\n",
      "[epoch: 3, i: 499] avg mini-batch loss: 2.603386832475662\n",
      "[epoch: 3, i: 599] avg mini-batch loss: 2.5945349490642546\n",
      "[epoch: 3, i: 699] avg mini-batch loss: 2.5438768494129183\n",
      "[epoch: 3, i: 799] avg mini-batch loss: 2.5527679920196533\n",
      "[epoch: 3, i: 899] avg mini-batch loss: 2.51069886803627\n",
      "[epoch: 3, i: 999] avg mini-batch loss: 2.578827916383743\n",
      "[epoch: 3, i: 1099] avg mini-batch loss: 2.5296736335754395\n",
      "[epoch: 3, i: 1199] avg mini-batch loss: 2.5623094856739046\n",
      "[epoch: 3, i: 1299] avg mini-batch loss: 2.4982622337341307\n",
      "[epoch: 3, i: 1399] avg mini-batch loss: 2.543763303756714\n",
      "[epoch: 3, i: 1499] avg mini-batch loss: 2.5838382196426393\n",
      "[epoch: 3, i: 1599] avg mini-batch loss: 2.54982679605484\n",
      "[epoch: 3, i: 1699] avg mini-batch loss: 2.5989623296260835\n",
      "[epoch: 3, i: 1799] avg mini-batch loss: 2.54372292637825\n",
      "[epoch: 3, i: 1899] avg mini-batch loss: 2.53079607963562\n",
      "[epoch: 3, i: 1999] avg mini-batch loss: 2.5367724454402922\n",
      "[epoch: 3, i: 2099] avg mini-batch loss: 2.546988127231598\n",
      "[epoch: 3, i: 2199] avg mini-batch loss: 2.602424634695053\n",
      "[epoch: 3, i: 2299] avg mini-batch loss: 2.572509264945984\n",
      "[epoch: 3, i: 2399] avg mini-batch loss: 2.5265051341056823\n",
      "[epoch: 3, i: 2499] avg mini-batch loss: 2.513736455440521\n",
      "[epoch: 3, i: 2599] avg mini-batch loss: 2.5538022148609163\n",
      "[epoch: 3, i: 2699] avg mini-batch loss: 2.476024322509766\n",
      "[epoch: 3, i: 2799] avg mini-batch loss: 2.516070114374161\n",
      "[epoch: 3, i: 2899] avg mini-batch loss: 2.5183977270126343\n",
      "[epoch: 3, i: 2999] avg mini-batch loss: 2.4947936940193176\n",
      "[epoch: 3, i: 3099] avg mini-batch loss: 2.593425352573395\n",
      "[epoch: 3, i: 3199] avg mini-batch loss: 2.5314022386074067\n",
      "[epoch: 3, i: 3299] avg mini-batch loss: 2.4883791005611418\n",
      "[epoch: 3, i: 3399] avg mini-batch loss: 2.5952449905872346\n",
      "[epoch: 3, i: 3499] avg mini-batch loss: 2.495294771194458\n",
      "[epoch: 3, i: 3599] avg mini-batch loss: 2.4970776629447937\n",
      "[epoch: 3, i: 3699] avg mini-batch loss: 2.490680111646652\n",
      "[epoch: 3, i: 3799] avg mini-batch loss: 2.5144020354747774\n",
      "[epoch: 3, i: 3899] avg mini-batch loss: 2.5081423318386076\n",
      "[epoch: 3, i: 3999] avg mini-batch loss: 2.465984661579132\n",
      "[epoch: 3, i: 4099] avg mini-batch loss: 2.5177450048923493\n",
      "[epoch: 3, i: 4199] avg mini-batch loss: 2.535097737312317\n",
      "[epoch: 3, i: 4299] avg mini-batch loss: 2.5178802251815795\n",
      "[epoch: 3, i: 4399] avg mini-batch loss: 2.493179794549942\n",
      "[epoch: 3, i: 4499] avg mini-batch loss: 2.4797774505615235\n",
      "[epoch: 3, i: 4599] avg mini-batch loss: 2.475342752933502\n",
      "[epoch: 3, i: 4699] avg mini-batch loss: 2.5900921094417573\n",
      "[epoch: 3, i: 4799] avg mini-batch loss: 2.5180672562122344\n",
      "[epoch: 3, i: 4899] avg mini-batch loss: 2.4819493246078492\n",
      "[epoch: 3, i: 4999] avg mini-batch loss: 2.4834579145908355\n",
      "[epoch: 3, i: 5099] avg mini-batch loss: 2.532667005062103\n",
      "[epoch: 3, i: 5199] avg mini-batch loss: 2.5350280797481535\n",
      "[epoch: 3, i: 5299] avg mini-batch loss: 2.539705228805542\n",
      "[epoch: 3, i: 5399] avg mini-batch loss: 2.5399998831748962\n",
      "[epoch: 3, i: 5499] avg mini-batch loss: 2.5452843153476716\n",
      "[epoch: 3, i: 5599] avg mini-batch loss: 2.556122274398804\n",
      "[epoch: 3, i: 5699] avg mini-batch loss: 2.4909649467468262\n",
      "[epoch: 3, i: 5799] avg mini-batch loss: 2.4896218967437744\n",
      "[epoch: 3, i: 5899] avg mini-batch loss: 2.5253459560871123\n",
      "[epoch: 3, i: 5999] avg mini-batch loss: 2.4555216360092165\n",
      "[epoch: 3, i: 6099] avg mini-batch loss: 2.5331138551235197\n",
      "[epoch: 3, i: 6199] avg mini-batch loss: 2.453678014278412\n",
      "[epoch: 3, i: 6299] avg mini-batch loss: 2.531354521512985\n",
      "[epoch: 3, i: 6399] avg mini-batch loss: 2.5096197545528414\n",
      "[epoch: 3, i: 6499] avg mini-batch loss: 2.528224868774414\n",
      "[epoch: 3, i: 6599] avg mini-batch loss: 2.474389396905899\n",
      "[epoch: 3, i: 6699] avg mini-batch loss: 2.4284214305877687\n",
      "[epoch: 3, i: 6799] avg mini-batch loss: 2.500397421121597\n",
      "[epoch: 3, i: 6899] avg mini-batch loss: 2.517744357585907\n",
      "[epoch: 3, i: 6999] avg mini-batch loss: 2.4482479548454283\n",
      "[epoch: 3, i: 7099] avg mini-batch loss: 2.4964100241661074\n",
      "[epoch: 3, i: 7199] avg mini-batch loss: 2.4632927882671356\n",
      "[epoch: 3, i: 7299] avg mini-batch loss: 2.4741941785812376\n",
      "[epoch: 3, i: 7399] avg mini-batch loss: 2.503364396095276\n",
      "[epoch: 3, i: 7499] avg mini-batch loss: 2.490320870876312\n",
      "[epoch: 3, i: 7599] avg mini-batch loss: 2.5022894847393036\n",
      "[epoch: 3, i: 7699] avg mini-batch loss: 2.483492511510849\n",
      "[epoch: 3, i: 7799] avg mini-batch loss: 2.4346701347827913\n",
      "[epoch: 3, i: 7899] avg mini-batch loss: 2.5459831166267395\n",
      "[epoch: 3, i: 7999] avg mini-batch loss: 2.4273182117938994\n",
      "[epoch: 3, i: 8099] avg mini-batch loss: 2.406353406906128\n",
      "[epoch: 3, i: 8199] avg mini-batch loss: 2.519449191093445\n",
      "[epoch: 3, i: 8299] avg mini-batch loss: 2.428503566980362\n",
      "[epoch: 3, i: 8399] avg mini-batch loss: 2.4950479459762573\n",
      "[epoch: 3, i: 8499] avg mini-batch loss: 2.4781019997596743\n",
      "[epoch: 3, i: 8599] avg mini-batch loss: 2.3988080167770387\n",
      "[epoch: 3, i: 8699] avg mini-batch loss: 2.495717499256134\n",
      "[epoch: 3, i: 8799] avg mini-batch loss: 2.4971797311306\n",
      "[epoch: 3, i: 8899] avg mini-batch loss: 2.4308756601810457\n",
      "[epoch: 3, i: 8999] avg mini-batch loss: 2.498145623207092\n",
      "[epoch: 3, i: 9099] avg mini-batch loss: 2.4929776179790495\n",
      "[epoch: 3, i: 9199] avg mini-batch loss: 2.4632000803947447\n",
      "[epoch: 3, i: 9299] avg mini-batch loss: 2.5212618100643156\n",
      "[epoch: 3, i: 9399] avg mini-batch loss: 2.4340264105796816\n",
      "[epoch: 3, i: 9499] avg mini-batch loss: 2.461280814409256\n",
      "[epoch: 3, i: 9599] avg mini-batch loss: 2.4053535759449005\n",
      "[epoch: 3, i: 9699] avg mini-batch loss: 2.4914444839954375\n",
      "[epoch: 4, i: 99] avg mini-batch loss: 2.4202806580066683\n",
      "[epoch: 4, i: 199] avg mini-batch loss: 2.4649393463134768\n",
      "[epoch: 4, i: 299] avg mini-batch loss: 2.3858278012275695\n",
      "[epoch: 4, i: 399] avg mini-batch loss: 2.4639635133743285\n",
      "[epoch: 4, i: 499] avg mini-batch loss: 2.455377999544144\n",
      "[epoch: 4, i: 599] avg mini-batch loss: 2.416513500213623\n",
      "[epoch: 4, i: 699] avg mini-batch loss: 2.4651723301410673\n",
      "[epoch: 4, i: 799] avg mini-batch loss: 2.472522642612457\n",
      "[epoch: 4, i: 899] avg mini-batch loss: 2.4703375124931335\n",
      "[epoch: 4, i: 999] avg mini-batch loss: 2.4777933311462403\n",
      "[epoch: 4, i: 1099] avg mini-batch loss: 2.3563526582717897\n",
      "[epoch: 4, i: 1199] avg mini-batch loss: 2.43225479722023\n",
      "[epoch: 4, i: 1299] avg mini-batch loss: 2.450443311929703\n",
      "[epoch: 4, i: 1399] avg mini-batch loss: 2.4064529407024384\n",
      "[epoch: 4, i: 1499] avg mini-batch loss: 2.424131498336792\n",
      "[epoch: 4, i: 1599] avg mini-batch loss: 2.4607195723056794\n",
      "[epoch: 4, i: 1699] avg mini-batch loss: 2.453733197450638\n",
      "[epoch: 4, i: 1799] avg mini-batch loss: 2.4898787677288055\n",
      "[epoch: 4, i: 1899] avg mini-batch loss: 2.4302830183506012\n",
      "[epoch: 4, i: 1999] avg mini-batch loss: 2.4282795107364654\n",
      "[epoch: 4, i: 2099] avg mini-batch loss: 2.4203640031814575\n",
      "[epoch: 4, i: 2199] avg mini-batch loss: 2.4926097023487093\n",
      "[epoch: 4, i: 2299] avg mini-batch loss: 2.452137703895569\n",
      "[epoch: 4, i: 2399] avg mini-batch loss: 2.4385224878787994\n",
      "[epoch: 4, i: 2499] avg mini-batch loss: 2.41235600233078\n",
      "[epoch: 4, i: 2599] avg mini-batch loss: 2.406632058620453\n",
      "[epoch: 4, i: 2699] avg mini-batch loss: 2.4229661297798155\n",
      "[epoch: 4, i: 2799] avg mini-batch loss: 2.4112196552753447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 4, i: 2899] avg mini-batch loss: 2.39104635477066\n",
      "[epoch: 4, i: 2999] avg mini-batch loss: 2.5146977877616883\n",
      "[epoch: 4, i: 3099] avg mini-batch loss: 2.4103864312171934\n",
      "[epoch: 4, i: 3199] avg mini-batch loss: 2.4222772479057313\n",
      "[epoch: 4, i: 3299] avg mini-batch loss: 2.403122798204422\n",
      "[epoch: 4, i: 3399] avg mini-batch loss: 2.425963945388794\n",
      "[epoch: 4, i: 3499] avg mini-batch loss: 2.4612422227859496\n",
      "[epoch: 4, i: 3599] avg mini-batch loss: 2.442263287305832\n",
      "[epoch: 4, i: 3699] avg mini-batch loss: 2.377813640832901\n",
      "[epoch: 4, i: 3799] avg mini-batch loss: 2.3972443640232086\n",
      "[epoch: 4, i: 3899] avg mini-batch loss: 2.464518733024597\n",
      "[epoch: 4, i: 3999] avg mini-batch loss: 2.4545074677467347\n",
      "[epoch: 4, i: 4099] avg mini-batch loss: 2.410927994251251\n",
      "[epoch: 4, i: 4199] avg mini-batch loss: 2.400344716310501\n",
      "[epoch: 4, i: 4299] avg mini-batch loss: 2.4906189358234405\n",
      "[epoch: 4, i: 4399] avg mini-batch loss: 2.440187945365906\n",
      "[epoch: 4, i: 4499] avg mini-batch loss: 2.4010310995578767\n",
      "[epoch: 4, i: 4599] avg mini-batch loss: 2.3785757386684416\n",
      "[epoch: 4, i: 4699] avg mini-batch loss: 2.3791045033931733\n",
      "[epoch: 4, i: 4799] avg mini-batch loss: 2.451817557811737\n",
      "[epoch: 4, i: 4899] avg mini-batch loss: 2.3734667563438414\n",
      "[epoch: 4, i: 4999] avg mini-batch loss: 2.3673115146160124\n",
      "[epoch: 4, i: 5099] avg mini-batch loss: 2.341091951131821\n",
      "[epoch: 4, i: 5199] avg mini-batch loss: 2.388969706296921\n",
      "[epoch: 4, i: 5299] avg mini-batch loss: 2.441169104576111\n",
      "[epoch: 4, i: 5399] avg mini-batch loss: 2.4222807013988494\n",
      "[epoch: 4, i: 5499] avg mini-batch loss: 2.4230747759342193\n",
      "[epoch: 4, i: 5599] avg mini-batch loss: 2.342195372581482\n",
      "[epoch: 4, i: 5699] avg mini-batch loss: 2.375511099100113\n",
      "[epoch: 4, i: 5799] avg mini-batch loss: 2.337604341506958\n",
      "[epoch: 4, i: 5899] avg mini-batch loss: 2.4162406277656556\n",
      "[epoch: 4, i: 5999] avg mini-batch loss: 2.412899307012558\n",
      "[epoch: 4, i: 6099] avg mini-batch loss: 2.3926154565811157\n",
      "[epoch: 4, i: 6199] avg mini-batch loss: 2.4623653745651244\n",
      "[epoch: 4, i: 6299] avg mini-batch loss: 2.4325508213043214\n",
      "[epoch: 4, i: 6399] avg mini-batch loss: 2.434058257341385\n",
      "[epoch: 4, i: 6499] avg mini-batch loss: 2.397999573945999\n",
      "[epoch: 4, i: 6599] avg mini-batch loss: 2.423041614294052\n",
      "[epoch: 4, i: 6699] avg mini-batch loss: 2.4475244760513304\n",
      "[epoch: 4, i: 6799] avg mini-batch loss: 2.4298633623123167\n",
      "[epoch: 4, i: 6899] avg mini-batch loss: 2.4287966358661652\n",
      "[epoch: 4, i: 6999] avg mini-batch loss: 2.3505386090278626\n",
      "[epoch: 4, i: 7099] avg mini-batch loss: 2.4514725542068483\n",
      "[epoch: 4, i: 7199] avg mini-batch loss: 2.455390111207962\n",
      "[epoch: 4, i: 7299] avg mini-batch loss: 2.358752919435501\n",
      "[epoch: 4, i: 7399] avg mini-batch loss: 2.466728332042694\n",
      "[epoch: 4, i: 7499] avg mini-batch loss: 2.415605283975601\n",
      "[epoch: 4, i: 7599] avg mini-batch loss: 2.4278993141651153\n",
      "[epoch: 4, i: 7699] avg mini-batch loss: 2.4198079454898833\n",
      "[epoch: 4, i: 7799] avg mini-batch loss: 2.4031322872638703\n",
      "[epoch: 4, i: 7899] avg mini-batch loss: 2.365986156463623\n",
      "[epoch: 4, i: 7999] avg mini-batch loss: 2.390835226774216\n",
      "[epoch: 4, i: 8099] avg mini-batch loss: 2.4521626496315\n",
      "[epoch: 4, i: 8199] avg mini-batch loss: 2.4233167469501495\n",
      "[epoch: 4, i: 8299] avg mini-batch loss: 2.3684643363952635\n",
      "[epoch: 4, i: 8399] avg mini-batch loss: 2.3215102565288546\n",
      "[epoch: 4, i: 8499] avg mini-batch loss: 2.347872083187103\n",
      "[epoch: 4, i: 8599] avg mini-batch loss: 2.358330627679825\n",
      "[epoch: 4, i: 8699] avg mini-batch loss: 2.411903398036957\n",
      "[epoch: 4, i: 8799] avg mini-batch loss: 2.411403373479843\n",
      "[epoch: 4, i: 8899] avg mini-batch loss: 2.425152369737625\n",
      "[epoch: 4, i: 8999] avg mini-batch loss: 2.3679070687294006\n",
      "[epoch: 4, i: 9099] avg mini-batch loss: 2.363716274499893\n",
      "[epoch: 4, i: 9199] avg mini-batch loss: 2.400320429801941\n",
      "[epoch: 4, i: 9299] avg mini-batch loss: 2.419118869304657\n",
      "[epoch: 4, i: 9399] avg mini-batch loss: 2.426955305337906\n",
      "[epoch: 4, i: 9499] avg mini-batch loss: 2.3868285465240477\n",
      "[epoch: 4, i: 9599] avg mini-batch loss: 2.355195643901825\n",
      "[epoch: 4, i: 9699] avg mini-batch loss: 2.3733496952056883\n",
      "[epoch: 5, i: 99] avg mini-batch loss: 2.3962005496025087\n",
      "[epoch: 5, i: 199] avg mini-batch loss: 2.4290440106391906\n",
      "[epoch: 5, i: 299] avg mini-batch loss: 2.3612940204143524\n",
      "[epoch: 5, i: 399] avg mini-batch loss: 2.3969798839092253\n",
      "[epoch: 5, i: 499] avg mini-batch loss: 2.357991145849228\n",
      "[epoch: 5, i: 599] avg mini-batch loss: 2.414730669260025\n",
      "[epoch: 5, i: 699] avg mini-batch loss: 2.3269779407978057\n",
      "[epoch: 5, i: 799] avg mini-batch loss: 2.3678539383411406\n",
      "[epoch: 5, i: 899] avg mini-batch loss: 2.348650461435318\n",
      "[epoch: 5, i: 999] avg mini-batch loss: 2.364118731021881\n",
      "[epoch: 5, i: 1099] avg mini-batch loss: 2.4124287283420562\n",
      "[epoch: 5, i: 1199] avg mini-batch loss: 2.386029441356659\n",
      "[epoch: 5, i: 1299] avg mini-batch loss: 2.4010957634449004\n",
      "[epoch: 5, i: 1399] avg mini-batch loss: 2.386635103225708\n",
      "[epoch: 5, i: 1499] avg mini-batch loss: 2.3472512316703797\n",
      "[epoch: 5, i: 1599] avg mini-batch loss: 2.360563212633133\n",
      "[epoch: 5, i: 1699] avg mini-batch loss: 2.3863438177108764\n",
      "[epoch: 5, i: 1799] avg mini-batch loss: 2.370444952249527\n",
      "[epoch: 5, i: 1899] avg mini-batch loss: 2.352285726070404\n",
      "[epoch: 5, i: 1999] avg mini-batch loss: 2.3584558951854704\n",
      "[epoch: 5, i: 2099] avg mini-batch loss: 2.3479882252216338\n",
      "[epoch: 5, i: 2199] avg mini-batch loss: 2.411191506385803\n",
      "[epoch: 5, i: 2299] avg mini-batch loss: 2.3289880645275116\n",
      "[epoch: 5, i: 2399] avg mini-batch loss: 2.3258644688129424\n",
      "[epoch: 5, i: 2499] avg mini-batch loss: 2.3238787615299223\n",
      "[epoch: 5, i: 2599] avg mini-batch loss: 2.379701610803604\n",
      "[epoch: 5, i: 2699] avg mini-batch loss: 2.3256554043293\n",
      "[epoch: 5, i: 2799] avg mini-batch loss: 2.398615061044693\n",
      "[epoch: 5, i: 2899] avg mini-batch loss: 2.351742833852768\n",
      "[epoch: 5, i: 2999] avg mini-batch loss: 2.3065314865112305\n",
      "[epoch: 5, i: 3099] avg mini-batch loss: 2.3865789270401\n",
      "[epoch: 5, i: 3199] avg mini-batch loss: 2.3533164834976197\n",
      "[epoch: 5, i: 3299] avg mini-batch loss: 2.369774646759033\n",
      "[epoch: 5, i: 3399] avg mini-batch loss: 2.366256527900696\n",
      "[epoch: 5, i: 3499] avg mini-batch loss: 2.3556829023361208\n",
      "[epoch: 5, i: 3599] avg mini-batch loss: 2.347323237657547\n",
      "[epoch: 5, i: 3699] avg mini-batch loss: 2.3870221722126006\n",
      "[epoch: 5, i: 3799] avg mini-batch loss: 2.358598346710205\n",
      "[epoch: 5, i: 3899] avg mini-batch loss: 2.3294050228595733\n",
      "[epoch: 5, i: 3999] avg mini-batch loss: 2.3012886810302735\n",
      "[epoch: 5, i: 4099] avg mini-batch loss: 2.349216980934143\n",
      "[epoch: 5, i: 4199] avg mini-batch loss: 2.3033653926849365\n",
      "[epoch: 5, i: 4299] avg mini-batch loss: 2.371977747678757\n",
      "[epoch: 5, i: 4399] avg mini-batch loss: 2.3326662266254425\n",
      "[epoch: 5, i: 4499] avg mini-batch loss: 2.380591106414795\n",
      "[epoch: 5, i: 4599] avg mini-batch loss: 2.3317468750476835\n",
      "[epoch: 5, i: 4699] avg mini-batch loss: 2.3624213457107546\n",
      "[epoch: 5, i: 4799] avg mini-batch loss: 2.3122277522087096\n",
      "[epoch: 5, i: 4899] avg mini-batch loss: 2.360071486234665\n",
      "[epoch: 5, i: 4999] avg mini-batch loss: 2.3842485439777374\n",
      "[epoch: 5, i: 5099] avg mini-batch loss: 2.3242328894138335\n",
      "[epoch: 5, i: 5199] avg mini-batch loss: 2.3497119474411012\n",
      "[epoch: 5, i: 5299] avg mini-batch loss: 2.3400681233406067\n",
      "[epoch: 5, i: 5399] avg mini-batch loss: 2.321007671356201\n",
      "[epoch: 5, i: 5499] avg mini-batch loss: 2.373861479759216\n",
      "[epoch: 5, i: 5599] avg mini-batch loss: 2.372112749814987\n",
      "[epoch: 5, i: 5699] avg mini-batch loss: 2.365561828613281\n",
      "[epoch: 5, i: 5799] avg mini-batch loss: 2.3653340983390807\n",
      "[epoch: 5, i: 5899] avg mini-batch loss: 2.3536643981933594\n",
      "[epoch: 5, i: 5999] avg mini-batch loss: 2.322140986919403\n",
      "[epoch: 5, i: 6099] avg mini-batch loss: 2.3827801966667175\n",
      "[epoch: 5, i: 6199] avg mini-batch loss: 2.3704184532165526\n",
      "[epoch: 5, i: 6299] avg mini-batch loss: 2.322446963787079\n",
      "[epoch: 5, i: 6399] avg mini-batch loss: 2.356824816465378\n",
      "[epoch: 5, i: 6499] avg mini-batch loss: 2.287586432695389\n",
      "[epoch: 5, i: 6599] avg mini-batch loss: 2.3092529869079588\n",
      "[epoch: 5, i: 6699] avg mini-batch loss: 2.327360244989395\n",
      "[epoch: 5, i: 6799] avg mini-batch loss: 2.3149233973026275\n",
      "[epoch: 5, i: 6899] avg mini-batch loss: 2.3552094435691835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 5, i: 6999] avg mini-batch loss: 2.3644348645210265\n",
      "[epoch: 5, i: 7099] avg mini-batch loss: 2.3898969721794128\n",
      "[epoch: 5, i: 7199] avg mini-batch loss: 2.3269088459014893\n",
      "[epoch: 5, i: 7299] avg mini-batch loss: 2.3329247033596037\n",
      "[epoch: 5, i: 7399] avg mini-batch loss: 2.2856049191951753\n",
      "[epoch: 5, i: 7499] avg mini-batch loss: 2.304569422006607\n",
      "[epoch: 5, i: 7599] avg mini-batch loss: 2.403701719045639\n",
      "[epoch: 5, i: 7699] avg mini-batch loss: 2.380055112838745\n",
      "[epoch: 5, i: 7799] avg mini-batch loss: 2.3776854836940764\n",
      "[epoch: 5, i: 7899] avg mini-batch loss: 2.376985387802124\n",
      "[epoch: 5, i: 7999] avg mini-batch loss: 2.357366272211075\n",
      "[epoch: 5, i: 8099] avg mini-batch loss: 2.341524577140808\n",
      "[epoch: 5, i: 8199] avg mini-batch loss: 2.2775875508785246\n",
      "[epoch: 5, i: 8299] avg mini-batch loss: 2.3706978344917298\n",
      "[epoch: 5, i: 8399] avg mini-batch loss: 2.3478330636024474\n",
      "[epoch: 5, i: 8499] avg mini-batch loss: 2.3455031371116637\n",
      "[epoch: 5, i: 8599] avg mini-batch loss: 2.3063983690738676\n",
      "[epoch: 5, i: 8699] avg mini-batch loss: 2.340960154533386\n",
      "[epoch: 5, i: 8799] avg mini-batch loss: 2.346031889915466\n",
      "[epoch: 5, i: 8899] avg mini-batch loss: 2.257108236551285\n",
      "[epoch: 5, i: 8999] avg mini-batch loss: 2.356665437221527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3418, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-57-f5e2f5ca8a5c>\", line 3, in <module>\n",
      "    for i, G in enumerate(graph_trainloader, 0):\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 363, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 403, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n",
      "    return self.collate_fn(data)\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/torch_geometric/data/dataloader.py\", line 34, in __call__\n",
      "    return self.collate(batch)\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/torch_geometric/data/dataloader.py\", line 15, in collate\n",
      "    return Batch.from_data_list(batch, self.follow_batch)\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/torch_geometric/data/batch.py\", line 150, in from_data_list\n",
      "    return batch.contiguous()\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/torch_geometric/data/data.py\", line 326, in contiguous\n",
      "    return self.apply(lambda x: x.contiguous(), *keys)\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/torch_geometric/data/data.py\", line 318, in apply\n",
      "    for key, item in self(*keys):\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/torch_geometric/data/data.py\", line 157, in __call__\n",
      "    if key in self:\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/torch_geometric/data/data.py\", line 143, in __contains__\n",
      "    return key in self.keys\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/torch_geometric/data/data.py\", line 132, in keys\n",
      "    keys = [key for key in self.__dict__.keys() if self[key] is not None]\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/torch_geometric/data/data.py\", line 132, in <listcomp>\n",
      "    keys = [key for key in self.__dict__.keys() if self[key] is not None]\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/torch_geometric/data/data.py\", line 121, in __getitem__\n",
      "    def __getitem__(self, key):\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2045, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1170, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/posixpath.py\", line 395, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/posixpath.py\", line 429, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/posixpath.py\", line 171, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3418, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-57-f5e2f5ca8a5c>\", line 3, in <module>\n",
      "    for i, G in enumerate(graph_trainloader, 0):\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 363, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 403, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n",
      "    return self.collate_fn(data)\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/torch_geometric/data/dataloader.py\", line 34, in __call__\n",
      "    return self.collate(batch)\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/torch_geometric/data/dataloader.py\", line 15, in collate\n",
      "    return Batch.from_data_list(batch, self.follow_batch)\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/torch_geometric/data/batch.py\", line 150, in from_data_list\n",
      "    return batch.contiguous()\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/torch_geometric/data/data.py\", line 326, in contiguous\n",
      "    return self.apply(lambda x: x.contiguous(), *keys)\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/torch_geometric/data/data.py\", line 318, in apply\n",
      "    for key, item in self(*keys):\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/torch_geometric/data/data.py\", line 157, in __call__\n",
      "    if key in self:\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/torch_geometric/data/data.py\", line 143, in __contains__\n",
      "    return key in self.keys\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/torch_geometric/data/data.py\", line 132, in keys\n",
      "    keys = [key for key in self.__dict__.keys() if self[key] is not None]\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/torch_geometric/data/data.py\", line 132, in <listcomp>\n",
      "    keys = [key for key in self.__dict__.keys() if self[key] is not None]\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/torch_geometric/data/data.py\", line 121, in __getitem__\n",
      "    def __getitem__(self, key):\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2045, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3338, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3435, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2048, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1437, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1337, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1194, in structured_traceback\n",
      "    tb_offset)\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1151, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 451, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2045, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1170, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/inspect.py\", line 739, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/inspect.py\", line 708, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/inspect.py\", line 693, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3418, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-57-f5e2f5ca8a5c>\", line 3, in <module>\n",
      "    for i, G in enumerate(graph_trainloader, 0):\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 363, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 403, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n",
      "    return self.collate_fn(data)\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/torch_geometric/data/dataloader.py\", line 34, in __call__\n",
      "    return self.collate(batch)\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/torch_geometric/data/dataloader.py\", line 15, in collate\n",
      "    return Batch.from_data_list(batch, self.follow_batch)\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/torch_geometric/data/batch.py\", line 150, in from_data_list\n",
      "    return batch.contiguous()\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/torch_geometric/data/data.py\", line 326, in contiguous\n",
      "    return self.apply(lambda x: x.contiguous(), *keys)\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/torch_geometric/data/data.py\", line 318, in apply\n",
      "    for key, item in self(*keys):\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/torch_geometric/data/data.py\", line 157, in __call__\n",
      "    if key in self:\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/torch_geometric/data/data.py\", line 143, in __contains__\n",
      "    return key in self.keys\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/torch_geometric/data/data.py\", line 132, in keys\n",
      "    keys = [key for key in self.__dict__.keys() if self[key] is not None]\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/torch_geometric/data/data.py\", line 132, in <listcomp>\n",
      "    keys = [key for key in self.__dict__.keys() if self[key] is not None]\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/torch_geometric/data/data.py\", line 121, in __getitem__\n",
      "    def __getitem__(self, key):\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2045, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3338, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3435, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2048, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1437, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1337, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1194, in structured_traceback\n",
      "    tb_offset)\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1151, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 451, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2045, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2923, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3147, in run_cell_async\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3357, in run_ast_nodes\n",
      "    self.showtraceback()\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2048, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1437, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1337, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1212, in structured_traceback\n",
      "    chained_exceptions_tb_offset)\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1151, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 451, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2045, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1170, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/inspect.py\", line 739, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/inspect.py\", line 708, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/inspect.py\", line 693, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, G in enumerate(graph_trainloader, 0):\n",
    "        y=G.y#.cuda()\n",
    "        z=G.x#.cuda()\n",
    "        edge_index=G.edge_index#.cuda()\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        \n",
    "        outputs = model_gcn(z, edge_index)\n",
    "        loss = loss_func(outputs, y)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        opt.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if i % print_freq == print_freq - 1:\n",
    "            avg_loss = running_loss / print_freq\n",
    "            print(f'[epoch: {epoch}, i: {i}] avg mini-batch loss: {avg_loss}')\n",
    "            avg_losses.append(avg_loss)\n",
    "            running_loss = 0.0\n",
    "        \n",
    "#         y_pred=model_gcn(z,edge_index)\n",
    "#         print(y_pred.argmax(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at ../src/models/resnet50_graph_80_10_3.pth\n"
     ]
    }
   ],
   "source": [
    "PATH = f'../src/models/{fname}.pth'\n",
    "torch.save(model_gcn.state_dict(), PATH)#, _use_new_zipfile_serialization=False)\n",
    "print(f'Model saved at {PATH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "575"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(avg_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(avg_losses).to_csv(f'../data/out/avg_losses_{fname}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in epochs:\n",
    "#     for G in graph_dataloader:\n",
    "#         y=G.y.cuda()\n",
    "#         z=G.x.cuda()\n",
    "#         edge_index=G.edge_index.cuda()\n",
    "#         y_pred=model_gcn(z,edge_index)\n",
    "#         print(y_pred.argmax(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchgeo",
   "language": "python",
   "name": "torchgeo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
