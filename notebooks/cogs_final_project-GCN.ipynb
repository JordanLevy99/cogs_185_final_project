{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch_geometric\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.insert(0, '../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import json\n",
    "import argparse\n",
    "# import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Function\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from torchvision.datasets import CocoDetection\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.insert(0, '../src/data/cocoapi/PythonAPI')\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "from graph_network import GCNNet\n",
    "from torch_geometric.data import DataLoader as TG_DataLoader\n",
    "\n",
    "from dataset_class import *\n",
    "from etl import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: add target transform to convert labels to proper labels using id_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = '/datasets/COCO-2017/train2017'\n",
    "val_dir = '/datasets/COCO-2017/val2017'\n",
    "\n",
    "train_ann = '/datasets/COCO-2017/anno2017/instances_train2017.json'\n",
    "val_ann = '/datasets/COCO-2017/anno2017/instances_val2017.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../src/data/id_dict.pkl', 'wb') as f:\n",
    "#     pickle.dump(id_dict, f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../src/data/id_dict.pkl', 'rb') as f:\n",
    "    id_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_ids(target):\n",
    "    return id_dict[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %%capture\n",
    "# train_coco, train_data = load_data(train_ann)\n",
    "# %time drop_null_annotations(coco=train_coco, data=train_data, dataDir='/datasets/COCO-2017', dataType='train2017',\\\n",
    "#                             annFile=train_ann, overwrite=True, tmpDataDir='../data/temp');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del train_coco\n",
    "# del train_data\n",
    "\n",
    "# del val_coco\n",
    "# del val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train_ann = '../data/temp/annotations/clean_instances_train2017.json'\n",
    "clean_val_ann = '../data/temp/annotations/clean_instances_val2017.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=17.55s)\n",
      "creating index...\n",
      "index created!\n",
      "CPU times: user 16.6 s, sys: 2.44 s, total: 19 s\n",
      "Wall time: 18.9 s\n",
      "loading annotations into memory...\n",
      "Done (t=0.65s)\n",
      "creating index...\n",
      "index created!\n",
      "CPU times: user 606 ms, sys: 81.9 ms, total: 688 ms\n",
      "Wall time: 686 ms\n"
     ]
    }
   ],
   "source": [
    "%time train_set = cocoDataset(root=train_dir,\\\n",
    "                      annotation=clean_train_ann,\\\n",
    "                      transforms=get_transform(),\\\n",
    "                      target_transform=convert_ids)\n",
    "\n",
    "%time val_set = cocoDataset(root=val_dir,\\\n",
    "                      annotation=clean_val_ann,\\\n",
    "                      transforms=get_transform(),\\\n",
    "                      target_transform=convert_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Params cell\n",
    "num_epoch = 10\n",
    "k = 3\n",
    "# fname = f'graph_80_{num_epoch}_{k}_resnet50'\n",
    "train_batch_size = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(train_set,\n",
    "                                          batch_size=train_batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=8,\n",
    "                                          collate_fn=collate_fn)\n",
    "\n",
    "valloader = torch.utils.data.DataLoader(val_set,\n",
    "                                        batch_size=train_batch_size,\n",
    "                                        shuffle=False,\n",
    "                                        num_workers=8,\n",
    "                                        collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our model for GCN embeddings\n",
    "model = models.resnet50(pretrained=False)\n",
    "state_dict=torch.load(\"../src/models/COCO_80_10_resnet50.pth\")\n",
    "model.fc = nn.Linear(2048, 80)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "model.fc=nn.Flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "import tqdm\n",
    "from torch_cluster import knn_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9773it [12:26, 13.09it/s]                            \n"
     ]
    }
   ],
   "source": [
    "Idx=[]\n",
    "Z=[]\n",
    "XY=[]\n",
    "Y=[]\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for i,(imgs,xy,y,idx) in tqdm.tqdm(enumerate(trainloader),total=len(trainloader.dataset)//trainloader.batch_size):\n",
    "#         print(imgs.shape,xy.shape,y.shape)\n",
    "#         edge_index=knn_graph(xy,k=5)\n",
    "        imgs=imgs.cuda()\n",
    "        z=model(imgs).cpu()\n",
    "        Z.append(z)\n",
    "        XY.append(xy)\n",
    "        Y.append(y)\n",
    "        Idx.append(idx)\n",
    "        \n",
    "#         graphs.append(Data(x=z,pos=xy,edge_index=edge_index,y=y))\n",
    "    #     print(graphs[i].x.shape)\n",
    "#         if i==100: break\n",
    "        del imgs, z, xy, y, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "del trainloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z=torch.cat(Z,0)\n",
    "XY=torch.cat(XY,0)\n",
    "Y=torch.cat(Y).flatten()\n",
    "# Idx=torch.cat(Idx).flatten().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Idx=np.hstack(Idx).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([101160,  36811,  36811, ...,  74517,  74517,  74517])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 3\n"
     ]
    }
   ],
   "source": [
    "print(f'k = {k}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 117266/117266 [16:26<00:00, 118.82it/s]\n"
     ]
    }
   ],
   "source": [
    "train_graphs=[]\n",
    "\n",
    "for idx in tqdm.tqdm(np.unique(Idx)):\n",
    "    select=(Idx==idx)\n",
    "    z=Z[select]\n",
    "    xy=XY[select]\n",
    "    y=Y[select]\n",
    "    edge_index=knn_graph(xy,k=k)\n",
    "    train_graphs.append(Data(x=z,pos=xy,edge_index=edge_index,y=y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "413it [00:34, 11.92it/s]                         \n"
     ]
    }
   ],
   "source": [
    "val_graphs=[]\n",
    "Idx=[]\n",
    "Z=[]\n",
    "XY=[]\n",
    "Y=[]\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for i,(imgs,xy,y,idx) in tqdm.tqdm(enumerate(valloader),total=len(valloader.dataset)//valloader.batch_size):\n",
    "#         print(imgs.shape,xy.shape,y.shape)\n",
    "#         edge_index=knn_graph(xy,k=5)\n",
    "        imgs=imgs.cuda()\n",
    "        z=model(imgs).cpu() # the 2048-dimensional embeddings provided by resnet\n",
    "        Z.append(z)\n",
    "        XY.append(xy)\n",
    "        Y.append(y)\n",
    "        Idx.append(idx)\n",
    "        \n",
    "#         graphs.append(Data(x=z,pos=xy,edge_index=edge_index,y=y))\n",
    "    #     print(graphs[i].x.shape)\n",
    "#         if i==100: break\n",
    "        del imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z=torch.cat(Z,0)\n",
    "XY=torch.cat(XY,0)\n",
    "Y=torch.cat(Y).flatten()\n",
    "Idx=np.hstack(Idx).flatten()#torch.cat(Idx).flatten().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4952/4952 [00:12<00:00, 381.26it/s]\n"
     ]
    }
   ],
   "source": [
    "for idx in tqdm.tqdm(np.unique(Idx)):\n",
    "    select=(Idx==idx)\n",
    "    z=Z[select]\n",
    "    xy=XY[select]\n",
    "    y=Y[select]\n",
    "    edge_index=knn_graph(xy,k=k)\n",
    "    val_graphs.append(Data(x=z,pos=xy,edge_index=edge_index,y=y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "del valloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COCO or resnet50 graph embeddings? (coco/resnet50) coco\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mod_select = input('COCO or resnet50 graph embeddings? (coco/resnet50) ')\n",
    "# if mod_select == 'coco':\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are you sure you want to overwrite ../data/temp/coco_train_graphs_k_3.pkl? ([y],n)y\n",
      "CPU times: user 28.8 s, sys: 13.4 s, total: 42.2 s\n",
      "Wall time: 1min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "### For writing new train graphs\n",
    "pickle_name = f'../data/temp/{mod_select}_train_graphs_k_{k}.pkl'\n",
    "ans = input(f'Are you sure you want to overwrite {pickle_name}? ([y],n)')\n",
    "if ans == 'y' or ans=='':\n",
    "    with open(pickle_name, 'wb') as f:\n",
    "    #     val_graphs = pickle.load(f)\n",
    "        pickle.dump(train_graphs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are you sure you want to overwrite ../data/temp/coco_val_graphs_k_3.pkl? ([y],n)y\n",
      "CPU times: user 1.17 s, sys: 436 ms, total: 1.61 s\n",
      "Wall time: 5.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "### For writing new val graphs\n",
    "val_pickle_name = f'../data/temp/{mod_select}_val_graphs_k_{k}.pkl'\n",
    "\n",
    "ans = input(f'Are you sure you want to overwrite {val_pickle_name}? ([y],n)')\n",
    "if ans == 'y' or ans =='':\n",
    "    with open(f'../data/temp/{mod_select}_val_graphs_k_{k}.pkl', 'wb') as f:\n",
    "    #     val_graphs = pickle.load(f)\n",
    "        pickle.dump(val_graphs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/temp/coco_train_graphs_k_3.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/temp/coco_train_graphs_k_3.pkl'"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "### For reading train_graphs\n",
    "with open(f'../data/temp/{mod_select}_train_graphs_k_{k}.pkl', 'rb') as f:\n",
    "    train_graphs = pickle.load(f)\n",
    "#     pickle.dump(val_graphs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.83 s, sys: 412 ms, total: 2.24 s\n",
      "Wall time: 2.23 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "### For reading val_graphs\n",
    "with open(f'../data/temp/{mod_select}_val_graphs_k_{k}.pkl', 'rb') as f:\n",
    "    val_graphs = pickle.load(f)\n",
    "#     pickle.dump(val_graphs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import DataLoader as TG_DataLoader\n",
    "# https://pytorch.org/docs/stable/data.html#torch.utils.data.Subset\n",
    "# Batch size should be the same as batch size for dataloader above\n",
    "graph_trainloader = TG_DataLoader(train_graphs, batch_size=train_batch_size, shuffle=True)\n",
    "\n",
    "graph_valloader=TG_DataLoader(val_graphs,batch_size=train_batch_size,shuffle=True)#[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/jlevy44/WSI-GTFE/blob/master/notebooks/3_fit_gnn_model.ipynb\n",
    "import torch, torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv, GATConv, DeepGraphInfomax, SAGEConv\n",
    "from torch_geometric.nn import DenseGraphConv\n",
    "from torch_geometric.utils import to_dense_batch, to_dense_adj, dense_to_sparse\n",
    "from torch_geometric.nn import GINEConv\n",
    "from torch_geometric.utils import dropout_adj\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "class GCNNet(torch.nn.Module):\n",
    "    def __init__(self, inp_dim, out_dim, hidden_topology=[32,64,128,128], p=0.2, p2=0.0, drop_each=True):\n",
    "        super(GCNNet, self).__init__()\n",
    "        self.out_dim=out_dim\n",
    "        self.convs = nn.ModuleList([GATConv(inp_dim, hidden_topology[0])]+[GATConv(hidden_topology[i],hidden_topology[i+1]) for i in range(len(hidden_topology[:-1]))])\n",
    "        self.drop_edge = lambda edge_index: dropout_adj(edge_index,p=p2)[0]\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.fc = nn.Linear(hidden_topology[-1], out_dim)\n",
    "        self.drop_each=drop_each\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr=None, return_attention=False):\n",
    "        attention_weights=[]\n",
    "        for conv in self.convs:\n",
    "            if self.drop_each and self.training: edge_index=self.drop_edge(edge_index)\n",
    "            x, attention = conv(x, edge_index, edge_attr,return_attention_weights=True)\n",
    "            x = F.relu(x)\n",
    "            attention_weights.append(attention)\n",
    "        if self.training:\n",
    "            x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        if return_attention: return x, attention_weights\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gcn=GCNNet(2048,80, [32]*2)\n",
    "model_mlp=nn.Sequential(nn.Sequential(nn.Linear(2048,32),nn.ReLU(),nn.Dropout(p=0.3)),nn.Linear(32,80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_gcn.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there are GPUs, choose the first one for computing. Otherwise use CPU.\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(device)\n",
    "# If 'cuda:0' is printed, it means GPU is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_gcn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'coco_graph_80_10_3'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Params Cell\n",
    "k = 3\n",
    "epochs = 10\n",
    "fname = f'{mod_select}_graph_80_{epochs}_{k}'\n",
    "batch_size=12\n",
    "fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: implement edge index as part of dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "opt = optim.SGD(model_gcn.parameters(), lr=0.0001, momentum=0.9)\n",
    "avg_losses = []\n",
    "print_freq = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 0, i: 99] avg mini-batch loss: 4.307699842453003\n",
      "[epoch: 0, i: 199] avg mini-batch loss: 4.132729833126068\n",
      "[epoch: 0, i: 299] avg mini-batch loss: 3.881756374835968\n",
      "[epoch: 0, i: 399] avg mini-batch loss: 3.745609750747681\n",
      "[epoch: 0, i: 499] avg mini-batch loss: 3.6838740944862365\n",
      "[epoch: 0, i: 599] avg mini-batch loss: 3.5968668389320375\n",
      "[epoch: 0, i: 699] avg mini-batch loss: 3.5303693866729735\n",
      "[epoch: 0, i: 799] avg mini-batch loss: 3.448225076198578\n",
      "[epoch: 0, i: 899] avg mini-batch loss: 3.579165117740631\n",
      "[epoch: 0, i: 999] avg mini-batch loss: 3.4674167132377622\n",
      "[epoch: 0, i: 1099] avg mini-batch loss: 3.486649694442749\n",
      "[epoch: 0, i: 1199] avg mini-batch loss: 3.4736986637115477\n",
      "[epoch: 0, i: 1299] avg mini-batch loss: 3.4105195665359496\n",
      "[epoch: 0, i: 1399] avg mini-batch loss: 3.431011264324188\n",
      "[epoch: 0, i: 1499] avg mini-batch loss: 3.4430779242515563\n",
      "[epoch: 0, i: 1599] avg mini-batch loss: 3.3659687352180483\n",
      "[epoch: 0, i: 1699] avg mini-batch loss: 3.3959333658218385\n",
      "[epoch: 0, i: 1799] avg mini-batch loss: 3.4097610664367677\n",
      "[epoch: 0, i: 1899] avg mini-batch loss: 3.372514555454254\n",
      "[epoch: 0, i: 1999] avg mini-batch loss: 3.39471129655838\n",
      "[epoch: 0, i: 2099] avg mini-batch loss: 3.3107459783554076\n",
      "[epoch: 0, i: 2199] avg mini-batch loss: 3.3899014019966125\n",
      "[epoch: 0, i: 2299] avg mini-batch loss: 3.2903053689002992\n",
      "[epoch: 0, i: 2399] avg mini-batch loss: 3.355887007713318\n",
      "[epoch: 0, i: 2499] avg mini-batch loss: 3.228504948616028\n",
      "[epoch: 0, i: 2599] avg mini-batch loss: 3.31031409740448\n",
      "[epoch: 0, i: 2699] avg mini-batch loss: 3.280448613166809\n",
      "[epoch: 0, i: 2799] avg mini-batch loss: 3.2629758167266845\n",
      "[epoch: 0, i: 2899] avg mini-batch loss: 3.285206925868988\n",
      "[epoch: 0, i: 2999] avg mini-batch loss: 3.316383411884308\n",
      "[epoch: 0, i: 3099] avg mini-batch loss: 3.3560207200050356\n",
      "[epoch: 0, i: 3199] avg mini-batch loss: 3.3303668451309205\n",
      "[epoch: 0, i: 3299] avg mini-batch loss: 3.3019188117980955\n",
      "[epoch: 0, i: 3399] avg mini-batch loss: 3.150844166278839\n",
      "[epoch: 0, i: 3499] avg mini-batch loss: 3.2518021631240845\n",
      "[epoch: 0, i: 3599] avg mini-batch loss: 3.2644078445434572\n",
      "[epoch: 0, i: 3699] avg mini-batch loss: 3.2102870035171507\n",
      "[epoch: 0, i: 3799] avg mini-batch loss: 3.1973537015914917\n",
      "[epoch: 0, i: 3899] avg mini-batch loss: 3.1708204448223114\n",
      "[epoch: 0, i: 3999] avg mini-batch loss: 3.2091359448432923\n",
      "[epoch: 0, i: 4099] avg mini-batch loss: 3.2074916529655457\n",
      "[epoch: 0, i: 4199] avg mini-batch loss: 3.1999303102493286\n",
      "[epoch: 0, i: 4299] avg mini-batch loss: 3.210535695552826\n",
      "[epoch: 0, i: 4399] avg mini-batch loss: 3.144431871175766\n",
      "[epoch: 0, i: 4499] avg mini-batch loss: 3.0975880098342894\n",
      "[epoch: 0, i: 4599] avg mini-batch loss: 3.0992269945144653\n",
      "[epoch: 0, i: 4699] avg mini-batch loss: 3.154365804195404\n",
      "[epoch: 0, i: 4799] avg mini-batch loss: 3.1006101155281067\n",
      "[epoch: 0, i: 4899] avg mini-batch loss: 3.1472918462753294\n",
      "[epoch: 0, i: 4999] avg mini-batch loss: 3.111969931125641\n",
      "[epoch: 0, i: 5099] avg mini-batch loss: 3.1074424958229065\n",
      "[epoch: 0, i: 5199] avg mini-batch loss: 3.1081155729293823\n",
      "[epoch: 0, i: 5299] avg mini-batch loss: 3.098675084114075\n",
      "[epoch: 0, i: 5399] avg mini-batch loss: 3.105091962814331\n",
      "[epoch: 0, i: 5499] avg mini-batch loss: 3.053825123310089\n",
      "[epoch: 0, i: 5599] avg mini-batch loss: 3.1212913894653322\n",
      "[epoch: 0, i: 5699] avg mini-batch loss: 3.0050387167930603\n",
      "[epoch: 0, i: 5799] avg mini-batch loss: 3.0328964686393736\n",
      "[epoch: 0, i: 5899] avg mini-batch loss: 3.083109829425812\n",
      "[epoch: 0, i: 5999] avg mini-batch loss: 3.035939302444458\n",
      "[epoch: 0, i: 6099] avg mini-batch loss: 3.0641788697242736\n",
      "[epoch: 0, i: 6199] avg mini-batch loss: 3.040577161312103\n",
      "[epoch: 0, i: 6299] avg mini-batch loss: 2.9885021376609804\n",
      "[epoch: 0, i: 6399] avg mini-batch loss: 2.947785038948059\n",
      "[epoch: 0, i: 6499] avg mini-batch loss: 3.059410810470581\n",
      "[epoch: 0, i: 6599] avg mini-batch loss: 2.9221645188331604\n",
      "[epoch: 0, i: 6699] avg mini-batch loss: 2.9521613049507143\n",
      "[epoch: 0, i: 6799] avg mini-batch loss: 2.9980849587917326\n",
      "[epoch: 0, i: 6899] avg mini-batch loss: 2.941127293109894\n",
      "[epoch: 0, i: 6999] avg mini-batch loss: 2.9228132653236387\n",
      "[epoch: 0, i: 7099] avg mini-batch loss: 2.964040448665619\n",
      "[epoch: 0, i: 7199] avg mini-batch loss: 2.969596049785614\n",
      "[epoch: 0, i: 7299] avg mini-batch loss: 2.911676561832428\n",
      "[epoch: 0, i: 7399] avg mini-batch loss: 2.9463258886337282\n",
      "[epoch: 0, i: 7499] avg mini-batch loss: 2.9124593794345857\n",
      "[epoch: 0, i: 7599] avg mini-batch loss: 2.916220589876175\n",
      "[epoch: 0, i: 7699] avg mini-batch loss: 2.8789217472076416\n",
      "[epoch: 0, i: 7799] avg mini-batch loss: 2.8513776993751527\n",
      "[epoch: 0, i: 7899] avg mini-batch loss: 2.856944452524185\n",
      "[epoch: 0, i: 7999] avg mini-batch loss: 2.896110417842865\n",
      "[epoch: 0, i: 8099] avg mini-batch loss: 2.8656516599655153\n",
      "[epoch: 0, i: 8199] avg mini-batch loss: 2.8692696642875672\n",
      "[epoch: 0, i: 8299] avg mini-batch loss: 2.877878675460815\n",
      "[epoch: 0, i: 8399] avg mini-batch loss: 2.9287766647338866\n",
      "[epoch: 0, i: 8499] avg mini-batch loss: 2.8350919699668884\n",
      "[epoch: 0, i: 8599] avg mini-batch loss: 2.8703129291534424\n",
      "[epoch: 0, i: 8699] avg mini-batch loss: 2.803434100151062\n",
      "[epoch: 0, i: 8799] avg mini-batch loss: 2.780652297735214\n",
      "[epoch: 0, i: 8899] avg mini-batch loss: 2.864490923881531\n",
      "[epoch: 0, i: 8999] avg mini-batch loss: 2.8038736844062804\n",
      "[epoch: 0, i: 9099] avg mini-batch loss: 2.8261639213562013\n",
      "[epoch: 0, i: 9199] avg mini-batch loss: 2.7511189913749696\n",
      "[epoch: 0, i: 9299] avg mini-batch loss: 2.82065486907959\n",
      "[epoch: 0, i: 9399] avg mini-batch loss: 2.801278553009033\n",
      "[epoch: 0, i: 9499] avg mini-batch loss: 2.7530514788627625\n",
      "[epoch: 0, i: 9599] avg mini-batch loss: 2.750916483402252\n",
      "[epoch: 0, i: 9699] avg mini-batch loss: 2.723495934009552\n",
      "[epoch: 1, i: 99] avg mini-batch loss: 2.8299249100685118\n",
      "[epoch: 1, i: 199] avg mini-batch loss: 2.682574574947357\n",
      "[epoch: 1, i: 299] avg mini-batch loss: 2.7682910537719727\n",
      "[epoch: 1, i: 399] avg mini-batch loss: 2.7415647542476655\n",
      "[epoch: 1, i: 499] avg mini-batch loss: 2.6635536479949953\n",
      "[epoch: 1, i: 599] avg mini-batch loss: 2.697132575511932\n",
      "[epoch: 1, i: 699] avg mini-batch loss: 2.693035340309143\n",
      "[epoch: 1, i: 799] avg mini-batch loss: 2.689481303691864\n",
      "[epoch: 1, i: 899] avg mini-batch loss: 2.688352428674698\n",
      "[epoch: 1, i: 999] avg mini-batch loss: 2.696441043615341\n",
      "[epoch: 1, i: 1099] avg mini-batch loss: 2.6124801588058473\n",
      "[epoch: 1, i: 1199] avg mini-batch loss: 2.6792165887355806\n",
      "[epoch: 1, i: 1299] avg mini-batch loss: 2.6709321880340577\n",
      "[epoch: 1, i: 1399] avg mini-batch loss: 2.6342264580726624\n",
      "[epoch: 1, i: 1499] avg mini-batch loss: 2.6574382722377776\n",
      "[epoch: 1, i: 1599] avg mini-batch loss: 2.642450877428055\n",
      "[epoch: 1, i: 1699] avg mini-batch loss: 2.6411645460128783\n",
      "[epoch: 1, i: 1799] avg mini-batch loss: 2.6291942405700683\n",
      "[epoch: 1, i: 1899] avg mini-batch loss: 2.5838438129425048\n",
      "[epoch: 1, i: 1999] avg mini-batch loss: 2.586550872325897\n",
      "[epoch: 1, i: 2099] avg mini-batch loss: 2.5839901196956636\n",
      "[epoch: 1, i: 2199] avg mini-batch loss: 2.6116834247112273\n",
      "[epoch: 1, i: 2299] avg mini-batch loss: 2.5969184708595274\n",
      "[epoch: 1, i: 2399] avg mini-batch loss: 2.56570068359375\n",
      "[epoch: 1, i: 2499] avg mini-batch loss: 2.535562480688095\n",
      "[epoch: 1, i: 2599] avg mini-batch loss: 2.565128586292267\n",
      "[epoch: 1, i: 2699] avg mini-batch loss: 2.5863441026210783\n",
      "[epoch: 1, i: 2799] avg mini-batch loss: 2.5909363341331484\n",
      "[epoch: 1, i: 2899] avg mini-batch loss: 2.617181168794632\n",
      "[epoch: 1, i: 2999] avg mini-batch loss: 2.5219840025901794\n",
      "[epoch: 1, i: 3099] avg mini-batch loss: 2.4684586787223814\n",
      "[epoch: 1, i: 3199] avg mini-batch loss: 2.507691432237625\n",
      "[epoch: 1, i: 3299] avg mini-batch loss: 2.541989767551422\n",
      "[epoch: 1, i: 3399] avg mini-batch loss: 2.530620433092117\n",
      "[epoch: 1, i: 3499] avg mini-batch loss: 2.5245063281059266\n",
      "[epoch: 1, i: 3599] avg mini-batch loss: 2.464303011894226\n",
      "[epoch: 1, i: 3699] avg mini-batch loss: 2.4923227858543395\n",
      "[epoch: 1, i: 3799] avg mini-batch loss: 2.482843869924545\n",
      "[epoch: 1, i: 3899] avg mini-batch loss: 2.5203599524497986\n",
      "[epoch: 1, i: 3999] avg mini-batch loss: 2.5000319409370424\n",
      "[epoch: 1, i: 4099] avg mini-batch loss: 2.448413290977478\n",
      "[epoch: 1, i: 4199] avg mini-batch loss: 2.4650605618953705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 1, i: 4299] avg mini-batch loss: 2.5003641128540037\n",
      "[epoch: 1, i: 4399] avg mini-batch loss: 2.4629008948802946\n",
      "[epoch: 1, i: 4499] avg mini-batch loss: 2.400014326572418\n",
      "[epoch: 1, i: 4599] avg mini-batch loss: 2.410565044879913\n",
      "[epoch: 1, i: 4699] avg mini-batch loss: 2.4378290355205534\n",
      "[epoch: 1, i: 4799] avg mini-batch loss: 2.365341703891754\n",
      "[epoch: 1, i: 4899] avg mini-batch loss: 2.394202609062195\n",
      "[epoch: 1, i: 4999] avg mini-batch loss: 2.4019194424152372\n",
      "[epoch: 1, i: 5099] avg mini-batch loss: 2.421831772327423\n",
      "[epoch: 1, i: 5199] avg mini-batch loss: 2.399269113540649\n",
      "[epoch: 1, i: 5299] avg mini-batch loss: 2.4347802114486696\n",
      "[epoch: 1, i: 5399] avg mini-batch loss: 2.358822467327118\n",
      "[epoch: 1, i: 5499] avg mini-batch loss: 2.4424258947372435\n",
      "[epoch: 1, i: 5599] avg mini-batch loss: 2.4279038345813753\n",
      "[epoch: 1, i: 5699] avg mini-batch loss: 2.358283050060272\n",
      "[epoch: 1, i: 5799] avg mini-batch loss: 2.4015484285354614\n",
      "[epoch: 1, i: 5899] avg mini-batch loss: 2.378764932155609\n",
      "[epoch: 1, i: 5999] avg mini-batch loss: 2.386446887254715\n",
      "[epoch: 1, i: 6099] avg mini-batch loss: 2.3713100230693818\n",
      "[epoch: 1, i: 6199] avg mini-batch loss: 2.344296534061432\n",
      "[epoch: 1, i: 6299] avg mini-batch loss: 2.3659445488452913\n",
      "[epoch: 1, i: 6399] avg mini-batch loss: 2.311735246181488\n",
      "[epoch: 1, i: 6499] avg mini-batch loss: 2.369704463481903\n",
      "[epoch: 1, i: 6599] avg mini-batch loss: 2.331563149690628\n",
      "[epoch: 1, i: 6699] avg mini-batch loss: 2.307472177743912\n",
      "[epoch: 1, i: 6799] avg mini-batch loss: 2.360789660215378\n",
      "[epoch: 1, i: 6899] avg mini-batch loss: 2.31638985991478\n",
      "[epoch: 1, i: 6999] avg mini-batch loss: 2.305679211616516\n",
      "[epoch: 1, i: 7099] avg mini-batch loss: 2.291224149465561\n",
      "[epoch: 1, i: 7199] avg mini-batch loss: 2.3296176755428313\n",
      "[epoch: 1, i: 7299] avg mini-batch loss: 2.2861769926548003\n",
      "[epoch: 1, i: 7399] avg mini-batch loss: 2.3034052515029906\n",
      "[epoch: 1, i: 7499] avg mini-batch loss: 2.2863952994346617\n",
      "[epoch: 1, i: 7599] avg mini-batch loss: 2.2750280809402468\n",
      "[epoch: 1, i: 7699] avg mini-batch loss: 2.346806963682175\n",
      "[epoch: 1, i: 7799] avg mini-batch loss: 2.2842520451545716\n",
      "[epoch: 1, i: 7899] avg mini-batch loss: 2.2756269693374636\n",
      "[epoch: 1, i: 7999] avg mini-batch loss: 2.2990696632862093\n",
      "[epoch: 1, i: 8099] avg mini-batch loss: 2.2984029805660247\n",
      "[epoch: 1, i: 8199] avg mini-batch loss: 2.2427376449108123\n",
      "[epoch: 1, i: 8299] avg mini-batch loss: 2.3038693022727967\n",
      "[epoch: 1, i: 8399] avg mini-batch loss: 2.2596432256698606\n",
      "[epoch: 1, i: 8499] avg mini-batch loss: 2.3273854625225066\n",
      "[epoch: 1, i: 8599] avg mini-batch loss: 2.2792575097084047\n",
      "[epoch: 1, i: 8699] avg mini-batch loss: 2.2493571782112123\n",
      "[epoch: 1, i: 8799] avg mini-batch loss: 2.1872471976280212\n",
      "[epoch: 1, i: 8899] avg mini-batch loss: 2.2424132883548737\n",
      "[epoch: 1, i: 8999] avg mini-batch loss: 2.241105375289917\n",
      "[epoch: 1, i: 9099] avg mini-batch loss: 2.258607783317566\n",
      "[epoch: 1, i: 9199] avg mini-batch loss: 2.2413352036476137\n",
      "[epoch: 1, i: 9299] avg mini-batch loss: 2.2222655773162843\n",
      "[epoch: 1, i: 9399] avg mini-batch loss: 2.236574205160141\n",
      "[epoch: 1, i: 9499] avg mini-batch loss: 2.230286099910736\n",
      "[epoch: 1, i: 9599] avg mini-batch loss: 2.2599461424350737\n",
      "[epoch: 1, i: 9699] avg mini-batch loss: 2.226767545938492\n",
      "[epoch: 2, i: 99] avg mini-batch loss: 2.1957739901542666\n",
      "[epoch: 2, i: 199] avg mini-batch loss: 2.1978781139850616\n",
      "[epoch: 2, i: 299] avg mini-batch loss: 2.2120466160774233\n",
      "[epoch: 2, i: 399] avg mini-batch loss: 2.199299764633179\n",
      "[epoch: 2, i: 499] avg mini-batch loss: 2.199536896944046\n",
      "[epoch: 2, i: 599] avg mini-batch loss: 2.1469626140594484\n",
      "[epoch: 2, i: 699] avg mini-batch loss: 2.175525870323181\n",
      "[epoch: 2, i: 799] avg mini-batch loss: 2.176880555152893\n",
      "[epoch: 2, i: 899] avg mini-batch loss: 2.1720082223415376\n",
      "[epoch: 2, i: 999] avg mini-batch loss: 2.1605943095684053\n",
      "[epoch: 2, i: 1099] avg mini-batch loss: 2.182968989610672\n",
      "[epoch: 2, i: 1199] avg mini-batch loss: 2.117543361186981\n",
      "[epoch: 2, i: 1299] avg mini-batch loss: 2.1566333079338076\n",
      "[epoch: 2, i: 1399] avg mini-batch loss: 2.150185123682022\n",
      "[epoch: 2, i: 1499] avg mini-batch loss: 2.1677295708656312\n",
      "[epoch: 2, i: 1599] avg mini-batch loss: 2.131572126150131\n",
      "[epoch: 2, i: 1699] avg mini-batch loss: 2.170299756526947\n",
      "[epoch: 2, i: 1799] avg mini-batch loss: 2.1337885236740113\n",
      "[epoch: 2, i: 1899] avg mini-batch loss: 2.1626659286022187\n",
      "[epoch: 2, i: 1999] avg mini-batch loss: 2.1360731983184813\n",
      "[epoch: 2, i: 2099] avg mini-batch loss: 2.137458072900772\n",
      "[epoch: 2, i: 2199] avg mini-batch loss: 2.211535859107971\n",
      "[epoch: 2, i: 2299] avg mini-batch loss: 2.07274539232254\n",
      "[epoch: 2, i: 2399] avg mini-batch loss: 2.136186671257019\n",
      "[epoch: 2, i: 2499] avg mini-batch loss: 2.1246903800964354\n",
      "[epoch: 2, i: 2599] avg mini-batch loss: 2.1375986433029173\n",
      "[epoch: 2, i: 2699] avg mini-batch loss: 2.0989950585365293\n",
      "[epoch: 2, i: 2799] avg mini-batch loss: 2.12667253613472\n",
      "[epoch: 2, i: 2899] avg mini-batch loss: 2.1017026805877688\n",
      "[epoch: 2, i: 2999] avg mini-batch loss: 2.140723108053207\n",
      "[epoch: 2, i: 3099] avg mini-batch loss: 2.0785889160633086\n",
      "[epoch: 2, i: 3199] avg mini-batch loss: 2.1203339755535127\n",
      "[epoch: 2, i: 3299] avg mini-batch loss: 2.151582487821579\n",
      "[epoch: 2, i: 3399] avg mini-batch loss: 2.098611191511154\n",
      "[epoch: 2, i: 3499] avg mini-batch loss: 2.087693156003952\n",
      "[epoch: 2, i: 3599] avg mini-batch loss: 2.1427737641334534\n",
      "[epoch: 2, i: 3699] avg mini-batch loss: 2.0712476313114165\n",
      "[epoch: 2, i: 3799] avg mini-batch loss: 2.078687906265259\n",
      "[epoch: 2, i: 3899] avg mini-batch loss: 2.0885486996173857\n",
      "[epoch: 2, i: 3999] avg mini-batch loss: 2.039276354312897\n",
      "[epoch: 2, i: 4099] avg mini-batch loss: 2.0624773287773133\n",
      "[epoch: 2, i: 4199] avg mini-batch loss: 2.0992224276065827\n",
      "[epoch: 2, i: 4299] avg mini-batch loss: 2.099546556472778\n",
      "[epoch: 2, i: 4399] avg mini-batch loss: 2.095901725292206\n",
      "[epoch: 2, i: 4499] avg mini-batch loss: 2.1384815764427185\n",
      "[epoch: 2, i: 4599] avg mini-batch loss: 2.0595455873012543\n",
      "[epoch: 2, i: 4699] avg mini-batch loss: 2.0898122715950014\n",
      "[epoch: 2, i: 4799] avg mini-batch loss: 2.085381075143814\n",
      "[epoch: 2, i: 4899] avg mini-batch loss: 2.0854625260829924\n",
      "[epoch: 2, i: 4999] avg mini-batch loss: 2.079902160167694\n",
      "[epoch: 2, i: 5099] avg mini-batch loss: 2.1210484766960143\n",
      "[epoch: 2, i: 5199] avg mini-batch loss: 2.1307535564899442\n",
      "[epoch: 2, i: 5299] avg mini-batch loss: 2.039052585363388\n",
      "[epoch: 2, i: 5399] avg mini-batch loss: 2.0946526622772215\n",
      "[epoch: 2, i: 5499] avg mini-batch loss: 2.0261296010017396\n",
      "[epoch: 2, i: 5599] avg mini-batch loss: 2.0465774166584016\n",
      "[epoch: 2, i: 5699] avg mini-batch loss: 1.9795971477031709\n",
      "[epoch: 2, i: 5799] avg mini-batch loss: 2.0020824897289278\n",
      "[epoch: 2, i: 5899] avg mini-batch loss: 2.042028865814209\n",
      "[epoch: 2, i: 5999] avg mini-batch loss: 2.0320786058902742\n",
      "[epoch: 2, i: 6099] avg mini-batch loss: 2.0487744569778443\n",
      "[epoch: 2, i: 6199] avg mini-batch loss: 2.0749738109111786\n",
      "[epoch: 2, i: 6299] avg mini-batch loss: 2.0483793091773985\n",
      "[epoch: 2, i: 6399] avg mini-batch loss: 2.0279634141922\n",
      "[epoch: 2, i: 6499] avg mini-batch loss: 2.0129340851306914\n",
      "[epoch: 2, i: 6599] avg mini-batch loss: 2.003796579837799\n",
      "[epoch: 2, i: 6699] avg mini-batch loss: 2.0202821254730225\n",
      "[epoch: 2, i: 6799] avg mini-batch loss: 2.053307594060898\n",
      "[epoch: 2, i: 6899] avg mini-batch loss: 1.9883889436721802\n",
      "[epoch: 2, i: 6999] avg mini-batch loss: 2.0700570976734163\n",
      "[epoch: 2, i: 7099] avg mini-batch loss: 1.9856015419960023\n",
      "[epoch: 2, i: 7199] avg mini-batch loss: 1.9734427535533905\n",
      "[epoch: 2, i: 7299] avg mini-batch loss: 2.0014419877529144\n",
      "[epoch: 2, i: 7399] avg mini-batch loss: 1.9976140558719635\n",
      "[epoch: 2, i: 7499] avg mini-batch loss: 2.00359282374382\n",
      "[epoch: 2, i: 7599] avg mini-batch loss: 2.038336396217346\n",
      "[epoch: 2, i: 7699] avg mini-batch loss: 2.0190445554256438\n",
      "[epoch: 2, i: 7799] avg mini-batch loss: 2.005236781835556\n",
      "[epoch: 2, i: 7899] avg mini-batch loss: 2.011583231687546\n",
      "[epoch: 2, i: 7999] avg mini-batch loss: 1.954592901468277\n",
      "[epoch: 2, i: 8099] avg mini-batch loss: 2.0196908032894134\n",
      "[epoch: 2, i: 8199] avg mini-batch loss: 2.0123076009750367\n",
      "[epoch: 2, i: 8299] avg mini-batch loss: 1.9879745852947235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 2, i: 8399] avg mini-batch loss: 2.03564910531044\n",
      "[epoch: 2, i: 8499] avg mini-batch loss: 1.9354020404815673\n",
      "[epoch: 2, i: 8599] avg mini-batch loss: 1.9911705684661865\n",
      "[epoch: 2, i: 8699] avg mini-batch loss: 1.9675495088100434\n",
      "[epoch: 2, i: 8799] avg mini-batch loss: 1.9777649545669556\n",
      "[epoch: 2, i: 8899] avg mini-batch loss: 1.9084068095684052\n",
      "[epoch: 2, i: 8999] avg mini-batch loss: 1.946078221797943\n",
      "[epoch: 2, i: 9099] avg mini-batch loss: 1.9295246481895447\n",
      "[epoch: 2, i: 9199] avg mini-batch loss: 2.0082895481586456\n",
      "[epoch: 2, i: 9299] avg mini-batch loss: 2.0147061729431153\n",
      "[epoch: 2, i: 9399] avg mini-batch loss: 1.953300586938858\n",
      "[epoch: 2, i: 9499] avg mini-batch loss: 1.96865358710289\n",
      "[epoch: 2, i: 9599] avg mini-batch loss: 1.955186778306961\n",
      "[epoch: 2, i: 9699] avg mini-batch loss: 1.9583187973499299\n",
      "[epoch: 3, i: 99] avg mini-batch loss: 1.9426151347160339\n",
      "[epoch: 3, i: 199] avg mini-batch loss: 1.921932727098465\n",
      "[epoch: 3, i: 299] avg mini-batch loss: 1.933357857465744\n",
      "[epoch: 3, i: 399] avg mini-batch loss: 1.9517401671409607\n",
      "[epoch: 3, i: 499] avg mini-batch loss: 1.999166933298111\n",
      "[epoch: 3, i: 599] avg mini-batch loss: 1.9734686315059662\n",
      "[epoch: 3, i: 699] avg mini-batch loss: 1.9725980281829834\n",
      "[epoch: 3, i: 799] avg mini-batch loss: 1.9376168859004974\n",
      "[epoch: 3, i: 899] avg mini-batch loss: 1.9050012695789338\n",
      "[epoch: 3, i: 999] avg mini-batch loss: 1.9797575378417969\n",
      "[epoch: 3, i: 1099] avg mini-batch loss: 1.9571630990505218\n",
      "[epoch: 3, i: 1199] avg mini-batch loss: 1.9727385890483857\n",
      "[epoch: 3, i: 1299] avg mini-batch loss: 1.9210794830322266\n",
      "[epoch: 3, i: 1399] avg mini-batch loss: 1.9590872263908385\n",
      "[epoch: 3, i: 1499] avg mini-batch loss: 1.9568946838378907\n",
      "[epoch: 3, i: 1599] avg mini-batch loss: 1.963228714466095\n",
      "[epoch: 3, i: 1699] avg mini-batch loss: 1.986761714220047\n",
      "[epoch: 3, i: 1799] avg mini-batch loss: 1.9320273506641388\n",
      "[epoch: 3, i: 1899] avg mini-batch loss: 1.965211706161499\n",
      "[epoch: 3, i: 1999] avg mini-batch loss: 1.939768396615982\n",
      "[epoch: 3, i: 2099] avg mini-batch loss: 1.934897820353508\n",
      "[epoch: 3, i: 2199] avg mini-batch loss: 1.965098773241043\n",
      "[epoch: 3, i: 2299] avg mini-batch loss: 1.945832440853119\n",
      "[epoch: 3, i: 2399] avg mini-batch loss: 1.922686879634857\n",
      "[epoch: 3, i: 2499] avg mini-batch loss: 1.9554259359836579\n",
      "[epoch: 3, i: 2599] avg mini-batch loss: 1.9486253154277802\n",
      "[epoch: 3, i: 2699] avg mini-batch loss: 1.9270813488960266\n",
      "[epoch: 3, i: 2799] avg mini-batch loss: 1.9110001397132874\n",
      "[epoch: 3, i: 2899] avg mini-batch loss: 1.877840565443039\n",
      "[epoch: 3, i: 2999] avg mini-batch loss: 1.9203644895553589\n",
      "[epoch: 3, i: 3099] avg mini-batch loss: 1.932068132162094\n",
      "[epoch: 3, i: 3199] avg mini-batch loss: 1.8789628005027772\n",
      "[epoch: 3, i: 3299] avg mini-batch loss: 1.9192401850223542\n",
      "[epoch: 3, i: 3399] avg mini-batch loss: 1.8708352172374725\n",
      "[epoch: 3, i: 3499] avg mini-batch loss: 1.909640486240387\n",
      "[epoch: 3, i: 3599] avg mini-batch loss: 1.94256831407547\n",
      "[epoch: 3, i: 3699] avg mini-batch loss: 1.9101684474945069\n",
      "[epoch: 3, i: 3799] avg mini-batch loss: 1.9497839379310609\n",
      "[epoch: 3, i: 3899] avg mini-batch loss: 1.9330621719360352\n",
      "[epoch: 3, i: 3999] avg mini-batch loss: 1.9219307208061218\n",
      "[epoch: 3, i: 4099] avg mini-batch loss: 1.886753625869751\n",
      "[epoch: 3, i: 4199] avg mini-batch loss: 1.9259207499027253\n",
      "[epoch: 3, i: 4299] avg mini-batch loss: 1.9252845180034637\n",
      "[epoch: 3, i: 4399] avg mini-batch loss: 1.8854196572303772\n",
      "[epoch: 3, i: 4499] avg mini-batch loss: 1.9101874136924744\n",
      "[epoch: 3, i: 4599] avg mini-batch loss: 1.9143676245212555\n",
      "[epoch: 3, i: 4699] avg mini-batch loss: 1.8825072467327117\n",
      "[epoch: 3, i: 4799] avg mini-batch loss: 1.8459668159484863\n",
      "[epoch: 3, i: 4899] avg mini-batch loss: 1.8681986439228058\n",
      "[epoch: 3, i: 4999] avg mini-batch loss: 1.8667111897468567\n",
      "[epoch: 3, i: 5099] avg mini-batch loss: 1.81485844373703\n",
      "[epoch: 3, i: 5199] avg mini-batch loss: 1.8395730006694793\n",
      "[epoch: 3, i: 5299] avg mini-batch loss: 1.8674586153030395\n",
      "[epoch: 3, i: 5399] avg mini-batch loss: 1.8987763142585754\n",
      "[epoch: 3, i: 5499] avg mini-batch loss: 1.8841849279403686\n",
      "[epoch: 3, i: 5599] avg mini-batch loss: 1.8642272305488587\n",
      "[epoch: 3, i: 5699] avg mini-batch loss: 1.8891009402275085\n",
      "[epoch: 3, i: 5799] avg mini-batch loss: 1.9592862856388091\n",
      "[epoch: 3, i: 5899] avg mini-batch loss: 1.8634348130226135\n",
      "[epoch: 3, i: 5999] avg mini-batch loss: 1.8968917906284333\n",
      "[epoch: 3, i: 6099] avg mini-batch loss: 1.8861436283588409\n",
      "[epoch: 3, i: 6199] avg mini-batch loss: 1.8720798480510712\n",
      "[epoch: 3, i: 6299] avg mini-batch loss: 1.9004731702804565\n",
      "[epoch: 3, i: 6399] avg mini-batch loss: 1.9062905895709992\n",
      "[epoch: 3, i: 6499] avg mini-batch loss: 1.88068052649498\n",
      "[epoch: 3, i: 6599] avg mini-batch loss: 1.872244929075241\n",
      "[epoch: 3, i: 6699] avg mini-batch loss: 1.879808554649353\n",
      "[epoch: 3, i: 6799] avg mini-batch loss: 1.9038754403591156\n",
      "[epoch: 3, i: 6899] avg mini-batch loss: 1.9048232400417329\n",
      "[epoch: 3, i: 6999] avg mini-batch loss: 1.865739529132843\n",
      "[epoch: 3, i: 7099] avg mini-batch loss: 1.8480553030967712\n",
      "[epoch: 3, i: 7199] avg mini-batch loss: 1.8467192542552948\n",
      "[epoch: 3, i: 7299] avg mini-batch loss: 1.8910471308231354\n",
      "[epoch: 3, i: 7399] avg mini-batch loss: 1.8643816339969634\n",
      "[epoch: 3, i: 7499] avg mini-batch loss: 1.8611413526535034\n",
      "[epoch: 3, i: 7599] avg mini-batch loss: 1.9033779299259186\n",
      "[epoch: 3, i: 7699] avg mini-batch loss: 1.8631190037727356\n",
      "[epoch: 3, i: 7799] avg mini-batch loss: 1.8643340933322907\n",
      "[epoch: 3, i: 7899] avg mini-batch loss: 1.8857518994808198\n",
      "[epoch: 3, i: 7999] avg mini-batch loss: 1.9013510274887084\n",
      "[epoch: 3, i: 8099] avg mini-batch loss: 1.9043014466762542\n",
      "[epoch: 3, i: 8199] avg mini-batch loss: 1.8730767381191253\n",
      "[epoch: 3, i: 8299] avg mini-batch loss: 1.8614317166805268\n",
      "[epoch: 3, i: 8399] avg mini-batch loss: 1.9529152166843415\n",
      "[epoch: 3, i: 8499] avg mini-batch loss: 1.8600908076763154\n",
      "[epoch: 3, i: 8599] avg mini-batch loss: 1.8714058470726014\n",
      "[epoch: 3, i: 8699] avg mini-batch loss: 1.8476696544885636\n",
      "[epoch: 3, i: 8799] avg mini-batch loss: 1.9206235992908478\n",
      "[epoch: 3, i: 8899] avg mini-batch loss: 1.8381526100635528\n",
      "[epoch: 3, i: 8999] avg mini-batch loss: 1.8685777616500854\n",
      "[epoch: 3, i: 9099] avg mini-batch loss: 1.83145246386528\n",
      "[epoch: 3, i: 9199] avg mini-batch loss: 1.8485124242305755\n",
      "[epoch: 3, i: 9299] avg mini-batch loss: 1.8585806334018706\n",
      "[epoch: 3, i: 9399] avg mini-batch loss: 1.872575135231018\n",
      "[epoch: 3, i: 9499] avg mini-batch loss: 1.7743747699260712\n",
      "[epoch: 3, i: 9599] avg mini-batch loss: 1.8878682804107667\n",
      "[epoch: 3, i: 9699] avg mini-batch loss: 1.8566069149971007\n",
      "[epoch: 4, i: 99] avg mini-batch loss: 1.8940057647228241\n",
      "[epoch: 4, i: 199] avg mini-batch loss: 1.8229758965969085\n",
      "[epoch: 4, i: 299] avg mini-batch loss: 1.8574221193790437\n",
      "[epoch: 4, i: 399] avg mini-batch loss: 1.8314840734004973\n",
      "[epoch: 4, i: 499] avg mini-batch loss: 1.8879926598072052\n",
      "[epoch: 4, i: 599] avg mini-batch loss: 1.8119813990592957\n",
      "[epoch: 4, i: 699] avg mini-batch loss: 1.8334537768363952\n",
      "[epoch: 4, i: 799] avg mini-batch loss: 1.8363583958148957\n",
      "[epoch: 4, i: 899] avg mini-batch loss: 1.8640948665142059\n",
      "[epoch: 4, i: 999] avg mini-batch loss: 1.8693251156806945\n",
      "[epoch: 4, i: 1099] avg mini-batch loss: 1.798125286102295\n",
      "[epoch: 4, i: 1199] avg mini-batch loss: 1.8665070712566376\n",
      "[epoch: 4, i: 1299] avg mini-batch loss: 1.791648610830307\n",
      "[epoch: 4, i: 1399] avg mini-batch loss: 1.8889763104915618\n",
      "[epoch: 4, i: 1499] avg mini-batch loss: 1.805145992040634\n",
      "[epoch: 4, i: 1599] avg mini-batch loss: 1.8165184605121611\n",
      "[epoch: 4, i: 1699] avg mini-batch loss: 1.8747790896892547\n",
      "[epoch: 4, i: 1799] avg mini-batch loss: 1.7717475187778473\n",
      "[epoch: 4, i: 1899] avg mini-batch loss: 1.8534251368045807\n",
      "[epoch: 4, i: 1999] avg mini-batch loss: 1.8723286926746368\n",
      "[epoch: 4, i: 2099] avg mini-batch loss: 1.8684327960014344\n",
      "[epoch: 4, i: 2199] avg mini-batch loss: 1.8510833144187928\n",
      "[epoch: 4, i: 2299] avg mini-batch loss: 1.8416293334960938\n",
      "[epoch: 4, i: 2399] avg mini-batch loss: 1.829530178308487\n",
      "[epoch: 4, i: 2499] avg mini-batch loss: 1.8681357085704804\n",
      "[epoch: 4, i: 2599] avg mini-batch loss: 1.7663090586662293\n",
      "[epoch: 4, i: 2699] avg mini-batch loss: 1.8592086493968965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 4, i: 2799] avg mini-batch loss: 1.8215906023979187\n",
      "[epoch: 4, i: 2899] avg mini-batch loss: 1.8678562581539153\n",
      "[epoch: 4, i: 2999] avg mini-batch loss: 1.8725179934501648\n",
      "[epoch: 4, i: 3099] avg mini-batch loss: 1.815916486978531\n",
      "[epoch: 4, i: 3199] avg mini-batch loss: 1.8539721810817718\n",
      "[epoch: 4, i: 3299] avg mini-batch loss: 1.7543614399433136\n",
      "[epoch: 4, i: 3399] avg mini-batch loss: 1.852257866859436\n",
      "[epoch: 4, i: 3499] avg mini-batch loss: 1.8542136323451996\n",
      "[epoch: 4, i: 3599] avg mini-batch loss: 1.8633446204662323\n",
      "[epoch: 4, i: 3699] avg mini-batch loss: 1.8079235422611237\n",
      "[epoch: 4, i: 3799] avg mini-batch loss: 1.8311863923072815\n",
      "[epoch: 4, i: 3899] avg mini-batch loss: 1.808873325586319\n",
      "[epoch: 4, i: 3999] avg mini-batch loss: 1.8342027258872986\n",
      "[epoch: 4, i: 4099] avg mini-batch loss: 1.785571244955063\n",
      "[epoch: 4, i: 4199] avg mini-batch loss: 1.835044709444046\n",
      "[epoch: 4, i: 4299] avg mini-batch loss: 1.8360327255725861\n",
      "[epoch: 4, i: 4399] avg mini-batch loss: 1.8351608127355576\n",
      "[epoch: 4, i: 4499] avg mini-batch loss: 1.810490220785141\n",
      "[epoch: 4, i: 4599] avg mini-batch loss: 1.8430552136898042\n",
      "[epoch: 4, i: 4699] avg mini-batch loss: 1.832680994272232\n",
      "[epoch: 4, i: 4799] avg mini-batch loss: 1.8351488292217255\n",
      "[epoch: 4, i: 4899] avg mini-batch loss: 1.8068182957172394\n",
      "[epoch: 4, i: 4999] avg mini-batch loss: 1.7911753463745117\n",
      "[epoch: 4, i: 5099] avg mini-batch loss: 1.7930456018447876\n",
      "[epoch: 4, i: 5199] avg mini-batch loss: 1.8197071754932403\n",
      "[epoch: 4, i: 5299] avg mini-batch loss: 1.770960669517517\n",
      "[epoch: 4, i: 5399] avg mini-batch loss: 1.8120110428333283\n",
      "[epoch: 4, i: 5499] avg mini-batch loss: 1.8176707303524018\n",
      "[epoch: 4, i: 5599] avg mini-batch loss: 1.8351028168201446\n",
      "[epoch: 4, i: 5699] avg mini-batch loss: 1.7901000118255614\n",
      "[epoch: 4, i: 5799] avg mini-batch loss: 1.8200244736671447\n",
      "[epoch: 4, i: 5899] avg mini-batch loss: 1.8477810728549957\n",
      "[epoch: 4, i: 5999] avg mini-batch loss: 1.8278557300567626\n",
      "[epoch: 4, i: 6099] avg mini-batch loss: 1.80670422911644\n",
      "[epoch: 4, i: 6199] avg mini-batch loss: 1.79353679895401\n",
      "[epoch: 4, i: 6299] avg mini-batch loss: 1.7778186571598054\n",
      "[epoch: 4, i: 6399] avg mini-batch loss: 1.7764666557312012\n",
      "[epoch: 4, i: 6499] avg mini-batch loss: 1.8571543538570403\n",
      "[epoch: 4, i: 6599] avg mini-batch loss: 1.8050699257850646\n",
      "[epoch: 4, i: 6699] avg mini-batch loss: 1.7899577844142913\n",
      "[epoch: 4, i: 6799] avg mini-batch loss: 1.7627723348140716\n",
      "[epoch: 4, i: 6899] avg mini-batch loss: 1.8528363585472107\n",
      "[epoch: 4, i: 6999] avg mini-batch loss: 1.8265610432624817\n",
      "[epoch: 4, i: 7099] avg mini-batch loss: 1.779680210351944\n",
      "[epoch: 4, i: 7199] avg mini-batch loss: 1.810283831357956\n",
      "[epoch: 4, i: 7299] avg mini-batch loss: 1.7635254216194154\n",
      "[epoch: 4, i: 7399] avg mini-batch loss: 1.7978711366653441\n",
      "[epoch: 4, i: 7499] avg mini-batch loss: 1.7740042102336884\n",
      "[epoch: 4, i: 7599] avg mini-batch loss: 1.7848141741752626\n",
      "[epoch: 4, i: 7699] avg mini-batch loss: 1.84517453789711\n",
      "[epoch: 4, i: 7799] avg mini-batch loss: 1.7612733763456345\n",
      "[epoch: 4, i: 7899] avg mini-batch loss: 1.7988902449607849\n",
      "[epoch: 4, i: 7999] avg mini-batch loss: 1.7544402718544005\n",
      "[epoch: 4, i: 8099] avg mini-batch loss: 1.8377879416942597\n",
      "[epoch: 4, i: 8199] avg mini-batch loss: 1.8003553092479705\n",
      "[epoch: 4, i: 8299] avg mini-batch loss: 1.800773605108261\n",
      "[epoch: 4, i: 8399] avg mini-batch loss: 1.7991383254528046\n",
      "[epoch: 4, i: 8499] avg mini-batch loss: 1.7784979557991027\n",
      "[epoch: 4, i: 8599] avg mini-batch loss: 1.7913727527856826\n",
      "[epoch: 4, i: 8699] avg mini-batch loss: 1.7974461698532105\n",
      "[epoch: 4, i: 8799] avg mini-batch loss: 1.7858672797679902\n",
      "[epoch: 4, i: 8899] avg mini-batch loss: 1.806385805606842\n",
      "[epoch: 4, i: 8999] avg mini-batch loss: 1.7958620154857636\n",
      "[epoch: 4, i: 9099] avg mini-batch loss: 1.7510733652114867\n",
      "[epoch: 4, i: 9199] avg mini-batch loss: 1.7556534278392792\n",
      "[epoch: 4, i: 9299] avg mini-batch loss: 1.8108068358898164\n",
      "[epoch: 4, i: 9399] avg mini-batch loss: 1.8237593603134155\n",
      "[epoch: 4, i: 9499] avg mini-batch loss: 1.7805825567245483\n",
      "[epoch: 4, i: 9599] avg mini-batch loss: 1.8032681453227997\n",
      "[epoch: 4, i: 9699] avg mini-batch loss: 1.7754741978645325\n",
      "[epoch: 5, i: 99] avg mini-batch loss: 1.7317366290092469\n",
      "[epoch: 5, i: 199] avg mini-batch loss: 1.759307930469513\n",
      "[epoch: 5, i: 299] avg mini-batch loss: 1.7565447771549225\n",
      "[epoch: 5, i: 399] avg mini-batch loss: 1.8084485363960265\n",
      "[epoch: 5, i: 499] avg mini-batch loss: 1.7479239201545715\n",
      "[epoch: 5, i: 599] avg mini-batch loss: 1.7418624174594879\n",
      "[epoch: 5, i: 699] avg mini-batch loss: 1.7941938799619674\n",
      "[epoch: 5, i: 799] avg mini-batch loss: 1.7943433022499085\n",
      "[epoch: 5, i: 899] avg mini-batch loss: 1.7704346561431885\n",
      "[epoch: 5, i: 999] avg mini-batch loss: 1.8126150238513947\n",
      "[epoch: 5, i: 1099] avg mini-batch loss: 1.8047429203987122\n",
      "[epoch: 5, i: 1199] avg mini-batch loss: 1.7244120812416077\n",
      "[epoch: 5, i: 1299] avg mini-batch loss: 1.7946871387958527\n",
      "[epoch: 5, i: 1399] avg mini-batch loss: 1.7927224099636079\n",
      "[epoch: 5, i: 1499] avg mini-batch loss: 1.7728680300712585\n",
      "[epoch: 5, i: 1599] avg mini-batch loss: 1.7914513170719146\n",
      "[epoch: 5, i: 1699] avg mini-batch loss: 1.795092786550522\n",
      "[epoch: 5, i: 1799] avg mini-batch loss: 1.8294870710372926\n",
      "[epoch: 5, i: 1899] avg mini-batch loss: 1.8183975756168365\n",
      "[epoch: 5, i: 1999] avg mini-batch loss: 1.7823957014083862\n",
      "[epoch: 5, i: 2099] avg mini-batch loss: 1.8385008370876312\n",
      "[epoch: 5, i: 2199] avg mini-batch loss: 1.73611368060112\n",
      "[epoch: 5, i: 2299] avg mini-batch loss: 1.7550742089748383\n",
      "[epoch: 5, i: 2399] avg mini-batch loss: 1.7179633015394211\n",
      "[epoch: 5, i: 2499] avg mini-batch loss: 1.7470380520820619\n",
      "[epoch: 5, i: 2599] avg mini-batch loss: 1.712787526845932\n",
      "[epoch: 5, i: 2699] avg mini-batch loss: 1.7558567005395889\n",
      "[epoch: 5, i: 2799] avg mini-batch loss: 1.7392270040512086\n",
      "[epoch: 5, i: 2899] avg mini-batch loss: 1.735348960161209\n",
      "[epoch: 5, i: 2999] avg mini-batch loss: 1.7946021950244904\n",
      "[epoch: 5, i: 3099] avg mini-batch loss: 1.7499087250232697\n",
      "[epoch: 5, i: 3199] avg mini-batch loss: 1.7757071709632875\n",
      "[epoch: 5, i: 3299] avg mini-batch loss: 1.766978143453598\n",
      "[epoch: 5, i: 3399] avg mini-batch loss: 1.781222779750824\n",
      "[epoch: 5, i: 3499] avg mini-batch loss: 1.8040518009662627\n",
      "[epoch: 5, i: 3599] avg mini-batch loss: 1.7610408294200897\n",
      "[epoch: 5, i: 3699] avg mini-batch loss: 1.796853064894676\n",
      "[epoch: 5, i: 3799] avg mini-batch loss: 1.7719817554950714\n",
      "[epoch: 5, i: 3899] avg mini-batch loss: 1.7654449999332429\n",
      "[epoch: 5, i: 3999] avg mini-batch loss: 1.756721373796463\n",
      "[epoch: 5, i: 4099] avg mini-batch loss: 1.7576724779605866\n",
      "[epoch: 5, i: 4199] avg mini-batch loss: 1.679866474866867\n",
      "[epoch: 5, i: 4299] avg mini-batch loss: 1.7577833658456803\n",
      "[epoch: 5, i: 4399] avg mini-batch loss: 1.7493532931804656\n",
      "[epoch: 5, i: 4499] avg mini-batch loss: 1.7343677330017089\n",
      "[epoch: 5, i: 4599] avg mini-batch loss: 1.7403592783212662\n",
      "[epoch: 5, i: 4699] avg mini-batch loss: 1.7692302286624908\n",
      "[epoch: 5, i: 4799] avg mini-batch loss: 1.7920738518238069\n",
      "[epoch: 5, i: 4899] avg mini-batch loss: 1.8343511891365052\n",
      "[epoch: 5, i: 4999] avg mini-batch loss: 1.7045906388759613\n",
      "[epoch: 5, i: 5099] avg mini-batch loss: 1.7872328197956084\n",
      "[epoch: 5, i: 5199] avg mini-batch loss: 1.748833292722702\n",
      "[epoch: 5, i: 5299] avg mini-batch loss: 1.7780597162246705\n",
      "[epoch: 5, i: 5399] avg mini-batch loss: 1.7751288056373595\n",
      "[epoch: 5, i: 5499] avg mini-batch loss: 1.727999233007431\n",
      "[epoch: 5, i: 5599] avg mini-batch loss: 1.73685906291008\n",
      "[epoch: 5, i: 5699] avg mini-batch loss: 1.7793782472610473\n",
      "[epoch: 5, i: 5799] avg mini-batch loss: 1.7975192141532899\n",
      "[epoch: 5, i: 5899] avg mini-batch loss: 1.7138122403621674\n",
      "[epoch: 5, i: 5999] avg mini-batch loss: 1.784720984697342\n",
      "[epoch: 5, i: 6099] avg mini-batch loss: 1.7773455464839936\n",
      "[epoch: 5, i: 6199] avg mini-batch loss: 1.7460900580883025\n",
      "[epoch: 5, i: 6299] avg mini-batch loss: 1.729950954914093\n",
      "[epoch: 5, i: 6399] avg mini-batch loss: 1.7262380516529083\n",
      "[epoch: 5, i: 6499] avg mini-batch loss: 1.743964194059372\n",
      "[epoch: 5, i: 6599] avg mini-batch loss: 1.7141494071483612\n",
      "[epoch: 5, i: 6699] avg mini-batch loss: 1.7602423042058946\n",
      "[epoch: 5, i: 6799] avg mini-batch loss: 1.75224853515625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 5, i: 6899] avg mini-batch loss: 1.753891065120697\n",
      "[epoch: 5, i: 6999] avg mini-batch loss: 1.7379262781143188\n",
      "[epoch: 5, i: 7099] avg mini-batch loss: 1.7742386114597322\n",
      "[epoch: 5, i: 7199] avg mini-batch loss: 1.7722557199001312\n",
      "[epoch: 5, i: 7299] avg mini-batch loss: 1.7477998077869414\n",
      "[epoch: 5, i: 7399] avg mini-batch loss: 1.7786516487598418\n",
      "[epoch: 5, i: 7499] avg mini-batch loss: 1.7142262440919875\n",
      "[epoch: 5, i: 7599] avg mini-batch loss: 1.7220972275733948\n",
      "[epoch: 5, i: 7699] avg mini-batch loss: 1.7444044148921967\n",
      "[epoch: 5, i: 7799] avg mini-batch loss: 1.715189447402954\n",
      "[epoch: 5, i: 7899] avg mini-batch loss: 1.7804766237735747\n",
      "[epoch: 5, i: 7999] avg mini-batch loss: 1.7837084794044495\n",
      "[epoch: 5, i: 8099] avg mini-batch loss: 1.8175872242450715\n",
      "[epoch: 5, i: 8199] avg mini-batch loss: 1.746301839351654\n",
      "[epoch: 5, i: 8299] avg mini-batch loss: 1.7640241992473602\n",
      "[epoch: 5, i: 8399] avg mini-batch loss: 1.775923582315445\n",
      "[epoch: 5, i: 8499] avg mini-batch loss: 1.7489791536331176\n",
      "[epoch: 5, i: 8599] avg mini-batch loss: 1.7512253355979919\n",
      "[epoch: 5, i: 8699] avg mini-batch loss: 1.7540008652210235\n",
      "[epoch: 5, i: 8799] avg mini-batch loss: 1.7583976542949677\n",
      "[epoch: 5, i: 8899] avg mini-batch loss: 1.724227352142334\n",
      "[epoch: 5, i: 8999] avg mini-batch loss: 1.7180975735187531\n",
      "[epoch: 5, i: 9099] avg mini-batch loss: 1.7297333079576491\n",
      "[epoch: 5, i: 9199] avg mini-batch loss: 1.7087576401233673\n",
      "[epoch: 5, i: 9299] avg mini-batch loss: 1.7055347418785096\n",
      "[epoch: 5, i: 9399] avg mini-batch loss: 1.7581285071372985\n",
      "[epoch: 5, i: 9499] avg mini-batch loss: 1.7353377413749695\n",
      "[epoch: 5, i: 9599] avg mini-batch loss: 1.775893747806549\n",
      "[epoch: 5, i: 9699] avg mini-batch loss: 1.7309543156623841\n",
      "[epoch: 6, i: 99] avg mini-batch loss: 1.7415032613277435\n",
      "[epoch: 6, i: 199] avg mini-batch loss: 1.778459231853485\n",
      "[epoch: 6, i: 299] avg mini-batch loss: 1.6367093187570572\n",
      "[epoch: 6, i: 399] avg mini-batch loss: 1.7669763386249542\n",
      "[epoch: 6, i: 499] avg mini-batch loss: 1.7490640842914582\n",
      "[epoch: 6, i: 599] avg mini-batch loss: 1.7459911143779754\n",
      "[epoch: 6, i: 699] avg mini-batch loss: 1.694821870326996\n",
      "[epoch: 6, i: 799] avg mini-batch loss: 1.7326222586631774\n",
      "[epoch: 6, i: 899] avg mini-batch loss: 1.7159843611717225\n",
      "[epoch: 6, i: 999] avg mini-batch loss: 1.7406009721755982\n",
      "[epoch: 6, i: 1099] avg mini-batch loss: 1.7542060297727584\n",
      "[epoch: 6, i: 1199] avg mini-batch loss: 1.7889505541324615\n",
      "[epoch: 6, i: 1299] avg mini-batch loss: 1.7250004088878632\n",
      "[epoch: 6, i: 1399] avg mini-batch loss: 1.7278251349925995\n",
      "[epoch: 6, i: 1499] avg mini-batch loss: 1.7467214798927306\n",
      "[epoch: 6, i: 1599] avg mini-batch loss: 1.7520699107646942\n",
      "[epoch: 6, i: 1699] avg mini-batch loss: 1.672438498735428\n",
      "[epoch: 6, i: 1799] avg mini-batch loss: 1.7080426609516144\n",
      "[epoch: 6, i: 1899] avg mini-batch loss: 1.7710805499553681\n",
      "[epoch: 6, i: 1999] avg mini-batch loss: 1.712245433330536\n",
      "[epoch: 6, i: 2099] avg mini-batch loss: 1.7706023561954498\n",
      "[epoch: 6, i: 2199] avg mini-batch loss: 1.6940999245643615\n",
      "[epoch: 6, i: 2299] avg mini-batch loss: 1.7121598076820375\n",
      "[epoch: 6, i: 2399] avg mini-batch loss: 1.7156481039524079\n",
      "[epoch: 6, i: 2499] avg mini-batch loss: 1.65630251288414\n",
      "[epoch: 6, i: 2599] avg mini-batch loss: 1.708838163614273\n",
      "[epoch: 6, i: 2699] avg mini-batch loss: 1.7355152851343154\n",
      "[epoch: 6, i: 2799] avg mini-batch loss: 1.7699491608142852\n",
      "[epoch: 6, i: 2899] avg mini-batch loss: 1.7438211739063263\n",
      "[epoch: 6, i: 2999] avg mini-batch loss: 1.6794201052188873\n",
      "[epoch: 6, i: 3099] avg mini-batch loss: 1.7540942740440368\n",
      "[epoch: 6, i: 3199] avg mini-batch loss: 1.754624365568161\n",
      "[epoch: 6, i: 3299] avg mini-batch loss: 1.713681359887123\n",
      "[epoch: 6, i: 3399] avg mini-batch loss: 1.7200347471237183\n",
      "[epoch: 6, i: 3499] avg mini-batch loss: 1.7375311040878296\n",
      "[epoch: 6, i: 3599] avg mini-batch loss: 1.7213709604740144\n",
      "[epoch: 6, i: 3699] avg mini-batch loss: 1.7263552343845368\n",
      "[epoch: 6, i: 3799] avg mini-batch loss: 1.7123142397403717\n",
      "[epoch: 6, i: 3899] avg mini-batch loss: 1.6821513879299164\n",
      "[epoch: 6, i: 3999] avg mini-batch loss: 1.7371613943576814\n",
      "[epoch: 6, i: 4099] avg mini-batch loss: 1.7345061278343201\n",
      "[epoch: 6, i: 4199] avg mini-batch loss: 1.7252719736099242\n",
      "[epoch: 6, i: 4299] avg mini-batch loss: 1.7251255857944487\n",
      "[epoch: 6, i: 4399] avg mini-batch loss: 1.6809528112411498\n",
      "[epoch: 6, i: 4499] avg mini-batch loss: 1.732405635714531\n",
      "[epoch: 6, i: 4599] avg mini-batch loss: 1.7628312194347382\n",
      "[epoch: 6, i: 4699] avg mini-batch loss: 1.7443086528778076\n",
      "[epoch: 6, i: 4799] avg mini-batch loss: 1.7278759825229644\n",
      "[epoch: 6, i: 4899] avg mini-batch loss: 1.6724320340156555\n",
      "[epoch: 6, i: 4999] avg mini-batch loss: 1.7545377576351167\n",
      "[epoch: 6, i: 5099] avg mini-batch loss: 1.7288868403434754\n",
      "[epoch: 6, i: 5199] avg mini-batch loss: 1.6726469779014588\n",
      "[epoch: 6, i: 5299] avg mini-batch loss: 1.7335342347621918\n",
      "[epoch: 6, i: 5399] avg mini-batch loss: 1.6938657021522523\n",
      "[epoch: 6, i: 5499] avg mini-batch loss: 1.7498107993602752\n",
      "[epoch: 6, i: 5599] avg mini-batch loss: 1.7230262315273286\n",
      "[epoch: 6, i: 5699] avg mini-batch loss: 1.7095529353618621\n",
      "[epoch: 6, i: 5799] avg mini-batch loss: 1.7549392664432526\n",
      "[epoch: 6, i: 5899] avg mini-batch loss: 1.7104613387584686\n",
      "[epoch: 6, i: 5999] avg mini-batch loss: 1.7206506955623626\n",
      "[epoch: 6, i: 6099] avg mini-batch loss: 1.7482186007499694\n",
      "[epoch: 6, i: 6199] avg mini-batch loss: 1.7002107727527618\n",
      "[epoch: 6, i: 6299] avg mini-batch loss: 1.716126104593277\n",
      "[epoch: 6, i: 6399] avg mini-batch loss: 1.673745978474617\n",
      "[epoch: 6, i: 6499] avg mini-batch loss: 1.6903163009881974\n",
      "[epoch: 6, i: 6599] avg mini-batch loss: 1.7417773258686067\n",
      "[epoch: 6, i: 6699] avg mini-batch loss: 1.7049065661430358\n",
      "[epoch: 6, i: 6799] avg mini-batch loss: 1.7310168075561523\n",
      "[epoch: 6, i: 6899] avg mini-batch loss: 1.7558400744199754\n",
      "[epoch: 6, i: 6999] avg mini-batch loss: 1.734557626247406\n",
      "[epoch: 6, i: 7099] avg mini-batch loss: 1.731308677792549\n",
      "[epoch: 6, i: 7199] avg mini-batch loss: 1.6793749767541886\n",
      "[epoch: 6, i: 7299] avg mini-batch loss: 1.7614815014600753\n",
      "[epoch: 6, i: 7399] avg mini-batch loss: 1.7076867163181304\n",
      "[epoch: 6, i: 7499] avg mini-batch loss: 1.7506004762649536\n",
      "[epoch: 6, i: 7599] avg mini-batch loss: 1.745946900844574\n",
      "[epoch: 6, i: 7699] avg mini-batch loss: 1.7542961376905442\n",
      "[epoch: 6, i: 7799] avg mini-batch loss: 1.736517049074173\n",
      "[epoch: 6, i: 7899] avg mini-batch loss: 1.711263627409935\n",
      "[epoch: 6, i: 7999] avg mini-batch loss: 1.6702458667755127\n",
      "[epoch: 6, i: 8099] avg mini-batch loss: 1.7029360008239747\n",
      "[epoch: 6, i: 8199] avg mini-batch loss: 1.7052400720119476\n",
      "[epoch: 6, i: 8299] avg mini-batch loss: 1.7277407562732696\n",
      "[epoch: 6, i: 8399] avg mini-batch loss: 1.6698857235908509\n",
      "[epoch: 6, i: 8499] avg mini-batch loss: 1.7343800270557403\n",
      "[epoch: 6, i: 8599] avg mini-batch loss: 1.7103292226791382\n",
      "[epoch: 6, i: 8699] avg mini-batch loss: 1.7408590400218964\n",
      "[epoch: 6, i: 8799] avg mini-batch loss: 1.7171363151073455\n",
      "[epoch: 6, i: 8899] avg mini-batch loss: 1.7172370409965516\n",
      "[epoch: 6, i: 8999] avg mini-batch loss: 1.7107737851142883\n",
      "[epoch: 6, i: 9099] avg mini-batch loss: 1.7444457221031189\n",
      "[epoch: 6, i: 9199] avg mini-batch loss: 1.6913772404193879\n",
      "[epoch: 6, i: 9299] avg mini-batch loss: 1.7104170894622803\n",
      "[epoch: 6, i: 9399] avg mini-batch loss: 1.7001551496982574\n",
      "[epoch: 6, i: 9499] avg mini-batch loss: 1.664350152015686\n",
      "[epoch: 6, i: 9599] avg mini-batch loss: 1.6518544071912766\n",
      "[epoch: 6, i: 9699] avg mini-batch loss: 1.724645345211029\n",
      "[epoch: 7, i: 99] avg mini-batch loss: 1.7236639368534088\n",
      "[epoch: 7, i: 199] avg mini-batch loss: 1.766114764213562\n",
      "[epoch: 7, i: 299] avg mini-batch loss: 1.7404200971126556\n",
      "[epoch: 7, i: 399] avg mini-batch loss: 1.701597625017166\n",
      "[epoch: 7, i: 499] avg mini-batch loss: 1.677605432868004\n",
      "[epoch: 7, i: 599] avg mini-batch loss: 1.7345813167095185\n",
      "[epoch: 7, i: 699] avg mini-batch loss: 1.6308919155597688\n",
      "[epoch: 7, i: 799] avg mini-batch loss: 1.6671617591381074\n",
      "[epoch: 7, i: 899] avg mini-batch loss: 1.7041874861717223\n",
      "[epoch: 7, i: 999] avg mini-batch loss: 1.7977999186515807\n",
      "[epoch: 7, i: 1099] avg mini-batch loss: 1.7004062503576278\n",
      "[epoch: 7, i: 1199] avg mini-batch loss: 1.7253391718864441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 7, i: 1299] avg mini-batch loss: 1.698062517642975\n",
      "[epoch: 7, i: 1399] avg mini-batch loss: 1.703765195608139\n",
      "[epoch: 7, i: 1499] avg mini-batch loss: 1.710321877002716\n",
      "[epoch: 7, i: 1599] avg mini-batch loss: 1.6580429768562317\n",
      "[epoch: 7, i: 1699] avg mini-batch loss: 1.7256973588466644\n",
      "[epoch: 7, i: 1799] avg mini-batch loss: 1.7022554171085358\n",
      "[epoch: 7, i: 1899] avg mini-batch loss: 1.7277014076709747\n",
      "[epoch: 7, i: 1999] avg mini-batch loss: 1.6560822880268098\n",
      "[epoch: 7, i: 2099] avg mini-batch loss: 1.7368002420663833\n",
      "[epoch: 7, i: 2199] avg mini-batch loss: 1.694552129507065\n",
      "[epoch: 7, i: 2299] avg mini-batch loss: 1.699389825463295\n",
      "[epoch: 7, i: 2399] avg mini-batch loss: 1.6362179279327393\n",
      "[epoch: 7, i: 2499] avg mini-batch loss: 1.7324692869186402\n",
      "[epoch: 7, i: 2599] avg mini-batch loss: 1.7006403923034668\n",
      "[epoch: 7, i: 2699] avg mini-batch loss: 1.6490208280086518\n",
      "[epoch: 7, i: 2799] avg mini-batch loss: 1.6945633137226104\n",
      "[epoch: 7, i: 2899] avg mini-batch loss: 1.7045616728067399\n",
      "[epoch: 7, i: 2999] avg mini-batch loss: 1.647971533536911\n",
      "[epoch: 7, i: 3099] avg mini-batch loss: 1.6891481232643129\n",
      "[epoch: 7, i: 3199] avg mini-batch loss: 1.7436242449283599\n",
      "[epoch: 7, i: 3299] avg mini-batch loss: 1.7127890133857726\n",
      "[epoch: 7, i: 3399] avg mini-batch loss: 1.6574355864524841\n",
      "[epoch: 7, i: 3499] avg mini-batch loss: 1.6969010031223297\n",
      "[epoch: 7, i: 3599] avg mini-batch loss: 1.6727025562524795\n",
      "[epoch: 7, i: 3699] avg mini-batch loss: 1.6529844450950621\n",
      "[epoch: 7, i: 3799] avg mini-batch loss: 1.6972828114032745\n",
      "[epoch: 7, i: 3899] avg mini-batch loss: 1.7153769409656525\n",
      "[epoch: 7, i: 3999] avg mini-batch loss: 1.6574489760398865\n",
      "[epoch: 7, i: 4099] avg mini-batch loss: 1.6743666195869447\n",
      "[epoch: 7, i: 4199] avg mini-batch loss: 1.6952505260705948\n",
      "[epoch: 7, i: 4299] avg mini-batch loss: 1.6516142570972443\n",
      "[epoch: 7, i: 4399] avg mini-batch loss: 1.7213706821203232\n",
      "[epoch: 7, i: 4499] avg mini-batch loss: 1.674925615787506\n",
      "[epoch: 7, i: 4599] avg mini-batch loss: 1.6826142299175262\n",
      "[epoch: 7, i: 4699] avg mini-batch loss: 1.6967164433002473\n",
      "[epoch: 7, i: 4799] avg mini-batch loss: 1.6685573434829712\n",
      "[epoch: 7, i: 4899] avg mini-batch loss: 1.6836102533340453\n",
      "[epoch: 7, i: 4999] avg mini-batch loss: 1.7082595390081405\n",
      "[epoch: 7, i: 5099] avg mini-batch loss: 1.6073882627487182\n",
      "[epoch: 7, i: 5199] avg mini-batch loss: 1.6732095831632614\n",
      "[epoch: 7, i: 5299] avg mini-batch loss: 1.714465845823288\n",
      "[epoch: 7, i: 5399] avg mini-batch loss: 1.7079458379745482\n",
      "[epoch: 7, i: 5499] avg mini-batch loss: 1.6528783810138703\n",
      "[epoch: 7, i: 5599] avg mini-batch loss: 1.635656907558441\n",
      "[epoch: 7, i: 5699] avg mini-batch loss: 1.6784635734558107\n",
      "[epoch: 7, i: 5799] avg mini-batch loss: 1.6819369852542878\n",
      "[epoch: 7, i: 5899] avg mini-batch loss: 1.71526615858078\n",
      "[epoch: 7, i: 5999] avg mini-batch loss: 1.6995214235782623\n",
      "[epoch: 7, i: 6099] avg mini-batch loss: 1.6956137347221374\n",
      "[epoch: 7, i: 6199] avg mini-batch loss: 1.6992577719688415\n",
      "[epoch: 7, i: 6299] avg mini-batch loss: 1.6893348956108094\n",
      "[epoch: 7, i: 6399] avg mini-batch loss: 1.6660905528068541\n",
      "[epoch: 7, i: 6499] avg mini-batch loss: 1.679381617307663\n",
      "[epoch: 7, i: 6599] avg mini-batch loss: 1.7503464758396148\n",
      "[epoch: 7, i: 6699] avg mini-batch loss: 1.7394010150432586\n",
      "[epoch: 7, i: 6799] avg mini-batch loss: 1.6681315422058105\n",
      "[epoch: 7, i: 6899] avg mini-batch loss: 1.698249944448471\n",
      "[epoch: 7, i: 6999] avg mini-batch loss: 1.6925317215919495\n",
      "[epoch: 7, i: 7099] avg mini-batch loss: 1.723362727165222\n",
      "[epoch: 7, i: 7199] avg mini-batch loss: 1.7101661944389344\n",
      "[epoch: 7, i: 7299] avg mini-batch loss: 1.7154404246807098\n",
      "[epoch: 7, i: 7399] avg mini-batch loss: 1.6709826517105102\n",
      "[epoch: 7, i: 7499] avg mini-batch loss: 1.6718665528297425\n",
      "[epoch: 7, i: 7599] avg mini-batch loss: 1.748879873752594\n",
      "[epoch: 7, i: 7699] avg mini-batch loss: 1.6400336968898772\n",
      "[epoch: 7, i: 7799] avg mini-batch loss: 1.7079194390773773\n",
      "[epoch: 7, i: 7899] avg mini-batch loss: 1.6755930924415587\n",
      "[epoch: 7, i: 7999] avg mini-batch loss: 1.7302606445550919\n",
      "[epoch: 7, i: 8099] avg mini-batch loss: 1.7496508169174194\n",
      "[epoch: 7, i: 8199] avg mini-batch loss: 1.674414911866188\n",
      "[epoch: 7, i: 8299] avg mini-batch loss: 1.6506229221820832\n",
      "[epoch: 7, i: 8399] avg mini-batch loss: 1.7234261226654053\n",
      "[epoch: 7, i: 8499] avg mini-batch loss: 1.6853407645225524\n",
      "[epoch: 7, i: 8599] avg mini-batch loss: 1.6391870737075807\n",
      "[epoch: 7, i: 8699] avg mini-batch loss: 1.672084597349167\n",
      "[epoch: 7, i: 8799] avg mini-batch loss: 1.686645735502243\n",
      "[epoch: 7, i: 8899] avg mini-batch loss: 1.6134274446964263\n",
      "[epoch: 7, i: 8999] avg mini-batch loss: 1.64746546626091\n",
      "[epoch: 7, i: 9099] avg mini-batch loss: 1.6641430616378785\n",
      "[epoch: 7, i: 9199] avg mini-batch loss: 1.6822194612026216\n",
      "[epoch: 7, i: 9299] avg mini-batch loss: 1.6695118141174317\n",
      "[epoch: 7, i: 9399] avg mini-batch loss: 1.7104245972633363\n",
      "[epoch: 7, i: 9499] avg mini-batch loss: 1.6943835693597793\n",
      "[epoch: 7, i: 9599] avg mini-batch loss: 1.7173754930496217\n",
      "[epoch: 7, i: 9699] avg mini-batch loss: 1.710658265352249\n",
      "[epoch: 8, i: 99] avg mini-batch loss: 1.656754720211029\n",
      "[epoch: 8, i: 199] avg mini-batch loss: 1.69935067653656\n",
      "[epoch: 8, i: 299] avg mini-batch loss: 1.6594043219089507\n",
      "[epoch: 8, i: 399] avg mini-batch loss: 1.6822456693649293\n",
      "[epoch: 8, i: 499] avg mini-batch loss: 1.6814291083812714\n",
      "[epoch: 8, i: 599] avg mini-batch loss: 1.6649019384384156\n",
      "[epoch: 8, i: 699] avg mini-batch loss: 1.646747807264328\n",
      "[epoch: 8, i: 799] avg mini-batch loss: 1.6740580999851227\n",
      "[epoch: 8, i: 899] avg mini-batch loss: 1.6850865840911866\n",
      "[epoch: 8, i: 999] avg mini-batch loss: 1.6467724251747131\n",
      "[epoch: 8, i: 1099] avg mini-batch loss: 1.6496048021316527\n",
      "[epoch: 8, i: 1199] avg mini-batch loss: 1.6682494163513184\n",
      "[epoch: 8, i: 1299] avg mini-batch loss: 1.6693569457530975\n",
      "[epoch: 8, i: 1399] avg mini-batch loss: 1.6573103934526443\n",
      "[epoch: 8, i: 1499] avg mini-batch loss: 1.6843650752305985\n",
      "[epoch: 8, i: 1599] avg mini-batch loss: 1.7113016474246978\n",
      "[epoch: 8, i: 1699] avg mini-batch loss: 1.6987866103649139\n",
      "[epoch: 8, i: 1799] avg mini-batch loss: 1.708969054222107\n",
      "[epoch: 8, i: 1899] avg mini-batch loss: 1.6402748680114747\n",
      "[epoch: 8, i: 1999] avg mini-batch loss: 1.6906066608428956\n",
      "[epoch: 8, i: 2099] avg mini-batch loss: 1.6352834224700927\n",
      "[epoch: 8, i: 2199] avg mini-batch loss: 1.6573287403583528\n",
      "[epoch: 8, i: 2299] avg mini-batch loss: 1.6840228515863418\n",
      "[epoch: 8, i: 2399] avg mini-batch loss: 1.6619850379228591\n",
      "[epoch: 8, i: 2499] avg mini-batch loss: 1.6928046917915345\n",
      "[epoch: 8, i: 2599] avg mini-batch loss: 1.6229599779844284\n",
      "[epoch: 8, i: 2699] avg mini-batch loss: 1.6851852905750275\n",
      "[epoch: 8, i: 2799] avg mini-batch loss: 1.6611393493413926\n",
      "[epoch: 8, i: 2899] avg mini-batch loss: 1.6839064741134644\n",
      "[epoch: 8, i: 2999] avg mini-batch loss: 1.7183025860786438\n",
      "[epoch: 8, i: 3099] avg mini-batch loss: 1.6726855725049972\n",
      "[epoch: 8, i: 3199] avg mini-batch loss: 1.7229792559146881\n",
      "[epoch: 8, i: 3299] avg mini-batch loss: 1.6514568364620208\n",
      "[epoch: 8, i: 3399] avg mini-batch loss: 1.6728868669271468\n",
      "[epoch: 8, i: 3499] avg mini-batch loss: 1.6742463076114655\n",
      "[epoch: 8, i: 3599] avg mini-batch loss: 1.6749735844135285\n",
      "[epoch: 8, i: 3699] avg mini-batch loss: 1.6706786167621612\n",
      "[epoch: 8, i: 3799] avg mini-batch loss: 1.6833208501338959\n",
      "[epoch: 8, i: 3899] avg mini-batch loss: 1.6750911319255828\n",
      "[epoch: 8, i: 3999] avg mini-batch loss: 1.6578421187400818\n",
      "[epoch: 8, i: 4099] avg mini-batch loss: 1.706049502491951\n",
      "[epoch: 8, i: 4199] avg mini-batch loss: 1.6563692808151245\n",
      "[epoch: 8, i: 4299] avg mini-batch loss: 1.698037235736847\n",
      "[epoch: 8, i: 4399] avg mini-batch loss: 1.7057594501972198\n",
      "[epoch: 8, i: 4499] avg mini-batch loss: 1.6632384592294693\n",
      "[epoch: 8, i: 4599] avg mini-batch loss: 1.725651662349701\n",
      "[epoch: 8, i: 4699] avg mini-batch loss: 1.717857756614685\n",
      "[epoch: 8, i: 4799] avg mini-batch loss: 1.6872554045915604\n",
      "[epoch: 8, i: 4899] avg mini-batch loss: 1.6500669980049134\n",
      "[epoch: 8, i: 4999] avg mini-batch loss: 1.6764747416973114\n",
      "[epoch: 8, i: 5099] avg mini-batch loss: 1.6775379943847657\n",
      "[epoch: 8, i: 5199] avg mini-batch loss: 1.6824381411075593\n",
      "[epoch: 8, i: 5299] avg mini-batch loss: 1.6222294247150422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 8, i: 5399] avg mini-batch loss: 1.6967119765281677\n",
      "[epoch: 8, i: 5499] avg mini-batch loss: 1.6355060565471649\n",
      "[epoch: 8, i: 5599] avg mini-batch loss: 1.663219717144966\n",
      "[epoch: 8, i: 5699] avg mini-batch loss: 1.666926744580269\n",
      "[epoch: 8, i: 5799] avg mini-batch loss: 1.6498251080513\n",
      "[epoch: 8, i: 5899] avg mini-batch loss: 1.6758922064304351\n",
      "[epoch: 8, i: 5999] avg mini-batch loss: 1.6720499140024185\n",
      "[epoch: 8, i: 6099] avg mini-batch loss: 1.6790328031778337\n",
      "[epoch: 8, i: 6199] avg mini-batch loss: 1.6544289255142213\n",
      "[epoch: 8, i: 6299] avg mini-batch loss: 1.6335422939062119\n",
      "[epoch: 8, i: 6399] avg mini-batch loss: 1.6430239856243134\n",
      "[epoch: 8, i: 6499] avg mini-batch loss: 1.6638167417049408\n",
      "[epoch: 8, i: 6599] avg mini-batch loss: 1.6022581189870835\n",
      "[epoch: 8, i: 6699] avg mini-batch loss: 1.6838260984420776\n",
      "[epoch: 8, i: 6799] avg mini-batch loss: 1.6944376361370086\n",
      "[epoch: 8, i: 6899] avg mini-batch loss: 1.681558530330658\n",
      "[epoch: 8, i: 6999] avg mini-batch loss: 1.6590527164936066\n",
      "[epoch: 8, i: 7099] avg mini-batch loss: 1.6256690269708634\n",
      "[epoch: 8, i: 7199] avg mini-batch loss: 1.5806148153543473\n",
      "[epoch: 8, i: 7299] avg mini-batch loss: 1.6639017009735106\n",
      "[epoch: 8, i: 7399] avg mini-batch loss: 1.6524191147089005\n",
      "[epoch: 8, i: 7499] avg mini-batch loss: 1.6371900916099549\n",
      "[epoch: 8, i: 7599] avg mini-batch loss: 1.6654760932922363\n",
      "[epoch: 8, i: 7699] avg mini-batch loss: 1.659252678155899\n",
      "[epoch: 8, i: 7799] avg mini-batch loss: 1.6786192554235457\n",
      "[epoch: 8, i: 7899] avg mini-batch loss: 1.6626971191167832\n",
      "[epoch: 8, i: 7999] avg mini-batch loss: 1.7148563766479492\n",
      "[epoch: 8, i: 8099] avg mini-batch loss: 1.6777486646175384\n",
      "[epoch: 8, i: 8199] avg mini-batch loss: 1.659201933145523\n",
      "[epoch: 8, i: 8299] avg mini-batch loss: 1.6961005330085754\n",
      "[epoch: 8, i: 8399] avg mini-batch loss: 1.678023801445961\n",
      "[epoch: 8, i: 8499] avg mini-batch loss: 1.6122623759508132\n",
      "[epoch: 8, i: 8599] avg mini-batch loss: 1.6777017033100128\n",
      "[epoch: 8, i: 8699] avg mini-batch loss: 1.6253945338726044\n",
      "[epoch: 8, i: 8799] avg mini-batch loss: 1.6730076777935028\n",
      "[epoch: 8, i: 8899] avg mini-batch loss: 1.6752499282360076\n",
      "[epoch: 8, i: 8999] avg mini-batch loss: 1.6887786638736726\n",
      "[epoch: 8, i: 9099] avg mini-batch loss: 1.6357035195827485\n",
      "[epoch: 8, i: 9199] avg mini-batch loss: 1.6325908428430558\n",
      "[epoch: 8, i: 9299] avg mini-batch loss: 1.7170794242620468\n",
      "[epoch: 8, i: 9399] avg mini-batch loss: 1.6322186970710755\n",
      "[epoch: 8, i: 9499] avg mini-batch loss: 1.6674560225009918\n",
      "[epoch: 8, i: 9599] avg mini-batch loss: 1.6253391265869142\n",
      "[epoch: 8, i: 9699] avg mini-batch loss: 1.6470998531579972\n",
      "[epoch: 9, i: 99] avg mini-batch loss: 1.6759851741790772\n",
      "[epoch: 9, i: 199] avg mini-batch loss: 1.616596245765686\n",
      "[epoch: 9, i: 299] avg mini-batch loss: 1.670743225812912\n",
      "[epoch: 9, i: 399] avg mini-batch loss: 1.7092390334606171\n",
      "[epoch: 9, i: 499] avg mini-batch loss: 1.6457439506053924\n",
      "[epoch: 9, i: 599] avg mini-batch loss: 1.6017314261198043\n",
      "[epoch: 9, i: 699] avg mini-batch loss: 1.5452766758203507\n",
      "[epoch: 9, i: 799] avg mini-batch loss: 1.646858880519867\n",
      "[epoch: 9, i: 899] avg mini-batch loss: 1.707339310646057\n",
      "[epoch: 9, i: 999] avg mini-batch loss: 1.6501883792877197\n",
      "[epoch: 9, i: 1099] avg mini-batch loss: 1.695959757566452\n",
      "[epoch: 9, i: 1199] avg mini-batch loss: 1.736319767832756\n",
      "[epoch: 9, i: 1299] avg mini-batch loss: 1.6377013510465621\n",
      "[epoch: 9, i: 1399] avg mini-batch loss: 1.6988548862934112\n",
      "[epoch: 9, i: 1499] avg mini-batch loss: 1.666647344827652\n",
      "[epoch: 9, i: 1599] avg mini-batch loss: 1.6512528896331786\n",
      "[epoch: 9, i: 1699] avg mini-batch loss: 1.5776255661249161\n",
      "[epoch: 9, i: 1799] avg mini-batch loss: 1.6615452498197556\n",
      "[epoch: 9, i: 1899] avg mini-batch loss: 1.690511989593506\n",
      "[epoch: 9, i: 1999] avg mini-batch loss: 1.6692530035972595\n",
      "[epoch: 9, i: 2099] avg mini-batch loss: 1.6075228452682495\n",
      "[epoch: 9, i: 2199] avg mini-batch loss: 1.6296262800693513\n",
      "[epoch: 9, i: 2299] avg mini-batch loss: 1.6649433594942094\n",
      "[epoch: 9, i: 2399] avg mini-batch loss: 1.6690697050094605\n",
      "[epoch: 9, i: 2499] avg mini-batch loss: 1.6566379296779632\n",
      "[epoch: 9, i: 2599] avg mini-batch loss: 1.67636574447155\n",
      "[epoch: 9, i: 2699] avg mini-batch loss: 1.700291034579277\n",
      "[epoch: 9, i: 2799] avg mini-batch loss: 1.6275205647945403\n",
      "[epoch: 9, i: 2899] avg mini-batch loss: 1.6079849714040757\n",
      "[epoch: 9, i: 2999] avg mini-batch loss: 1.6604977917671204\n",
      "[epoch: 9, i: 3099] avg mini-batch loss: 1.625547091960907\n",
      "[epoch: 9, i: 3199] avg mini-batch loss: 1.6389252126216889\n",
      "[epoch: 9, i: 3299] avg mini-batch loss: 1.6676349121332168\n",
      "[epoch: 9, i: 3399] avg mini-batch loss: 1.607890961766243\n",
      "[epoch: 9, i: 3499] avg mini-batch loss: 1.6747640407085418\n",
      "[epoch: 9, i: 3599] avg mini-batch loss: 1.5976589423418046\n",
      "[epoch: 9, i: 3699] avg mini-batch loss: 1.6486869692802428\n",
      "[epoch: 9, i: 3799] avg mini-batch loss: 1.623320762515068\n",
      "[epoch: 9, i: 3899] avg mini-batch loss: 1.6665140688419342\n",
      "[epoch: 9, i: 3999] avg mini-batch loss: 1.6164327538013459\n",
      "[epoch: 9, i: 4099] avg mini-batch loss: 1.6144398301839828\n",
      "[epoch: 9, i: 4199] avg mini-batch loss: 1.66135715842247\n",
      "[epoch: 9, i: 4299] avg mini-batch loss: 1.6475686395168305\n",
      "[epoch: 9, i: 4399] avg mini-batch loss: 1.7004895526170731\n",
      "[epoch: 9, i: 4499] avg mini-batch loss: 1.6269183611869813\n",
      "[epoch: 9, i: 4599] avg mini-batch loss: 1.6796169400215148\n",
      "[epoch: 9, i: 4699] avg mini-batch loss: 1.6813142383098603\n",
      "[epoch: 9, i: 4799] avg mini-batch loss: 1.6650921177864075\n",
      "[epoch: 9, i: 4899] avg mini-batch loss: 1.6333278501033783\n",
      "[epoch: 9, i: 4999] avg mini-batch loss: 1.6575480687618256\n",
      "[epoch: 9, i: 5099] avg mini-batch loss: 1.6738215458393098\n",
      "[epoch: 9, i: 5199] avg mini-batch loss: 1.6703676319122314\n",
      "[epoch: 9, i: 5299] avg mini-batch loss: 1.6217974907159804\n",
      "[epoch: 9, i: 5399] avg mini-batch loss: 1.6499144077301025\n",
      "[epoch: 9, i: 5499] avg mini-batch loss: 1.611773223876953\n",
      "[epoch: 9, i: 5599] avg mini-batch loss: 1.6722704392671586\n",
      "[epoch: 9, i: 5699] avg mini-batch loss: 1.6758954155445098\n",
      "[epoch: 9, i: 5799] avg mini-batch loss: 1.6199045741558076\n",
      "[epoch: 9, i: 5899] avg mini-batch loss: 1.607301886677742\n",
      "[epoch: 9, i: 5999] avg mini-batch loss: 1.6772355926036835\n",
      "[epoch: 9, i: 6099] avg mini-batch loss: 1.6042212021350861\n",
      "[epoch: 9, i: 6199] avg mini-batch loss: 1.6434824204444884\n",
      "[epoch: 9, i: 6299] avg mini-batch loss: 1.5715031278133393\n",
      "[epoch: 9, i: 6399] avg mini-batch loss: 1.663508722782135\n",
      "[epoch: 9, i: 6499] avg mini-batch loss: 1.6065655732154847\n",
      "[epoch: 9, i: 6599] avg mini-batch loss: 1.6043429255485535\n",
      "[epoch: 9, i: 6699] avg mini-batch loss: 1.6122674190998076\n",
      "[epoch: 9, i: 6799] avg mini-batch loss: 1.6433330833911897\n",
      "[epoch: 9, i: 6899] avg mini-batch loss: 1.704172101020813\n",
      "[epoch: 9, i: 6999] avg mini-batch loss: 1.659586849808693\n",
      "[epoch: 9, i: 7099] avg mini-batch loss: 1.6280880630016328\n",
      "[epoch: 9, i: 7199] avg mini-batch loss: 1.622509782910347\n",
      "[epoch: 9, i: 7299] avg mini-batch loss: 1.6606679522991181\n",
      "[epoch: 9, i: 7399] avg mini-batch loss: 1.697274342775345\n",
      "[epoch: 9, i: 7499] avg mini-batch loss: 1.6175532752275468\n",
      "[epoch: 9, i: 7599] avg mini-batch loss: 1.6920846980810165\n",
      "[epoch: 9, i: 7699] avg mini-batch loss: 1.6354255729913711\n",
      "[epoch: 9, i: 7799] avg mini-batch loss: 1.637740523815155\n",
      "[epoch: 9, i: 7899] avg mini-batch loss: 1.6579662907123565\n",
      "[epoch: 9, i: 7999] avg mini-batch loss: 1.684444955587387\n",
      "[epoch: 9, i: 8099] avg mini-batch loss: 1.6391643953323365\n",
      "[epoch: 9, i: 8199] avg mini-batch loss: 1.6550020861625672\n",
      "[epoch: 9, i: 8299] avg mini-batch loss: 1.6078948283195496\n",
      "[epoch: 9, i: 8399] avg mini-batch loss: 1.6370502817630768\n",
      "[epoch: 9, i: 8499] avg mini-batch loss: 1.6545901465415955\n",
      "[epoch: 9, i: 8599] avg mini-batch loss: 1.6680071502923965\n",
      "[epoch: 9, i: 8699] avg mini-batch loss: 1.6170755845308304\n",
      "[epoch: 9, i: 8799] avg mini-batch loss: 1.6476474571228028\n",
      "[epoch: 9, i: 8899] avg mini-batch loss: 1.6219573414325714\n",
      "[epoch: 9, i: 8999] avg mini-batch loss: 1.664093782901764\n",
      "[epoch: 9, i: 9099] avg mini-batch loss: 1.6513804191350936\n",
      "[epoch: 9, i: 9199] avg mini-batch loss: 1.65814881503582\n",
      "[epoch: 9, i: 9299] avg mini-batch loss: 1.6211945188045502\n",
      "[epoch: 9, i: 9399] avg mini-batch loss: 1.6352202326059342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 9, i: 9499] avg mini-batch loss: 1.6218756085634232\n",
      "[epoch: 9, i: 9599] avg mini-batch loss: 1.5879722094535829\n",
      "[epoch: 9, i: 9699] avg mini-batch loss: 1.6736328828334808\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, G in enumerate(graph_trainloader, 0):\n",
    "        y=G.y#.cuda()\n",
    "        z=G.x#.cuda()\n",
    "        edge_index=G.edge_index#.cuda()\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        \n",
    "        outputs = model_gcn(z, edge_index)\n",
    "        loss = loss_func(outputs, y)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        opt.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if i % print_freq == print_freq - 1:\n",
    "            avg_loss = running_loss / print_freq\n",
    "            print(f'[epoch: {epoch}, i: {i}] avg mini-batch loss: {avg_loss}')\n",
    "            avg_losses.append(avg_loss)\n",
    "            running_loss = 0.0\n",
    "        \n",
    "#         y_pred=model_gcn(z,edge_index)\n",
    "#         print(y_pred.argmax(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at ../src/models/coco_graph_80_10_3.pth\n"
     ]
    }
   ],
   "source": [
    "PATH = f'../src/models/{fname}.pth'\n",
    "torch.save(model_gcn.state_dict(), PATH)#, _use_new_zipfile_serialization=False)\n",
    "print(f'Model saved at {PATH}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Avg Loss Curves**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.74180901, 3.30108178, 3.18726127, 3.09314609, 3.05046857,\n",
       "       2.94124481, 2.89932525, 2.91138922, 2.81965742, 2.67611179,\n",
       "       2.68950353, 2.63067372, 2.54789731, 2.52701   , 2.42434521,\n",
       "       2.41313235, 2.3006439 , 2.22819111, 2.16117792, 2.18587913,\n",
       "       2.17130347, 2.08066894, 1.99673106, 2.01004496, 1.96495304,\n",
       "       1.94440446, 1.90609401, 1.80269321, 1.79330174, 1.82854661,\n",
       "       1.76738272, 1.7724109 , 1.72534122, 1.65268592, 1.69215985,\n",
       "       1.66554974, 1.67863773, 1.6349474 , 1.66041947, 1.62797758,\n",
       "       1.5917943 , 1.59531796, 1.56180202, 1.55780528, 1.57831382,\n",
       "       1.50112767, 1.52480217, 1.47037713, 1.44180689, 1.48018624,\n",
       "       1.44026997, 1.45638546, 1.48901882, 1.40855195, 1.3793916 ,\n",
       "       1.41695464, 1.37725395, 1.39097082, 1.40737379, 1.37033001,\n",
       "       1.37807844, 1.38463991, 1.43878521, 1.32859889, 1.42404164,\n",
       "       1.40432096, 1.33226773, 1.36402865, 1.34744722, 1.32151281,\n",
       "       1.28552666, 1.35426873, 1.30215849, 1.31525838, 1.32692224,\n",
       "       1.24650933, 1.21540189, 1.29237936, 1.28956451, 1.24141909,\n",
       "       1.28062839, 1.26693816, 1.28424818, 1.24321356, 1.21899463,\n",
       "       1.25547899, 1.20853375, 1.25361031, 1.30207919, 1.25285796,\n",
       "       1.27340225, 1.24510561, 1.2405991 , 1.26107088, 1.13079689,\n",
       "       1.22925058, 1.18539732, 1.22925094, 1.22441269, 1.17096645,\n",
       "       1.16511415, 1.16098276, 1.18266482, 1.1574756 , 1.20940651,\n",
       "       1.12428024, 1.20635175, 1.18040912, 1.14589563, 1.18458385,\n",
       "       1.13533376, 1.09351555, 1.1204697 , 1.14325402, 1.16807858,\n",
       "       1.12924707, 1.12381631, 1.1116749 , 1.16863809, 1.0763012 ,\n",
       "       1.09451911, 1.11514944, 1.16132241, 1.0819515 , 1.10426233,\n",
       "       1.11055383, 1.09732411, 1.10839241, 1.13551251, 1.09040546,\n",
       "       1.0944001 , 1.15182337, 1.06815171, 1.08959213, 1.1149218 ,\n",
       "       1.07823827, 1.09416605, 1.08905916, 1.07118162, 1.10125705,\n",
       "       1.10577949, 1.11115404, 1.07217013, 1.09108833, 1.1305526 ,\n",
       "       1.05551663, 1.06102229, 1.05439549, 1.02690812, 1.07603499,\n",
       "       1.12722472, 1.03704689, 1.07713794, 1.02995626, 1.06887616,\n",
       "       1.00659002, 1.07363033, 1.06754071, 1.06811956, 1.04561092,\n",
       "       1.02907937, 1.07366334, 0.99554166, 1.07293958, 1.08011537,\n",
       "       1.00721384, 0.99685096, 1.04045596, 1.01913216, 1.0691684 ,\n",
       "       1.02163263, 1.02202877, 1.05938844, 1.01838521, 1.08803735,\n",
       "       1.03906805, 1.00499734, 1.04667534, 1.05585815, 1.00690199,\n",
       "       0.99200883, 0.99882628, 1.0540803 , 1.02016701, 1.02304473,\n",
       "       0.99412302, 1.00005283, 1.01446989, 0.97210334, 0.9977191 ,\n",
       "       1.00557037, 0.95917367, 0.99357139, 1.02279413, 0.97353062,\n",
       "       0.99491471, 0.96191365, 0.99065285, 0.98463659, 0.95523848,\n",
       "       0.95772051, 0.95314107, 0.96943568, 0.98006031, 0.98803281,\n",
       "       0.97353067, 1.01721882, 0.89032977, 0.97961939, 0.97712347,\n",
       "       0.94842588, 0.9502743 , 0.98190723, 0.96738637, 0.9629633 ,\n",
       "       0.96102832, 0.95962032, 0.96877704, 0.97258494, 0.93410371,\n",
       "       0.97246427, 0.96277214, 0.9329234 , 0.95900904, 0.89529652,\n",
       "       1.01767091, 0.91412334, 0.93548777, 0.92813813, 0.96219734,\n",
       "       0.92604667, 0.91267848, 0.9363966 , 0.9080919 , 0.94989906,\n",
       "       0.96513843, 0.91207065, 0.88928848, 0.9213777 , 0.94819367,\n",
       "       0.96427522, 0.93892707, 0.90166522, 0.97216034, 0.93738627,\n",
       "       0.88497192, 0.9786921 , 0.92454627, 0.88246164, 0.91933415,\n",
       "       0.92076494, 0.8776046 , 0.9286113 , 0.96360283, 0.90990639,\n",
       "       0.98254955, 0.92577424, 0.93830331, 0.95084624, 0.89293313,\n",
       "       0.94798334, 0.91556131, 0.90376711, 0.92390058, 0.87890558,\n",
       "       0.91319934, 0.97444077, 0.91252868, 0.91971508, 0.84729569,\n",
       "       0.90925899, 0.91466565, 0.89702903, 0.87712354, 0.90060283,\n",
       "       0.95047099, 0.88316596, 0.87215243, 0.87983225, 0.85357815,\n",
       "       0.89312084, 0.91941661, 0.93901817, 0.88848272, 0.94724599,\n",
       "       0.91892088, 0.89363733, 0.88692211, 0.97168666, 0.89813602,\n",
       "       0.88014756, 0.8756005 , 0.87826026, 0.8376473 , 0.90201915,\n",
       "       0.86188618, 0.93570843, 0.837303  , 0.87392228, 0.88396328,\n",
       "       0.86140409, 0.85036541, 0.87106299, 0.86757074, 0.82182665,\n",
       "       0.86016112, 0.85855751, 0.83694913, 0.88147468, 0.85952027,\n",
       "       0.85252725, 0.85304959, 0.86282741, 0.85214887, 0.83531717,\n",
       "       0.85736362, 0.8592902 , 0.86412311, 0.85215477, 0.8474164 ,\n",
       "       0.87061956, 0.83217089, 0.85384265, 0.81784659, 0.85786289,\n",
       "       0.86442844, 0.859319  , 0.81745083, 0.87259271, 0.80649793,\n",
       "       0.86278583, 0.82533583, 0.82223052, 0.80533475, 0.86444051,\n",
       "       0.8263917 , 0.84396416, 0.87446387, 0.82345409, 0.82196717,\n",
       "       0.83107955, 0.82120203, 0.85720362, 0.86030597, 0.79764544,\n",
       "       0.85580825, 0.83028302, 0.89952526, 0.83299775, 0.82434236,\n",
       "       0.8508424 , 0.85394046, 0.84179108, 0.82648272, 0.83937793,\n",
       "       0.83232892, 0.85939539, 0.81062502, 0.84727857, 0.81487679,\n",
       "       0.8464328 , 0.86000722, 0.82996407, 0.82358843, 0.86470692,\n",
       "       0.76610228, 0.87254305, 0.85403948, 0.82775907, 0.81565286,\n",
       "       0.82116181, 0.81222547, 0.79416774, 0.84466688, 0.82730953,\n",
       "       0.81723025, 0.80911143, 0.84361795, 0.80787296, 0.86274086,\n",
       "       0.8352128 , 0.79279911, 0.82647525, 0.82211266, 0.85207454,\n",
       "       0.82421611, 0.8307349 , 0.83808804, 0.83195417, 0.76252279,\n",
       "       0.85623399, 0.77688957, 0.770488  , 0.76659803, 0.83359441,\n",
       "       0.77699105, 0.77192561, 0.85673415, 0.76457888, 0.78921601,\n",
       "       0.79489505, 0.79177864, 0.81263146, 0.79658128, 0.76925864,\n",
       "       0.75092489, 0.77215402, 0.78432332, 0.75588721, 0.79988823,\n",
       "       0.80406028, 0.78228957, 0.7417485 , 0.79242124, 0.82209653,\n",
       "       0.78074954, 0.78869812, 0.81498355, 0.77243131, 0.83013147,\n",
       "       0.80365378, 0.78044807, 0.79439056, 0.79386339, 0.7642424 ,\n",
       "       0.80444433, 0.72461313, 0.76670092, 0.81383703, 0.78986525,\n",
       "       0.828774  , 0.77699238, 0.78941002, 0.78893872, 0.80365975,\n",
       "       0.76977261, 0.78521317, 0.77652129, 0.75326928, 0.79532772,\n",
       "       0.77290088, 0.76367538, 0.80148201, 0.76808859, 0.76838398,\n",
       "       0.74843591, 0.76172954, 0.78265178, 0.80372094, 0.77845593,\n",
       "       0.74638805, 0.79640252, 0.80005376, 0.76646641, 0.75817719,\n",
       "       0.76373283, 0.75108253, 0.75630042, 0.80245896, 0.75915676,\n",
       "       0.73860055, 0.79646806, 0.79466392, 0.77981287, 0.7987637 ,\n",
       "       0.72497627, 0.75574109, 0.76113018, 0.78169261, 0.74212836,\n",
       "       0.78424947, 0.78851197, 0.76552912, 0.75524072, 0.76157731,\n",
       "       0.71129702, 0.75731756, 0.74863756, 0.79725687, 0.78365565,\n",
       "       0.79976584, 0.74433652, 0.78299978, 0.73966824, 0.78794864,\n",
       "       0.71353274, 0.71459492, 0.74196265, 0.72361939, 0.75863178,\n",
       "       0.73705747, 0.76004839, 0.7233687 , 0.77057813, 0.73182183,\n",
       "       0.69656637, 0.71951523, 0.7360142 , 0.75001771, 0.74938276,\n",
       "       0.70275622, 0.74652743, 0.74266537, 0.69125394, 0.73160158,\n",
       "       0.71912244, 0.75930349, 0.71671399, 0.73825391, 0.75545818,\n",
       "       0.74883959, 0.7373052 , 0.73194701, 0.70855505, 0.72827605,\n",
       "       0.76637892, 0.7316478 , 0.71217016, 0.76093544, 0.70074717,\n",
       "       0.75216289, 0.72677688, 0.75236324, 0.74614827, 0.75707365,\n",
       "       0.72212266, 0.73114168, 0.70763594, 0.72563235, 0.74049787,\n",
       "       0.70557402, 0.66943473, 0.73510905, 0.73082587, 0.74498262,\n",
       "       0.7193113 , 0.72299685, 0.71391174, 0.6994537 , 0.73555885,\n",
       "       0.77844676, 0.7243088 , 0.72486216, 0.7632199 , 0.73380812,\n",
       "       0.72748246, 0.73274478, 0.71164758, 0.75306658, 0.73837819,\n",
       "       0.74580156, 0.69765346, 0.72663759, 0.72963151, 0.70276719,\n",
       "       0.74572937, 0.70647277, 0.7210892 , 0.73055835, 0.67731339,\n",
       "       0.69732105, 0.71903513, 0.73507515, 0.7675311 , 0.70303583,\n",
       "       0.71208307, 0.76190847, 0.65729327, 0.72293048, 0.73285279,\n",
       "       0.76761642, 0.72704704, 0.72829033, 0.76693439, 0.70353328,\n",
       "       0.73092211, 0.70453284, 0.77080634, 0.71263106, 0.70787644,\n",
       "       0.69839697, 0.72594914, 0.67910167, 0.69305299, 0.70010201,\n",
       "       0.67779062, 0.67953418, 0.66788531, 0.67712169, 0.68635885,\n",
       "       0.73245635, 0.69018902, 0.67364383, 0.66303376, 0.66959631,\n",
       "       0.68873193, 0.67699065, 0.70395669, 0.68681336, 0.66659053,\n",
       "       0.67870929, 0.69073793, 0.69174109, 0.64688368, 0.69600028,\n",
       "       0.65096105, 0.71918054, 0.68576781, 0.66019411, 0.720031  ,\n",
       "       0.74331643, 0.72659054, 0.67499319, 0.67232483, 0.71405385,\n",
       "       0.64049355, 0.66231521, 0.70778452, 0.6711264 , 0.65784972,\n",
       "       0.67955346, 0.64877111, 0.70080119, 0.71607127, 0.68434864,\n",
       "       0.67404645, 0.69030489, 0.66997679, 0.68995933, 0.67654256,\n",
       "       0.67534643, 0.6485713 , 0.69660735, 0.71608517, 0.67885265,\n",
       "       0.70237891, 0.68444359, 0.68291464, 0.67884027, 0.69951587,\n",
       "       0.69819941, 0.67583272, 0.66357732, 0.68599369, 0.71956592,\n",
       "       0.71134947, 0.66637049, 0.6556648 , 0.61038652, 0.69224283,\n",
       "       0.64295657, 0.68291552, 0.68705274, 0.6716504 , 0.6540284 ,\n",
       "       0.69971998, 0.68697519, 0.69122553, 0.6883479 , 0.65772407,\n",
       "       0.69465391, 0.65563606, 0.70908565, 0.68508596, 0.69707077,\n",
       "       0.66376413, 0.68773846, 0.66541786, 0.6833236 , 0.68640667,\n",
       "       0.6988948 , 0.71375529, 0.68353392, 0.69212722, 0.6761573 ,\n",
       "       0.67928305, 0.65144339, 0.68268039, 0.68801041, 0.67112081,\n",
       "       0.61919327, 0.63656254, 0.62240272, 0.62729823, 0.63076003,\n",
       "       0.64432903, 0.66439235, 0.64504908, 0.58815242, 0.66415731,\n",
       "       0.67399231, 0.63111585, 0.63714567, 0.61641614, 0.60950293,\n",
       "       0.63700302, 0.64065948, 0.68360163, 0.65825295, 0.64494568,\n",
       "       0.69205913, 0.60317982, 0.67326038, 0.62281078, 0.62349897,\n",
       "       0.6386994 , 0.62067288, 0.69808674, 0.65553571, 0.61808695,\n",
       "       0.67118863, 0.62578417, 0.62880199, 0.64370731, 0.6646594 ,\n",
       "       0.64009568, 0.66684567, 0.65770594, 0.61570316, 0.65191392,\n",
       "       0.62086428, 0.62773287, 0.68288257, 0.6425588 , 0.65760542,\n",
       "       0.62516388, 0.62283178, 0.63851179, 0.66199789, 0.66581844,\n",
       "       0.62085639, 0.64888203, 0.64778927, 0.69197616, 0.64613519,\n",
       "       0.64962149, 0.62192713, 0.62487507, 0.67770128, 0.63492393,\n",
       "       0.61740831, 0.66487898, 0.63816927, 0.60902223, 0.64645125,\n",
       "       0.64483218, 0.6433814 , 0.6731994 , 0.62862744, 0.67109233,\n",
       "       0.69082261, 0.63770075, 0.63817271, 0.64555134, 0.64556034,\n",
       "       0.68430752, 0.66391411, 0.69977002, 0.63802973, 0.61438865,\n",
       "       0.62619239, 0.67986882, 0.67854704, 0.66565233, 0.63495963,\n",
       "       0.64468932, 0.65542049, 0.61601629, 0.62237841, 0.63627664,\n",
       "       0.65449646, 0.65709861, 0.63879242, 0.63105171, 0.59113965,\n",
       "       0.65236003, 0.63359031, 0.61616804, 0.62075746, 0.59376231,\n",
       "       0.61089715, 0.62281086, 0.59562508, 0.61286414, 0.5803473 ,\n",
       "       0.62065221, 0.58010735, 0.60706103, 0.60515791, 0.625063  ,\n",
       "       0.60970903, 0.60228721, 0.62742595, 0.61202081, 0.61630002,\n",
       "       0.62752531, 0.6107222 , 0.60482405, 0.60143296, 0.57758308,\n",
       "       0.59783423, 0.64212104, 0.61222186, 0.59356484, 0.5969399 ,\n",
       "       0.62177271, 0.64388624, 0.60908828, 0.61789386, 0.68476729,\n",
       "       0.59456755, 0.611155  , 0.61031582, 0.62328402, 0.59322236,\n",
       "       0.62162256, 0.62778497, 0.60259827, 0.63136291, 0.64093262,\n",
       "       0.60746492, 0.63052372, 0.59471768, 0.63538082, 0.56817902,\n",
       "       0.60743059, 0.63166783, 0.57049716, 0.61614097, 0.61926183,\n",
       "       0.62994579, 0.6042373 , 0.62051735, 0.6113912 , 0.59933909,\n",
       "       0.59137666, 0.61939563, 0.62040524, 0.57967508, 0.60253461,\n",
       "       0.57941848, 0.59589472, 0.6439113 , 0.59710854, 0.59792033,\n",
       "       0.60316118, 0.61605027, 0.61214545, 0.59847992, 0.6273795 ,\n",
       "       0.65563202, 0.61798095, 0.58778407, 0.638715  , 0.59915649,\n",
       "       0.60171139, 0.62521408, 0.57066812, 0.58692968, 0.5977905 ,\n",
       "       0.63896821, 0.6359867 , 0.61135695, 0.59189305, 0.62217684,\n",
       "       0.61488668, 0.5830639 , 0.57181723, 0.61559312, 0.59425962,\n",
       "       0.56832161, 0.63823149, 0.62358879, 0.55678252, 0.59851812,\n",
       "       0.57355943, 0.5904379 , 0.60454734, 0.5623609 , 0.60711226,\n",
       "       0.59579126, 0.59235525, 0.57322996, 0.61182729, 0.60327769,\n",
       "       0.54108233, 0.53897234, 0.56844475, 0.59455149, 0.5621325 ,\n",
       "       0.56123876, 0.57084732, 0.54241034, 0.59286112, 0.56365004,\n",
       "       0.58986058, 0.61979448, 0.60735704, 0.59839784, 0.57352889,\n",
       "       0.54300504, 0.60203485, 0.57848504, 0.55899371, 0.55549868,\n",
       "       0.58708899, 0.57624931, 0.61026612, 0.60794464, 0.61996893,\n",
       "       0.58422188, 0.59524002, 0.58501575, 0.54285374, 0.57626956,\n",
       "       0.56382861, 0.57271942, 0.59575478, 0.54656912, 0.57218394,\n",
       "       0.5824973 , 0.58345161, 0.55423449, 0.5657996 , 0.61688404,\n",
       "       0.56412643, 0.61216585, 0.58400794, 0.57408809, 0.58626441,\n",
       "       0.58026691, 0.58445548, 0.56060944, 0.57365818, 0.58232794,\n",
       "       0.55893884, 0.55622781, 0.56019731, 0.55400206, 0.5535328 ,\n",
       "       0.59186208, 0.54555021, 0.59935124, 0.5741179 , 0.60104003,\n",
       "       0.57340666, 0.60752606, 0.53691196, 0.54937781, 0.58013989,\n",
       "       0.54731132, 0.56888024, 0.57351625, 0.55072267, 0.59287676,\n",
       "       0.5757009 , 0.54046046, 0.54737005, 0.56463073, 0.61226583,\n",
       "       0.56413493, 0.59569306, 0.57588115, 0.57262339, 0.57251563,\n",
       "       0.57269006, 0.63514614, 0.59516642, 0.58055794, 0.5832226 ])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3 = list(pd.read_csv('../data/out/avg_losses_80_10_resnet50.csv', usecols=['0']).values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "970"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(avg_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(avg_losses).to_csv(f'../data/out/avg_losses_{fname}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in epochs:\n",
    "#     for G in graph_dataloader:\n",
    "#         y=G.y.cuda()\n",
    "#         z=G.x.cuda()\n",
    "#         edge_index=G.edge_index.cuda()\n",
    "#         y_pred=model_gcn(z,edge_index)\n",
    "#         print(y_pred.argmax(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchgeo",
   "language": "python",
   "name": "torchgeo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
