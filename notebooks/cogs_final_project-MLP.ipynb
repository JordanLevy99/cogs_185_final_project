{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch_geometric\n",
    "import sys\n",
    "sys.path.insert(0, '../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import json\n",
    "import argparse\n",
    "# import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Function\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from torchvision.datasets import CocoDetection\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import sys\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '../src/')\n",
    "from graph_network import GCNNet\n",
    "from torch_geometric.data import DataLoader as TG_DataLoader\n",
    "\n",
    "from dataset_class import *\n",
    "from etl import *\n",
    "\n",
    "sys.path.insert(0, '../src/data/cocoapi/PythonAPI')\n",
    "from pycocotools.coco import COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_gcn=GCNNet(2048,80, [32]*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mlp=nn.Sequential(nn.Sequential(nn.Linear(2048,32),nn.ReLU(),nn.Dropout(p=0.3)),nn.Linear(32,80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_gcn.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (0): Linear(in_features=2048, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (1): Linear(in_features=32, out_features=80, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (0): Linear(in_features=2048, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (1): Linear(in_features=32, out_features=80, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! jupyter nbextension enable --py widgetsnbextension\n",
    "# ! python -c \"from torchvision import models; model = models.resnet50(pretrained=True)\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jdlevy/COGS_185/cogs_185_final_project/notebooks'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jdlevy/COGS_185/cogs_185_final_project/notebooks'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading in resnet pretrained\n",
    "model = models.resnet50(pretrained=False)\n",
    "state_dict=torch.load(\"../src/models/resnet50-19c8e357.pth\")\n",
    "model.load_state_dict(state_dict)\n",
    "# model.fc = nn.Linear(2048, 80)\n",
    "# Flatten to get 2048 embeddings\n",
    "model.fc=nn.Flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! git clone https://github.com/cocodataset/cocoapi.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class cocoDataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, root, annotation, transforms=None):\n",
    "#         self.root = root\n",
    "#         self.transforms = transforms\n",
    "#         self.coco = COCO(annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: add target transform to convert labels to proper labels using id_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.listdir('/datasets/COCO-2017/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = '/datasets/COCO-2017/train2017'\n",
    "val_dir = '/datasets/COCO-2017/val2017'\n",
    "\n",
    "train_ann = '/datasets/COCO-2017/anno2017/instances_train2017.json'\n",
    "val_ann = '/datasets/COCO-2017/anno2017/instances_val2017.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.81s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "val_coco, val_data = load_data(val_ann)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jdlevy/COGS_185/cogs_185_final_project/notebooks'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../src/data/id_dict.pkl', 'rb') as f:\n",
    "    id_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform():\n",
    "    custom_transforms = []\n",
    "    custom_transforms.append(torchvision.transforms.Resize(size=(128, 128)))\n",
    "    custom_transforms.append(torchvision.transforms.ToTensor())\n",
    "    return torchvision.transforms.Compose(custom_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = '/datasets/COCO-2017/train2017'\n",
    "val_dir = '/datasets/COCO-2017/val2017'\n",
    "\n",
    "train_ann = '/datasets/COCO-2017/anno2017/instances_train2017.json'\n",
    "val_ann = '/datasets/COCO-2017/anno2017/instances_val2017.json'\n",
    "\n",
    "clean_train_ann = '../data/temp/annotations/clean_instances_train2017.json'\n",
    "clean_val_ann = '../data/temp/annotations/clean_instances_val2017.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_ids(target):\n",
    "    return id_dict[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=19.65s)\n",
      "creating index...\n",
      "index created!\n",
      "CPU times: user 20.4 s, sys: 2.7 s, total: 23.1 s\n",
      "Wall time: 23 s\n",
      "loading annotations into memory...\n",
      "Done (t=0.72s)\n",
      "creating index...\n",
      "index created!\n",
      "CPU times: user 663 ms, sys: 102 ms, total: 765 ms\n",
      "Wall time: 776 ms\n"
     ]
    }
   ],
   "source": [
    "%time train_set = cocoDataset(root=train_dir,\\\n",
    "                      annotation=clean_train_ann,\\\n",
    "                      transforms=get_transform(),\\\n",
    "                      target_transform=convert_ids)\n",
    "\n",
    "%time val_set = cocoDataset(root=val_dir,\\\n",
    "                      annotation=clean_val_ann,\\\n",
    "                      transforms=get_transform(),\\\n",
    "                      target_transform=convert_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Params cell\n",
    "num_epoch = 10\n",
    "fname = 'resnet50_80_10'\n",
    "train_batch_size = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_set,\n",
    "                                          batch_size=train_batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=8,\n",
    "                                          collate_fn=collate_fn)\n",
    "\n",
    "valloader = torch.utils.data.DataLoader(val_set,\n",
    "                                        batch_size=train_batch_size,\n",
    "                                        shuffle=False,\n",
    "                                        num_workers=8,\n",
    "                                        collate_fn=collate_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, data in enumerate(trainloader, 0):\n",
    "#     f = model(data[0])\n",
    "#     print(f.shape)\n",
    "#     o = model_mlp(f)\n",
    "#     print(o.shape)\n",
    "# #     print(data[0].shape)\n",
    "# #     print(data[1].shape)\n",
    "# #     print(data[2].shape)\n",
    "#     print(len(data[3]))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "model parameters sent to cuda:0\n",
      "[epoch: 0, i: 99] avg mini-batch loss: 4.361015701293946\n",
      "[epoch: 0, i: 199] avg mini-batch loss: 4.353169994354248\n",
      "[epoch: 0, i: 299] avg mini-batch loss: 4.343806920051574\n",
      "[epoch: 0, i: 399] avg mini-batch loss: 4.329608383178711\n",
      "[epoch: 0, i: 499] avg mini-batch loss: 4.325777707099914\n",
      "[epoch: 0, i: 599] avg mini-batch loss: 4.307078580856324\n",
      "[epoch: 0, i: 699] avg mini-batch loss: 4.285385375022888\n",
      "[epoch: 0, i: 799] avg mini-batch loss: 4.254860320091248\n",
      "[epoch: 0, i: 899] avg mini-batch loss: 4.221631302833557\n",
      "[epoch: 0, i: 999] avg mini-batch loss: 4.165383124351502\n",
      "[epoch: 0, i: 1099] avg mini-batch loss: 4.111093707084656\n",
      "[epoch: 0, i: 1199] avg mini-batch loss: 4.064686479568482\n",
      "[epoch: 0, i: 1299] avg mini-batch loss: 3.9829866695404053\n",
      "[epoch: 0, i: 1399] avg mini-batch loss: 3.9377061319351196\n",
      "[epoch: 0, i: 1499] avg mini-batch loss: 3.8634119391441346\n",
      "[epoch: 0, i: 1599] avg mini-batch loss: 3.7918328714370726\n",
      "[epoch: 0, i: 1699] avg mini-batch loss: 3.7707859897613525\n",
      "[epoch: 0, i: 1799] avg mini-batch loss: 3.753729536533356\n",
      "[epoch: 0, i: 1899] avg mini-batch loss: 3.715779309272766\n",
      "[epoch: 0, i: 1999] avg mini-batch loss: 3.68228985786438\n",
      "[epoch: 0, i: 2099] avg mini-batch loss: 3.657432143688202\n",
      "[epoch: 0, i: 2199] avg mini-batch loss: 3.6309728121757505\n",
      "[epoch: 0, i: 2299] avg mini-batch loss: 3.6424844622612\n",
      "[epoch: 0, i: 2399] avg mini-batch loss: 3.6002863526344298\n",
      "[epoch: 0, i: 2499] avg mini-batch loss: 3.5369107937812805\n",
      "[epoch: 0, i: 2599] avg mini-batch loss: 3.600144865512848\n",
      "[epoch: 0, i: 2699] avg mini-batch loss: 3.599316430091858\n",
      "[epoch: 0, i: 2799] avg mini-batch loss: 3.560016746520996\n",
      "[epoch: 0, i: 2899] avg mini-batch loss: 3.5610054397583006\n",
      "[epoch: 0, i: 2999] avg mini-batch loss: 3.533090512752533\n",
      "[epoch: 0, i: 3099] avg mini-batch loss: 3.5742790412902834\n",
      "[epoch: 0, i: 3199] avg mini-batch loss: 3.5548630213737487\n",
      "[epoch: 0, i: 3299] avg mini-batch loss: 3.565794200897217\n",
      "[epoch: 0, i: 3399] avg mini-batch loss: 3.5439681005477905\n",
      "[epoch: 0, i: 3499] avg mini-batch loss: 3.5038376879692077\n",
      "[epoch: 0, i: 3599] avg mini-batch loss: 3.5216615629196166\n",
      "[epoch: 0, i: 3699] avg mini-batch loss: 3.5121351790428164\n",
      "[epoch: 0, i: 3799] avg mini-batch loss: 3.548329713344574\n",
      "[epoch: 0, i: 3899] avg mini-batch loss: 3.4862802386283875\n",
      "[epoch: 0, i: 3999] avg mini-batch loss: 3.4951378750801085\n",
      "[epoch: 0, i: 4099] avg mini-batch loss: 3.47690896987915\n",
      "[epoch: 0, i: 4199] avg mini-batch loss: 3.4684134984016417\n",
      "[epoch: 0, i: 4299] avg mini-batch loss: 3.4533978700637817\n",
      "[epoch: 0, i: 4399] avg mini-batch loss: 3.463930354118347\n",
      "[epoch: 0, i: 4499] avg mini-batch loss: 3.5582710576057432\n",
      "[epoch: 0, i: 4599] avg mini-batch loss: 3.468568048477173\n",
      "[epoch: 0, i: 4699] avg mini-batch loss: 3.5479516220092773\n",
      "[epoch: 0, i: 4799] avg mini-batch loss: 3.4377346539497378\n",
      "[epoch: 0, i: 4899] avg mini-batch loss: 3.4748680877685545\n",
      "[epoch: 0, i: 4999] avg mini-batch loss: 3.514906189441681\n",
      "[epoch: 0, i: 5099] avg mini-batch loss: 3.441361129283905\n",
      "[epoch: 0, i: 5199] avg mini-batch loss: 3.459914982318878\n",
      "[epoch: 0, i: 5299] avg mini-batch loss: 3.4595121788978576\n",
      "[epoch: 0, i: 5399] avg mini-batch loss: 3.433209726810455\n",
      "[epoch: 0, i: 5499] avg mini-batch loss: 3.4673591542243956\n",
      "[epoch: 0, i: 5599] avg mini-batch loss: 3.418762092590332\n",
      "[epoch: 0, i: 5699] avg mini-batch loss: 3.452246880531311\n",
      "[epoch: 0, i: 5799] avg mini-batch loss: 3.474705295562744\n",
      "[epoch: 0, i: 5899] avg mini-batch loss: 3.4629973196983337\n",
      "[epoch: 0, i: 5999] avg mini-batch loss: 3.445001969337463\n",
      "[epoch: 0, i: 6099] avg mini-batch loss: 3.458840322494507\n",
      "[epoch: 0, i: 6199] avg mini-batch loss: 3.4654473161697386\n",
      "[epoch: 0, i: 6299] avg mini-batch loss: 3.459057011604309\n",
      "[epoch: 0, i: 6399] avg mini-batch loss: 3.456722891330719\n",
      "[epoch: 0, i: 6499] avg mini-batch loss: 3.4861276054382326\n",
      "[epoch: 0, i: 6599] avg mini-batch loss: 3.413780515193939\n",
      "[epoch: 0, i: 6699] avg mini-batch loss: 3.4028216004371643\n",
      "[epoch: 0, i: 6799] avg mini-batch loss: 3.5477912640571594\n",
      "[epoch: 0, i: 6899] avg mini-batch loss: 3.41382018327713\n",
      "[epoch: 0, i: 6999] avg mini-batch loss: 3.4238012075424193\n",
      "[epoch: 0, i: 7099] avg mini-batch loss: 3.397429356575012\n",
      "[epoch: 0, i: 7199] avg mini-batch loss: 3.3833904337882994\n",
      "[epoch: 0, i: 7299] avg mini-batch loss: 3.4460604190826416\n",
      "[epoch: 0, i: 7399] avg mini-batch loss: 3.418469443321228\n",
      "[epoch: 0, i: 7499] avg mini-batch loss: 3.3882137155532837\n",
      "[epoch: 0, i: 7599] avg mini-batch loss: 3.441899378299713\n",
      "[epoch: 0, i: 7699] avg mini-batch loss: 3.368158133029938\n",
      "[epoch: 0, i: 7799] avg mini-batch loss: 3.3967241430282593\n",
      "[epoch: 0, i: 7899] avg mini-batch loss: 3.414590587615967\n",
      "[epoch: 0, i: 7999] avg mini-batch loss: 3.3736564779281615\n",
      "[epoch: 0, i: 8099] avg mini-batch loss: 3.4036988019943237\n",
      "[epoch: 0, i: 8199] avg mini-batch loss: 3.388769812583923\n",
      "[epoch: 0, i: 8299] avg mini-batch loss: 3.3863496208190917\n",
      "[epoch: 0, i: 8399] avg mini-batch loss: 3.366085801124573\n",
      "[epoch: 0, i: 8499] avg mini-batch loss: 3.441961631774902\n",
      "[epoch: 0, i: 8599] avg mini-batch loss: 3.4189609813690187\n",
      "[epoch: 0, i: 8699] avg mini-batch loss: 3.417273280620575\n",
      "[epoch: 0, i: 8799] avg mini-batch loss: 3.489395754337311\n",
      "[epoch: 0, i: 8899] avg mini-batch loss: 3.3531978726387024\n",
      "[epoch: 0, i: 8999] avg mini-batch loss: 3.3911725068092347\n",
      "[epoch: 0, i: 9099] avg mini-batch loss: 3.356858108043671\n",
      "[epoch: 0, i: 9199] avg mini-batch loss: 3.339752459526062\n",
      "[epoch: 0, i: 9299] avg mini-batch loss: 3.3802745580673217\n",
      "[epoch: 0, i: 9399] avg mini-batch loss: 3.3774368238449095\n",
      "[epoch: 0, i: 9499] avg mini-batch loss: 3.383625001907349\n",
      "[epoch: 0, i: 9599] avg mini-batch loss: 3.3239004516601565\n",
      "[epoch: 0, i: 9699] avg mini-batch loss: 3.338827464580536\n",
      "[epoch: 1, i: 99] avg mini-batch loss: 3.348241379261017\n",
      "[epoch: 1, i: 199] avg mini-batch loss: 3.345935719013214\n",
      "[epoch: 1, i: 299] avg mini-batch loss: 3.3616099643707273\n",
      "[epoch: 1, i: 399] avg mini-batch loss: 3.3848911929130554\n",
      "[epoch: 1, i: 499] avg mini-batch loss: 3.278327100276947\n",
      "[epoch: 1, i: 599] avg mini-batch loss: 3.381247570514679\n",
      "[epoch: 1, i: 699] avg mini-batch loss: 3.3495733642578127\n",
      "[epoch: 1, i: 799] avg mini-batch loss: 3.2869017052650453\n",
      "[epoch: 1, i: 899] avg mini-batch loss: 3.3217477416992187\n",
      "[epoch: 1, i: 999] avg mini-batch loss: 3.2939952397346497\n",
      "[epoch: 1, i: 1099] avg mini-batch loss: 3.2682195854187013\n",
      "[epoch: 1, i: 1199] avg mini-batch loss: 3.358392207622528\n",
      "[epoch: 1, i: 1299] avg mini-batch loss: 3.319085476398468\n",
      "[epoch: 1, i: 1399] avg mini-batch loss: 3.3217452573776245\n",
      "[epoch: 1, i: 1499] avg mini-batch loss: 3.33089022397995\n",
      "[epoch: 1, i: 1599] avg mini-batch loss: 3.254283971786499\n",
      "[epoch: 1, i: 1699] avg mini-batch loss: 3.317236087322235\n",
      "[epoch: 1, i: 1799] avg mini-batch loss: 3.3040366101264955\n",
      "[epoch: 1, i: 1899] avg mini-batch loss: 3.3051895761489867\n",
      "[epoch: 1, i: 1999] avg mini-batch loss: 3.3065605211257934\n",
      "[epoch: 1, i: 2099] avg mini-batch loss: 3.3449094581604\n",
      "[epoch: 1, i: 2199] avg mini-batch loss: 3.295961632728577\n",
      "[epoch: 1, i: 2299] avg mini-batch loss: 3.2709907627105714\n",
      "[epoch: 1, i: 2399] avg mini-batch loss: 3.274749743938446\n",
      "[epoch: 1, i: 2499] avg mini-batch loss: 3.2922711110115053\n",
      "[epoch: 1, i: 2599] avg mini-batch loss: 3.3017689108848574\n",
      "[epoch: 1, i: 2699] avg mini-batch loss: 3.3089102482795716\n",
      "[epoch: 1, i: 2799] avg mini-batch loss: 3.289557108879089\n",
      "[epoch: 1, i: 2899] avg mini-batch loss: 3.3440645432472227\n",
      "[epoch: 1, i: 2999] avg mini-batch loss: 3.3651642751693727\n",
      "[epoch: 1, i: 3099] avg mini-batch loss: 3.2480517077445983\n",
      "[epoch: 1, i: 3199] avg mini-batch loss: 3.2285105705261232\n",
      "[epoch: 1, i: 3299] avg mini-batch loss: 3.2620280718803407\n",
      "[epoch: 1, i: 3399] avg mini-batch loss: 3.324443531036377\n",
      "[epoch: 1, i: 3499] avg mini-batch loss: 3.3057441544532775\n",
      "[epoch: 1, i: 3599] avg mini-batch loss: 3.213687241077423\n",
      "[epoch: 1, i: 3699] avg mini-batch loss: 3.26892165184021\n",
      "[epoch: 1, i: 3799] avg mini-batch loss: 3.282834942340851\n",
      "[epoch: 1, i: 3899] avg mini-batch loss: 3.266138801574707\n",
      "[epoch: 1, i: 3999] avg mini-batch loss: 3.302503080368042\n",
      "[epoch: 1, i: 4099] avg mini-batch loss: 3.1862762928009034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 1, i: 4199] avg mini-batch loss: 3.2147447633743287\n",
      "[epoch: 1, i: 4299] avg mini-batch loss: 3.2598787546157837\n",
      "[epoch: 1, i: 4399] avg mini-batch loss: 3.2415180516242983\n",
      "[epoch: 1, i: 4499] avg mini-batch loss: 3.2261087536811828\n",
      "[epoch: 1, i: 4599] avg mini-batch loss: 3.2079469847679136\n",
      "[epoch: 1, i: 4699] avg mini-batch loss: 3.2217109036445617\n",
      "[epoch: 1, i: 4799] avg mini-batch loss: 3.254996085166931\n",
      "[epoch: 1, i: 4899] avg mini-batch loss: 3.2156294703483583\n",
      "[epoch: 1, i: 4999] avg mini-batch loss: 3.192521963119507\n",
      "[epoch: 1, i: 5099] avg mini-batch loss: 3.240382294654846\n",
      "[epoch: 1, i: 5199] avg mini-batch loss: 3.177373456954956\n",
      "[epoch: 1, i: 5299] avg mini-batch loss: 3.189113526344299\n",
      "[epoch: 1, i: 5399] avg mini-batch loss: 3.2016156363487243\n",
      "[epoch: 1, i: 5499] avg mini-batch loss: 3.27059951543808\n",
      "[epoch: 1, i: 5599] avg mini-batch loss: 3.2239147663116454\n",
      "[epoch: 1, i: 5699] avg mini-batch loss: 3.2098137855529787\n",
      "[epoch: 1, i: 5799] avg mini-batch loss: 3.2058308506011963\n",
      "[epoch: 1, i: 5899] avg mini-batch loss: 3.246764681339264\n",
      "[epoch: 1, i: 5999] avg mini-batch loss: 3.2444835710525513\n",
      "[epoch: 1, i: 6099] avg mini-batch loss: 3.190198292732239\n",
      "[epoch: 1, i: 6199] avg mini-batch loss: 3.2124483704566957\n",
      "[epoch: 1, i: 6299] avg mini-batch loss: 3.216412613391876\n",
      "[epoch: 1, i: 6399] avg mini-batch loss: 3.246418650150299\n",
      "[epoch: 1, i: 6499] avg mini-batch loss: 3.2161005616188048\n",
      "[epoch: 1, i: 6599] avg mini-batch loss: 3.1365981793403623\n",
      "[epoch: 1, i: 6699] avg mini-batch loss: 3.16856965303421\n",
      "[epoch: 1, i: 6799] avg mini-batch loss: 3.205522210597992\n",
      "[epoch: 1, i: 6899] avg mini-batch loss: 3.1530640816688535\n",
      "[epoch: 1, i: 6999] avg mini-batch loss: 3.129007077217102\n",
      "[epoch: 1, i: 7099] avg mini-batch loss: 3.207246968746185\n",
      "[epoch: 1, i: 7199] avg mini-batch loss: 3.212589073181152\n",
      "[epoch: 1, i: 7299] avg mini-batch loss: 3.1947689771652223\n",
      "[epoch: 1, i: 7399] avg mini-batch loss: 3.208696491718292\n",
      "[epoch: 1, i: 7499] avg mini-batch loss: 3.1344555497169493\n",
      "[epoch: 1, i: 7599] avg mini-batch loss: 3.150498971939087\n",
      "[epoch: 1, i: 7699] avg mini-batch loss: 3.127760796546936\n",
      "[epoch: 1, i: 7799] avg mini-batch loss: 3.1254712653160097\n",
      "[epoch: 1, i: 7899] avg mini-batch loss: 3.184698965549469\n",
      "[epoch: 1, i: 7999] avg mini-batch loss: 3.146758131980896\n",
      "[epoch: 1, i: 8099] avg mini-batch loss: 3.164370985031128\n",
      "[epoch: 1, i: 8199] avg mini-batch loss: 3.1593243432044984\n",
      "[epoch: 1, i: 8299] avg mini-batch loss: 3.105956697463989\n",
      "[epoch: 1, i: 8399] avg mini-batch loss: 3.1694357204437256\n",
      "[epoch: 1, i: 8499] avg mini-batch loss: 3.096339855194092\n",
      "[epoch: 1, i: 8599] avg mini-batch loss: 3.136158664226532\n",
      "[epoch: 1, i: 8699] avg mini-batch loss: 3.166928219795227\n",
      "[epoch: 1, i: 8799] avg mini-batch loss: 3.175145001411438\n",
      "[epoch: 1, i: 8899] avg mini-batch loss: 3.169178764820099\n",
      "[epoch: 1, i: 8999] avg mini-batch loss: 3.15416574716568\n",
      "[epoch: 1, i: 9099] avg mini-batch loss: 3.1751556062698363\n",
      "[epoch: 1, i: 9199] avg mini-batch loss: 3.1618962717056274\n",
      "[epoch: 1, i: 9299] avg mini-batch loss: 3.1214678287506104\n",
      "[epoch: 1, i: 9399] avg mini-batch loss: 3.1096893048286436\n",
      "[epoch: 1, i: 9499] avg mini-batch loss: 3.1281905937194825\n",
      "[epoch: 1, i: 9599] avg mini-batch loss: 3.1405414032936094\n",
      "[epoch: 1, i: 9699] avg mini-batch loss: 3.1148174595832825\n",
      "[epoch: 2, i: 99] avg mini-batch loss: 3.058381178379059\n",
      "[epoch: 2, i: 199] avg mini-batch loss: 3.124284324645996\n",
      "[epoch: 2, i: 299] avg mini-batch loss: 3.1388039064407347\n",
      "[epoch: 2, i: 399] avg mini-batch loss: 3.130691659450531\n",
      "[epoch: 2, i: 499] avg mini-batch loss: 3.14623939037323\n",
      "[epoch: 2, i: 599] avg mini-batch loss: 3.142168300151825\n",
      "[epoch: 2, i: 699] avg mini-batch loss: 3.089790096282959\n",
      "[epoch: 2, i: 799] avg mini-batch loss: 3.0896881079673766\n",
      "[epoch: 2, i: 899] avg mini-batch loss: 3.1150206542015075\n",
      "[epoch: 2, i: 999] avg mini-batch loss: 3.0593623661994935\n",
      "[epoch: 2, i: 1099] avg mini-batch loss: 3.080628991127014\n",
      "[epoch: 2, i: 1199] avg mini-batch loss: 3.1134754824638367\n",
      "[epoch: 2, i: 1299] avg mini-batch loss: 3.1212843465805054\n",
      "[epoch: 2, i: 1399] avg mini-batch loss: 3.1290383672714235\n",
      "[epoch: 2, i: 1499] avg mini-batch loss: 3.0352519583702087\n",
      "[epoch: 2, i: 1599] avg mini-batch loss: 3.0942982745170595\n",
      "[epoch: 2, i: 1699] avg mini-batch loss: 3.0402480602264403\n",
      "[epoch: 2, i: 1799] avg mini-batch loss: 3.041535077095032\n",
      "[epoch: 2, i: 1899] avg mini-batch loss: 3.0976498031616213\n",
      "[epoch: 2, i: 1999] avg mini-batch loss: 3.098773231506348\n",
      "[epoch: 2, i: 2099] avg mini-batch loss: 3.055948257446289\n",
      "[epoch: 2, i: 2199] avg mini-batch loss: 3.046077506542206\n",
      "[epoch: 2, i: 2299] avg mini-batch loss: 3.0813912034034727\n",
      "[epoch: 2, i: 2399] avg mini-batch loss: 3.1081052947044374\n",
      "[epoch: 2, i: 2499] avg mini-batch loss: 3.0371176481246946\n",
      "[epoch: 2, i: 2599] avg mini-batch loss: 3.0926724553108214\n",
      "[epoch: 2, i: 2699] avg mini-batch loss: 3.08521915435791\n",
      "[epoch: 2, i: 2799] avg mini-batch loss: 3.096446576118469\n",
      "[epoch: 2, i: 2899] avg mini-batch loss: 3.0485877704620363\n",
      "[epoch: 2, i: 2999] avg mini-batch loss: 3.062853193283081\n",
      "[epoch: 2, i: 3099] avg mini-batch loss: 3.1051760149002074\n",
      "[epoch: 2, i: 3199] avg mini-batch loss: 3.0633792638778687\n",
      "[epoch: 2, i: 3299] avg mini-batch loss: 3.0905755662918093\n",
      "[epoch: 2, i: 3399] avg mini-batch loss: 3.0039796209335328\n",
      "[epoch: 2, i: 3499] avg mini-batch loss: 3.0989855837821962\n",
      "[epoch: 2, i: 3599] avg mini-batch loss: 3.0702195525169373\n",
      "[epoch: 2, i: 3699] avg mini-batch loss: 3.040580680370331\n",
      "[epoch: 2, i: 3799] avg mini-batch loss: 3.0311280035972596\n",
      "[epoch: 2, i: 3899] avg mini-batch loss: 3.0508195662498476\n",
      "[epoch: 2, i: 3999] avg mini-batch loss: 3.055401883125305\n",
      "[epoch: 2, i: 4099] avg mini-batch loss: 3.015676474571228\n",
      "[epoch: 2, i: 4199] avg mini-batch loss: 2.966765234470367\n",
      "[epoch: 2, i: 4299] avg mini-batch loss: 3.0585282468795776\n",
      "[epoch: 2, i: 4399] avg mini-batch loss: 3.009554557800293\n",
      "[epoch: 2, i: 4499] avg mini-batch loss: 3.0854485201835633\n",
      "[epoch: 2, i: 4599] avg mini-batch loss: 3.022514898777008\n",
      "[epoch: 2, i: 4699] avg mini-batch loss: 3.0335435485839843\n",
      "[epoch: 2, i: 4799] avg mini-batch loss: 2.9294093203544618\n",
      "[epoch: 2, i: 4899] avg mini-batch loss: 3.0347760009765623\n",
      "[epoch: 2, i: 4999] avg mini-batch loss: 3.115779070854187\n",
      "[epoch: 2, i: 5099] avg mini-batch loss: 3.033802297115326\n",
      "[epoch: 2, i: 5199] avg mini-batch loss: 3.037408676147461\n",
      "[epoch: 2, i: 5299] avg mini-batch loss: 3.0579919171333314\n",
      "[epoch: 2, i: 5399] avg mini-batch loss: 2.993614001274109\n",
      "[epoch: 2, i: 5499] avg mini-batch loss: 3.0489023303985596\n",
      "[epoch: 2, i: 5599] avg mini-batch loss: 3.0622943353652956\n",
      "[epoch: 2, i: 5699] avg mini-batch loss: 2.975027148723602\n",
      "[epoch: 2, i: 5799] avg mini-batch loss: 3.0438934564590454\n",
      "[epoch: 2, i: 5899] avg mini-batch loss: 2.9657421016693117\n",
      "[epoch: 2, i: 5999] avg mini-batch loss: 2.973837854862213\n",
      "[epoch: 2, i: 6099] avg mini-batch loss: 3.0335710787773134\n",
      "[epoch: 2, i: 6199] avg mini-batch loss: 2.986302127838135\n",
      "[epoch: 2, i: 6299] avg mini-batch loss: 2.999036259651184\n",
      "[epoch: 2, i: 6399] avg mini-batch loss: 2.9870025300979615\n",
      "[epoch: 2, i: 6499] avg mini-batch loss: 2.9607945895195007\n",
      "[epoch: 2, i: 6599] avg mini-batch loss: 2.9292050337791444\n",
      "[epoch: 2, i: 6699] avg mini-batch loss: 3.012035312652588\n",
      "[epoch: 2, i: 6799] avg mini-batch loss: 3.0214233350753785\n",
      "[epoch: 2, i: 6899] avg mini-batch loss: 2.9843711256980896\n",
      "[epoch: 2, i: 6999] avg mini-batch loss: 3.022107229232788\n",
      "[epoch: 2, i: 7099] avg mini-batch loss: 2.9722927451133727\n",
      "[epoch: 2, i: 7199] avg mini-batch loss: 2.972769682407379\n",
      "[epoch: 2, i: 7299] avg mini-batch loss: 3.0265198016166686\n",
      "[epoch: 2, i: 7399] avg mini-batch loss: 2.9986508083343506\n",
      "[epoch: 2, i: 7499] avg mini-batch loss: 3.0391653108596803\n",
      "[epoch: 2, i: 7599] avg mini-batch loss: 3.030263328552246\n",
      "[epoch: 2, i: 7699] avg mini-batch loss: 2.9320201468467713\n",
      "[epoch: 2, i: 7799] avg mini-batch loss: 2.988779757022858\n",
      "[epoch: 2, i: 7899] avg mini-batch loss: 2.9646873259544373\n",
      "[epoch: 2, i: 7999] avg mini-batch loss: 2.989946331977844\n",
      "[epoch: 2, i: 8099] avg mini-batch loss: 2.936929988861084\n",
      "[epoch: 2, i: 8199] avg mini-batch loss: 3.0363785433769226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 2, i: 8299] avg mini-batch loss: 2.9890104079246522\n",
      "[epoch: 2, i: 8399] avg mini-batch loss: 2.9475206542015076\n",
      "[epoch: 2, i: 8499] avg mini-batch loss: 3.0265791058540343\n",
      "[epoch: 2, i: 8599] avg mini-batch loss: 2.9526639366149903\n",
      "[epoch: 2, i: 8699] avg mini-batch loss: 2.9165991139411926\n",
      "[epoch: 2, i: 8799] avg mini-batch loss: 2.9834918189048767\n",
      "[epoch: 2, i: 8899] avg mini-batch loss: 2.9235280394554137\n",
      "[epoch: 2, i: 8999] avg mini-batch loss: 3.0011381912231445\n",
      "[epoch: 2, i: 9099] avg mini-batch loss: 2.9727698945999146\n",
      "[epoch: 2, i: 9199] avg mini-batch loss: 2.970596604347229\n",
      "[epoch: 2, i: 9299] avg mini-batch loss: 3.005007631778717\n",
      "[epoch: 2, i: 9399] avg mini-batch loss: 2.9386173725128173\n",
      "[epoch: 2, i: 9499] avg mini-batch loss: 2.9876079249382017\n",
      "[epoch: 2, i: 9599] avg mini-batch loss: 2.974640064239502\n",
      "[epoch: 2, i: 9699] avg mini-batch loss: 2.9887929654121397\n",
      "[epoch: 3, i: 99] avg mini-batch loss: 2.957038998603821\n",
      "[epoch: 3, i: 199] avg mini-batch loss: 2.9514525938034057\n",
      "[epoch: 3, i: 299] avg mini-batch loss: 2.9623974347114563\n",
      "[epoch: 3, i: 399] avg mini-batch loss: 2.9324083304405213\n",
      "[epoch: 3, i: 499] avg mini-batch loss: 2.9607223987579347\n",
      "[epoch: 3, i: 599] avg mini-batch loss: 2.940995941162109\n",
      "[epoch: 3, i: 699] avg mini-batch loss: 2.9185782623291017\n",
      "[epoch: 3, i: 799] avg mini-batch loss: 2.9813087034225463\n",
      "[epoch: 3, i: 899] avg mini-batch loss: 2.9551422333717348\n",
      "[epoch: 3, i: 999] avg mini-batch loss: 2.8979741191864012\n",
      "[epoch: 3, i: 1099] avg mini-batch loss: 2.957003698348999\n",
      "[epoch: 3, i: 1199] avg mini-batch loss: 2.963189175128937\n",
      "[epoch: 3, i: 1299] avg mini-batch loss: 2.906734874248505\n",
      "[epoch: 3, i: 1399] avg mini-batch loss: 2.9200720763206482\n",
      "[epoch: 3, i: 1499] avg mini-batch loss: 2.896043782234192\n",
      "[epoch: 3, i: 1599] avg mini-batch loss: 2.861111569404602\n",
      "[epoch: 3, i: 1699] avg mini-batch loss: 2.866082663536072\n",
      "[epoch: 3, i: 1799] avg mini-batch loss: 2.894610059261322\n",
      "[epoch: 3, i: 1899] avg mini-batch loss: 2.902969319820404\n",
      "[epoch: 3, i: 1999] avg mini-batch loss: 2.926427013874054\n",
      "[epoch: 3, i: 2099] avg mini-batch loss: 2.930170760154724\n",
      "[epoch: 3, i: 2199] avg mini-batch loss: 2.9817855167388916\n",
      "[epoch: 3, i: 2299] avg mini-batch loss: 2.90413535118103\n",
      "[epoch: 3, i: 2399] avg mini-batch loss: 2.911056869029999\n",
      "[epoch: 3, i: 2499] avg mini-batch loss: 2.8947782206535337\n",
      "[epoch: 3, i: 2599] avg mini-batch loss: 2.8762435340881347\n",
      "[epoch: 3, i: 2699] avg mini-batch loss: 2.9241288447380067\n",
      "[epoch: 3, i: 2799] avg mini-batch loss: 2.8863518047332763\n",
      "[epoch: 3, i: 2899] avg mini-batch loss: 2.9564249968528746\n",
      "[epoch: 3, i: 2999] avg mini-batch loss: 2.886612913608551\n",
      "[epoch: 3, i: 3099] avg mini-batch loss: 2.90070946931839\n",
      "[epoch: 3, i: 3199] avg mini-batch loss: 2.8940006852149964\n",
      "[epoch: 3, i: 3299] avg mini-batch loss: 2.9272065234184264\n",
      "[epoch: 3, i: 3399] avg mini-batch loss: 2.91002471446991\n",
      "[epoch: 3, i: 3499] avg mini-batch loss: 2.8923131322860716\n",
      "[epoch: 3, i: 3599] avg mini-batch loss: 2.937056293487549\n",
      "[epoch: 3, i: 3699] avg mini-batch loss: 2.8712111496925354\n",
      "[epoch: 3, i: 3799] avg mini-batch loss: 2.914513211250305\n",
      "[epoch: 3, i: 3899] avg mini-batch loss: 2.9033081269264223\n",
      "[epoch: 3, i: 3999] avg mini-batch loss: 2.934128758907318\n",
      "[epoch: 3, i: 4099] avg mini-batch loss: 2.9043646621704102\n",
      "[epoch: 3, i: 4199] avg mini-batch loss: 2.8053502678871154\n",
      "[epoch: 3, i: 4299] avg mini-batch loss: 2.8957058501243593\n",
      "[epoch: 3, i: 4399] avg mini-batch loss: 2.930876157283783\n",
      "[epoch: 3, i: 4499] avg mini-batch loss: 2.916383409500122\n",
      "[epoch: 3, i: 4599] avg mini-batch loss: 2.894759187698364\n",
      "[epoch: 3, i: 4699] avg mini-batch loss: 2.880062279701233\n",
      "[epoch: 3, i: 4799] avg mini-batch loss: 2.977982943058014\n",
      "[epoch: 3, i: 4899] avg mini-batch loss: 2.879466509819031\n",
      "[epoch: 3, i: 4999] avg mini-batch loss: 2.859863305091858\n",
      "[epoch: 3, i: 5099] avg mini-batch loss: 2.916289601325989\n",
      "[epoch: 3, i: 5199] avg mini-batch loss: 2.800782675743103\n",
      "[epoch: 3, i: 5299] avg mini-batch loss: 2.882651948928833\n",
      "[epoch: 3, i: 5399] avg mini-batch loss: 2.925453312397003\n",
      "[epoch: 3, i: 5499] avg mini-batch loss: 2.850452198982239\n",
      "[epoch: 3, i: 5599] avg mini-batch loss: 2.912238175868988\n",
      "[epoch: 3, i: 5699] avg mini-batch loss: 2.8317380237579344\n",
      "[epoch: 3, i: 5799] avg mini-batch loss: 2.85383620262146\n",
      "[epoch: 3, i: 5899] avg mini-batch loss: 2.900388841629028\n",
      "[epoch: 3, i: 5999] avg mini-batch loss: 2.871958737373352\n",
      "[epoch: 3, i: 6099] avg mini-batch loss: 2.8389350748062134\n",
      "[epoch: 3, i: 6199] avg mini-batch loss: 2.8357311248779298\n",
      "[epoch: 3, i: 6299] avg mini-batch loss: 2.8825660347938538\n",
      "[epoch: 3, i: 6399] avg mini-batch loss: 2.909307265281677\n",
      "[epoch: 3, i: 6499] avg mini-batch loss: 2.872743046283722\n",
      "[epoch: 3, i: 6599] avg mini-batch loss: 2.8141991567611693\n",
      "[epoch: 3, i: 6699] avg mini-batch loss: 2.850253264904022\n",
      "[epoch: 3, i: 6799] avg mini-batch loss: 2.8202669715881346\n",
      "[epoch: 3, i: 6899] avg mini-batch loss: 2.9564871859550474\n",
      "[epoch: 3, i: 6999] avg mini-batch loss: 2.9047744512557983\n",
      "[epoch: 3, i: 7099] avg mini-batch loss: 2.875708694458008\n",
      "[epoch: 3, i: 7199] avg mini-batch loss: 2.8848117566108704\n",
      "[epoch: 3, i: 7299] avg mini-batch loss: 2.810575032234192\n",
      "[epoch: 3, i: 7399] avg mini-batch loss: 2.8505708813667296\n",
      "[epoch: 3, i: 7499] avg mini-batch loss: 2.916990110874176\n",
      "[epoch: 3, i: 7599] avg mini-batch loss: 2.9064623975753783\n",
      "[epoch: 3, i: 7699] avg mini-batch loss: 2.823197021484375\n",
      "[epoch: 3, i: 7799] avg mini-batch loss: 2.794142687320709\n",
      "[epoch: 3, i: 7899] avg mini-batch loss: 2.8478559637069703\n",
      "[epoch: 3, i: 7999] avg mini-batch loss: 2.8907236671447754\n",
      "[epoch: 3, i: 8099] avg mini-batch loss: 2.863200235366821\n",
      "[epoch: 3, i: 8199] avg mini-batch loss: 2.8925177550315855\n",
      "[epoch: 3, i: 8299] avg mini-batch loss: 2.8277922296524047\n",
      "[epoch: 3, i: 8399] avg mini-batch loss: 2.778700575828552\n",
      "[epoch: 3, i: 8499] avg mini-batch loss: 2.866910843849182\n",
      "[epoch: 3, i: 8599] avg mini-batch loss: 2.8425538992881774\n",
      "[epoch: 3, i: 8699] avg mini-batch loss: 2.8575189185142515\n",
      "[epoch: 3, i: 8799] avg mini-batch loss: 2.8926665329933168\n",
      "[epoch: 3, i: 8899] avg mini-batch loss: 2.858677442073822\n",
      "[epoch: 3, i: 8999] avg mini-batch loss: 2.8432178854942323\n",
      "[epoch: 3, i: 9099] avg mini-batch loss: 2.8324827194213866\n",
      "[epoch: 3, i: 9199] avg mini-batch loss: 2.7615727019309997\n",
      "[epoch: 3, i: 9299] avg mini-batch loss: 2.8506504344940184\n",
      "[epoch: 3, i: 9399] avg mini-batch loss: 2.8143231320381163\n",
      "[epoch: 3, i: 9499] avg mini-batch loss: 2.82644371509552\n",
      "[epoch: 3, i: 9599] avg mini-batch loss: 2.8162548422813414\n",
      "[epoch: 3, i: 9699] avg mini-batch loss: 2.8231951332092287\n",
      "[epoch: 4, i: 99] avg mini-batch loss: 2.8451780676841736\n",
      "[epoch: 4, i: 199] avg mini-batch loss: 2.8296775078773497\n",
      "[epoch: 4, i: 299] avg mini-batch loss: 2.779288852214813\n",
      "[epoch: 4, i: 399] avg mini-batch loss: 2.8694783806800843\n",
      "[epoch: 4, i: 499] avg mini-batch loss: 2.8862027668952943\n",
      "[epoch: 4, i: 599] avg mini-batch loss: 2.8173022150993345\n",
      "[epoch: 4, i: 699] avg mini-batch loss: 2.8160723996162416\n",
      "[epoch: 4, i: 799] avg mini-batch loss: 2.7837030625343324\n",
      "[epoch: 4, i: 899] avg mini-batch loss: 2.716093034744263\n",
      "[epoch: 4, i: 999] avg mini-batch loss: 2.855279071331024\n",
      "[epoch: 4, i: 1099] avg mini-batch loss: 2.8196780562400816\n",
      "[epoch: 4, i: 1199] avg mini-batch loss: 2.8020877838134766\n",
      "[epoch: 4, i: 1299] avg mini-batch loss: 2.8240700554847717\n",
      "[epoch: 4, i: 1399] avg mini-batch loss: 2.8078768467903137\n",
      "[epoch: 4, i: 1499] avg mini-batch loss: 2.8443356418609618\n",
      "[epoch: 4, i: 1599] avg mini-batch loss: 2.735154058933258\n",
      "[epoch: 4, i: 1699] avg mini-batch loss: 2.803160157203674\n",
      "[epoch: 4, i: 1799] avg mini-batch loss: 2.8249134397506714\n",
      "[epoch: 4, i: 1899] avg mini-batch loss: 2.816841096878052\n",
      "[epoch: 4, i: 1999] avg mini-batch loss: 2.7669121742248537\n",
      "[epoch: 4, i: 2099] avg mini-batch loss: 2.794466488361359\n",
      "[epoch: 4, i: 2199] avg mini-batch loss: 2.8112907242774963\n",
      "[epoch: 4, i: 2299] avg mini-batch loss: 2.8224203610420227\n",
      "[epoch: 4, i: 2399] avg mini-batch loss: 2.772635316848755\n",
      "[epoch: 4, i: 2499] avg mini-batch loss: 2.7836103582382203\n",
      "[epoch: 4, i: 2599] avg mini-batch loss: 2.7855718445777895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 4, i: 2699] avg mini-batch loss: 2.8163313007354738\n",
      "[epoch: 4, i: 2799] avg mini-batch loss: 2.8485106348991396\n",
      "[epoch: 4, i: 2899] avg mini-batch loss: 2.8095420885086058\n",
      "[epoch: 4, i: 2999] avg mini-batch loss: 2.7328752446174622\n",
      "[epoch: 4, i: 3099] avg mini-batch loss: 2.8032688522338867\n",
      "[epoch: 4, i: 3199] avg mini-batch loss: 2.768956770896912\n",
      "[epoch: 4, i: 3299] avg mini-batch loss: 2.8053554344177245\n",
      "[epoch: 4, i: 3399] avg mini-batch loss: 2.7456996178627016\n",
      "[epoch: 4, i: 3499] avg mini-batch loss: 2.802022864818573\n",
      "[epoch: 4, i: 3599] avg mini-batch loss: 2.7758408999443054\n",
      "[epoch: 4, i: 3699] avg mini-batch loss: 2.7902882623672487\n",
      "[epoch: 4, i: 3799] avg mini-batch loss: 2.8144374227523805\n",
      "[epoch: 4, i: 3899] avg mini-batch loss: 2.769060537815094\n",
      "[epoch: 4, i: 3999] avg mini-batch loss: 2.790167076587677\n",
      "[epoch: 4, i: 4099] avg mini-batch loss: 2.7225531220436094\n",
      "[epoch: 4, i: 4199] avg mini-batch loss: 2.8038570046424867\n",
      "[epoch: 4, i: 4299] avg mini-batch loss: 2.7901349592208864\n",
      "[epoch: 4, i: 4399] avg mini-batch loss: 2.798521671295166\n",
      "[epoch: 4, i: 4499] avg mini-batch loss: 2.762819926738739\n",
      "[epoch: 4, i: 4599] avg mini-batch loss: 2.7958068013191224\n",
      "[epoch: 4, i: 4699] avg mini-batch loss: 2.7633314323425293\n",
      "[epoch: 4, i: 4799] avg mini-batch loss: 2.723084604740143\n",
      "[epoch: 4, i: 4899] avg mini-batch loss: 2.7432922863960267\n",
      "[epoch: 4, i: 4999] avg mini-batch loss: 2.7637717628479006\n",
      "[epoch: 4, i: 5099] avg mini-batch loss: 2.7774579095840455\n",
      "[epoch: 4, i: 5199] avg mini-batch loss: 2.828055431842804\n",
      "[epoch: 4, i: 5299] avg mini-batch loss: 2.708498275279999\n",
      "[epoch: 4, i: 5399] avg mini-batch loss: 2.7882230782508852\n",
      "[epoch: 4, i: 5499] avg mini-batch loss: 2.8105730748176576\n",
      "[epoch: 4, i: 5599] avg mini-batch loss: 2.749875638484955\n",
      "[epoch: 4, i: 5699] avg mini-batch loss: 2.721618282794952\n",
      "[epoch: 4, i: 5799] avg mini-batch loss: 2.6574523317813874\n",
      "[epoch: 4, i: 5899] avg mini-batch loss: 2.7425645780563355\n",
      "[epoch: 4, i: 5999] avg mini-batch loss: 2.7971874976158144\n",
      "[epoch: 4, i: 6099] avg mini-batch loss: 2.8014874339103697\n",
      "[epoch: 4, i: 6199] avg mini-batch loss: 2.730171422958374\n",
      "[epoch: 4, i: 6299] avg mini-batch loss: 2.810816433429718\n",
      "[epoch: 4, i: 6399] avg mini-batch loss: 2.7777152013778688\n",
      "[epoch: 4, i: 6499] avg mini-batch loss: 2.7238385224342347\n",
      "[epoch: 4, i: 6599] avg mini-batch loss: 2.7498953795433043\n",
      "[epoch: 4, i: 6699] avg mini-batch loss: 2.7371987295150757\n",
      "[epoch: 4, i: 6799] avg mini-batch loss: 2.7273265409469603\n",
      "[epoch: 4, i: 6899] avg mini-batch loss: 2.732483780384064\n",
      "[epoch: 4, i: 6999] avg mini-batch loss: 2.7508113479614256\n",
      "[epoch: 4, i: 7099] avg mini-batch loss: 2.7143565726280214\n",
      "[epoch: 4, i: 7199] avg mini-batch loss: 2.801238338947296\n",
      "[epoch: 4, i: 7299] avg mini-batch loss: 2.7701739501953124\n",
      "[epoch: 4, i: 7399] avg mini-batch loss: 2.795126657485962\n",
      "[epoch: 4, i: 7499] avg mini-batch loss: 2.7528144979476927\n",
      "[epoch: 4, i: 7599] avg mini-batch loss: 2.762632277011871\n",
      "[epoch: 4, i: 7699] avg mini-batch loss: 2.7653140234947204\n",
      "[epoch: 4, i: 7799] avg mini-batch loss: 2.755210461616516\n",
      "[epoch: 4, i: 7899] avg mini-batch loss: 2.728500063419342\n",
      "[epoch: 4, i: 7999] avg mini-batch loss: 2.7336092615127563\n",
      "[epoch: 4, i: 8099] avg mini-batch loss: 2.6969934844970704\n",
      "[epoch: 4, i: 8199] avg mini-batch loss: 2.75673504114151\n",
      "[epoch: 4, i: 8299] avg mini-batch loss: 2.721374025344849\n",
      "[epoch: 4, i: 8399] avg mini-batch loss: 2.7757374906539916\n",
      "[epoch: 4, i: 8499] avg mini-batch loss: 2.7833622312545776\n",
      "[epoch: 4, i: 8599] avg mini-batch loss: 2.6809118676185606\n",
      "[epoch: 4, i: 8699] avg mini-batch loss: 2.7463783621788025\n",
      "[epoch: 4, i: 8799] avg mini-batch loss: 2.7103305864334106\n",
      "[epoch: 4, i: 8899] avg mini-batch loss: 2.714519374370575\n",
      "[epoch: 4, i: 8999] avg mini-batch loss: 2.6746301651000977\n",
      "[epoch: 4, i: 9099] avg mini-batch loss: 2.7821769070625306\n",
      "[epoch: 4, i: 9199] avg mini-batch loss: 2.725103600025177\n",
      "[epoch: 4, i: 9299] avg mini-batch loss: 2.6699970817565917\n",
      "[epoch: 4, i: 9399] avg mini-batch loss: 2.7294016802310943\n",
      "[epoch: 4, i: 9499] avg mini-batch loss: 2.675935595035553\n",
      "[epoch: 4, i: 9599] avg mini-batch loss: 2.716680088043213\n",
      "[epoch: 4, i: 9699] avg mini-batch loss: 2.6824906420707704\n",
      "[epoch: 5, i: 99] avg mini-batch loss: 2.6976734685897825\n",
      "[epoch: 5, i: 199] avg mini-batch loss: 2.70634712934494\n",
      "[epoch: 5, i: 299] avg mini-batch loss: 2.6176606357097625\n",
      "[epoch: 5, i: 399] avg mini-batch loss: 2.6794770765304565\n",
      "[epoch: 5, i: 499] avg mini-batch loss: 2.6295522809028626\n",
      "[epoch: 5, i: 599] avg mini-batch loss: 2.6822067379951475\n",
      "[epoch: 5, i: 699] avg mini-batch loss: 2.7023326635360716\n",
      "[epoch: 5, i: 799] avg mini-batch loss: 2.7094222998619077\n",
      "[epoch: 5, i: 899] avg mini-batch loss: 2.701517765522003\n",
      "[epoch: 5, i: 999] avg mini-batch loss: 2.736139531135559\n",
      "[epoch: 5, i: 1099] avg mini-batch loss: 2.7078551483154296\n",
      "[epoch: 5, i: 1199] avg mini-batch loss: 2.772647352218628\n",
      "[epoch: 5, i: 1299] avg mini-batch loss: 2.6426498651504517\n",
      "[epoch: 5, i: 1399] avg mini-batch loss: 2.6552382278442384\n",
      "[epoch: 5, i: 1499] avg mini-batch loss: 2.7223744225502013\n",
      "[epoch: 5, i: 1599] avg mini-batch loss: 2.6657254672050477\n",
      "[epoch: 5, i: 1699] avg mini-batch loss: 2.684428346157074\n",
      "[epoch: 5, i: 1799] avg mini-batch loss: 2.6593601441383363\n",
      "[epoch: 5, i: 1899] avg mini-batch loss: 2.6656639862060545\n",
      "[epoch: 5, i: 1999] avg mini-batch loss: 2.670082552433014\n",
      "[epoch: 5, i: 2099] avg mini-batch loss: 2.6935631251335144\n",
      "[epoch: 5, i: 2199] avg mini-batch loss: 2.6112854886054992\n",
      "[epoch: 5, i: 2299] avg mini-batch loss: 2.6619275224208834\n",
      "[epoch: 5, i: 2399] avg mini-batch loss: 2.705613605976105\n",
      "[epoch: 5, i: 2499] avg mini-batch loss: 2.68328284740448\n",
      "[epoch: 5, i: 2599] avg mini-batch loss: 2.6784439480304716\n",
      "[epoch: 5, i: 2699] avg mini-batch loss: 2.7340707397460937\n",
      "[epoch: 5, i: 2799] avg mini-batch loss: 2.6847495579719545\n",
      "[epoch: 5, i: 2899] avg mini-batch loss: 2.653569828271866\n",
      "[epoch: 5, i: 2999] avg mini-batch loss: 2.6582751774787905\n",
      "[epoch: 5, i: 3099] avg mini-batch loss: 2.6617040050029757\n",
      "[epoch: 5, i: 3199] avg mini-batch loss: 2.6982857894897463\n",
      "[epoch: 5, i: 3299] avg mini-batch loss: 2.732062623500824\n",
      "[epoch: 5, i: 3399] avg mini-batch loss: 2.6702386498451234\n",
      "[epoch: 5, i: 3499] avg mini-batch loss: 2.6751055121421814\n",
      "[epoch: 5, i: 3599] avg mini-batch loss: 2.643710732460022\n",
      "[epoch: 5, i: 3699] avg mini-batch loss: 2.6100832748413088\n",
      "[epoch: 5, i: 3799] avg mini-batch loss: 2.629331741333008\n",
      "[epoch: 5, i: 3899] avg mini-batch loss: 2.7026342940330506\n",
      "[epoch: 5, i: 3999] avg mini-batch loss: 2.545950322151184\n",
      "[epoch: 5, i: 4099] avg mini-batch loss: 2.641793341636658\n",
      "[epoch: 5, i: 4199] avg mini-batch loss: 2.606359877586365\n",
      "[epoch: 5, i: 4299] avg mini-batch loss: 2.6527391147613524\n",
      "[epoch: 5, i: 4399] avg mini-batch loss: 2.634375796318054\n",
      "[epoch: 5, i: 4499] avg mini-batch loss: 2.648999512195587\n",
      "[epoch: 5, i: 4599] avg mini-batch loss: 2.6704594039916993\n",
      "[epoch: 5, i: 4699] avg mini-batch loss: 2.6406768131256104\n",
      "[epoch: 5, i: 4799] avg mini-batch loss: 2.6111192762851716\n",
      "[epoch: 5, i: 4899] avg mini-batch loss: 2.6667353439331056\n",
      "[epoch: 5, i: 4999] avg mini-batch loss: 2.6843127739429473\n",
      "[epoch: 5, i: 5099] avg mini-batch loss: 2.638813543319702\n",
      "[epoch: 5, i: 5199] avg mini-batch loss: 2.7328110551834106\n",
      "[epoch: 5, i: 5299] avg mini-batch loss: 2.6072176158428193\n",
      "[epoch: 5, i: 5399] avg mini-batch loss: 2.664982568025589\n",
      "[epoch: 5, i: 5499] avg mini-batch loss: 2.64185177564621\n",
      "[epoch: 5, i: 5599] avg mini-batch loss: 2.630083900690079\n",
      "[epoch: 5, i: 5699] avg mini-batch loss: 2.618335872888565\n",
      "[epoch: 5, i: 5799] avg mini-batch loss: 2.660734097957611\n",
      "[epoch: 5, i: 5899] avg mini-batch loss: 2.692580168247223\n",
      "[epoch: 5, i: 5999] avg mini-batch loss: 2.6433236372470854\n",
      "[epoch: 5, i: 6099] avg mini-batch loss: 2.667872440814972\n",
      "[epoch: 5, i: 6199] avg mini-batch loss: 2.685767146348953\n",
      "[epoch: 5, i: 6299] avg mini-batch loss: 2.656764051914215\n",
      "[epoch: 5, i: 6399] avg mini-batch loss: 2.6754704213142393\n",
      "[epoch: 5, i: 6499] avg mini-batch loss: 2.6078816437721253\n",
      "[epoch: 5, i: 6599] avg mini-batch loss: 2.6377627658843994\n",
      "[epoch: 5, i: 6699] avg mini-batch loss: 2.6085691356658938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 5, i: 6799] avg mini-batch loss: 2.631645300388336\n",
      "[epoch: 5, i: 6899] avg mini-batch loss: 2.6557607007026673\n",
      "[epoch: 5, i: 6999] avg mini-batch loss: 2.5920537972450255\n",
      "[epoch: 5, i: 7099] avg mini-batch loss: 2.6421417927742006\n",
      "[epoch: 5, i: 7199] avg mini-batch loss: 2.623697566986084\n",
      "[epoch: 5, i: 7299] avg mini-batch loss: 2.6100093054771425\n",
      "[epoch: 5, i: 7399] avg mini-batch loss: 2.6182085132598876\n",
      "[epoch: 5, i: 7499] avg mini-batch loss: 2.6343050360679627\n",
      "[epoch: 5, i: 7599] avg mini-batch loss: 2.676932631731033\n",
      "[epoch: 5, i: 7699] avg mini-batch loss: 2.6258399581909178\n",
      "[epoch: 5, i: 7799] avg mini-batch loss: 2.6444478023052214\n",
      "[epoch: 5, i: 7899] avg mini-batch loss: 2.6077910017967225\n",
      "[epoch: 5, i: 7999] avg mini-batch loss: 2.588411042690277\n",
      "[epoch: 5, i: 8099] avg mini-batch loss: 2.5730313777923586\n",
      "[epoch: 5, i: 8199] avg mini-batch loss: 2.7094161701202393\n",
      "[epoch: 5, i: 8299] avg mini-batch loss: 2.657686324119568\n",
      "[epoch: 5, i: 8399] avg mini-batch loss: 2.647386395931244\n",
      "[epoch: 5, i: 8499] avg mini-batch loss: 2.649979122877121\n",
      "[epoch: 5, i: 8599] avg mini-batch loss: 2.6006643283367157\n",
      "[epoch: 5, i: 8699] avg mini-batch loss: 2.6670375204086305\n",
      "[epoch: 5, i: 8799] avg mini-batch loss: 2.6798911190032957\n",
      "[epoch: 5, i: 8899] avg mini-batch loss: 2.649391088485718\n",
      "[epoch: 5, i: 8999] avg mini-batch loss: 2.5953686666488647\n",
      "[epoch: 5, i: 9099] avg mini-batch loss: 2.5965166091918945\n",
      "[epoch: 5, i: 9199] avg mini-batch loss: 2.5833491718769075\n",
      "[epoch: 5, i: 9299] avg mini-batch loss: 2.635662945508957\n",
      "[epoch: 5, i: 9399] avg mini-batch loss: 2.648664654493332\n",
      "[epoch: 5, i: 9499] avg mini-batch loss: 2.63986474275589\n",
      "[epoch: 5, i: 9599] avg mini-batch loss: 2.607160139083862\n",
      "[epoch: 5, i: 9699] avg mini-batch loss: 2.62019539475441\n",
      "[epoch: 6, i: 99] avg mini-batch loss: 2.5485717844963074\n",
      "[epoch: 6, i: 199] avg mini-batch loss: 2.5794069921970366\n",
      "[epoch: 6, i: 299] avg mini-batch loss: 2.5930555510520934\n",
      "[epoch: 6, i: 399] avg mini-batch loss: 2.608977254629135\n",
      "[epoch: 6, i: 499] avg mini-batch loss: 2.582511377334595\n",
      "[epoch: 6, i: 599] avg mini-batch loss: 2.5229860866069793\n",
      "[epoch: 6, i: 699] avg mini-batch loss: 2.6382594621181488\n",
      "[epoch: 6, i: 799] avg mini-batch loss: 2.5955015659332275\n",
      "[epoch: 6, i: 899] avg mini-batch loss: 2.539015009403229\n",
      "[epoch: 6, i: 999] avg mini-batch loss: 2.5965251684188844\n",
      "[epoch: 6, i: 1099] avg mini-batch loss: 2.5587658858299256\n",
      "[epoch: 6, i: 1199] avg mini-batch loss: 2.5679466676712037\n",
      "[epoch: 6, i: 1299] avg mini-batch loss: 2.6315976214408874\n",
      "[epoch: 6, i: 1399] avg mini-batch loss: 2.5946390450000765\n",
      "[epoch: 6, i: 1499] avg mini-batch loss: 2.6198639464378357\n",
      "[epoch: 6, i: 1599] avg mini-batch loss: 2.5843966364860536\n",
      "[epoch: 6, i: 1699] avg mini-batch loss: 2.527028248310089\n",
      "[epoch: 6, i: 1799] avg mini-batch loss: 2.612605645656586\n",
      "[epoch: 6, i: 1899] avg mini-batch loss: 2.5963694930076597\n",
      "[epoch: 6, i: 1999] avg mini-batch loss: 2.5605235576629637\n",
      "[epoch: 6, i: 2099] avg mini-batch loss: 2.5897420907020567\n",
      "[epoch: 6, i: 2199] avg mini-batch loss: 2.5868296957015993\n",
      "[epoch: 6, i: 2299] avg mini-batch loss: 2.5799479985237124\n",
      "[epoch: 6, i: 2399] avg mini-batch loss: 2.5337334752082823\n",
      "[epoch: 6, i: 2499] avg mini-batch loss: 2.5743302941322326\n",
      "[epoch: 6, i: 2599] avg mini-batch loss: 2.5611684918403625\n",
      "[epoch: 6, i: 2699] avg mini-batch loss: 2.581101830005646\n",
      "[epoch: 6, i: 2799] avg mini-batch loss: 2.6246879076957703\n",
      "[epoch: 6, i: 2899] avg mini-batch loss: 2.536289145946503\n",
      "[epoch: 6, i: 2999] avg mini-batch loss: 2.6163959240913393\n",
      "[epoch: 6, i: 3099] avg mini-batch loss: 2.558430446386337\n",
      "[epoch: 6, i: 3199] avg mini-batch loss: 2.5281220972537994\n",
      "[epoch: 6, i: 3299] avg mini-batch loss: 2.607911230325699\n",
      "[epoch: 6, i: 3399] avg mini-batch loss: 2.5672088253498075\n",
      "[epoch: 6, i: 3499] avg mini-batch loss: 2.613014006614685\n",
      "[epoch: 6, i: 3599] avg mini-batch loss: 2.5653220295906065\n",
      "[epoch: 6, i: 3699] avg mini-batch loss: 2.514405118227005\n",
      "[epoch: 6, i: 3799] avg mini-batch loss: 2.6103760588169096\n",
      "[epoch: 6, i: 3899] avg mini-batch loss: 2.5871087551116942\n",
      "[epoch: 6, i: 3999] avg mini-batch loss: 2.5428827381134034\n",
      "[epoch: 6, i: 4099] avg mini-batch loss: 2.557123534679413\n",
      "[epoch: 6, i: 4199] avg mini-batch loss: 2.5490104222297667\n",
      "[epoch: 6, i: 4299] avg mini-batch loss: 2.5975345396995544\n",
      "[epoch: 6, i: 4399] avg mini-batch loss: 2.5656539499759674\n",
      "[epoch: 6, i: 4499] avg mini-batch loss: 2.524480850696564\n",
      "[epoch: 6, i: 4599] avg mini-batch loss: 2.5736765396595\n",
      "[epoch: 6, i: 4699] avg mini-batch loss: 2.5510722541809083\n",
      "[epoch: 6, i: 4799] avg mini-batch loss: 2.5737666308879854\n",
      "[epoch: 6, i: 4899] avg mini-batch loss: 2.537035446166992\n",
      "[epoch: 6, i: 4999] avg mini-batch loss: 2.5046109056472776\n",
      "[epoch: 6, i: 5099] avg mini-batch loss: 2.499266608953476\n",
      "[epoch: 6, i: 5199] avg mini-batch loss: 2.5785116922855376\n",
      "[epoch: 6, i: 5299] avg mini-batch loss: 2.5070633149147032\n",
      "[epoch: 6, i: 5399] avg mini-batch loss: 2.556011743545532\n",
      "[epoch: 6, i: 5499] avg mini-batch loss: 2.5875018572807313\n",
      "[epoch: 6, i: 5599] avg mini-batch loss: 2.5591746044158934\n",
      "[epoch: 6, i: 5699] avg mini-batch loss: 2.5620464730262755\n",
      "[epoch: 6, i: 5799] avg mini-batch loss: 2.549182895421982\n",
      "[epoch: 6, i: 5899] avg mini-batch loss: 2.591607005596161\n",
      "[epoch: 6, i: 5999] avg mini-batch loss: 2.57680806517601\n",
      "[epoch: 6, i: 6099] avg mini-batch loss: 2.5884137678146364\n",
      "[epoch: 6, i: 6199] avg mini-batch loss: 2.5276616430282592\n",
      "[epoch: 6, i: 6299] avg mini-batch loss: 2.562924585342407\n",
      "[epoch: 6, i: 6399] avg mini-batch loss: 2.50485524892807\n",
      "[epoch: 6, i: 6499] avg mini-batch loss: 2.4890595257282255\n",
      "[epoch: 6, i: 6599] avg mini-batch loss: 2.4713671791553495\n",
      "[epoch: 6, i: 6699] avg mini-batch loss: 2.540968405008316\n",
      "[epoch: 6, i: 6799] avg mini-batch loss: 2.5093876111507414\n",
      "[epoch: 6, i: 6899] avg mini-batch loss: 2.5855413830280303\n",
      "[epoch: 6, i: 6999] avg mini-batch loss: 2.467200163602829\n",
      "[epoch: 6, i: 7099] avg mini-batch loss: 2.474325793981552\n",
      "[epoch: 6, i: 7199] avg mini-batch loss: 2.5521398830413817\n",
      "[epoch: 6, i: 7299] avg mini-batch loss: 2.4625146770477295\n",
      "[epoch: 6, i: 7399] avg mini-batch loss: 2.506133749485016\n",
      "[epoch: 6, i: 7499] avg mini-batch loss: 2.5043203020095826\n",
      "[epoch: 6, i: 7599] avg mini-batch loss: 2.54092298746109\n",
      "[epoch: 6, i: 7699] avg mini-batch loss: 2.5297708308696745\n",
      "[epoch: 6, i: 7799] avg mini-batch loss: 2.5747532844543457\n",
      "[epoch: 6, i: 7899] avg mini-batch loss: 2.5411531007289887\n",
      "[epoch: 6, i: 7999] avg mini-batch loss: 2.5097284162044526\n",
      "[epoch: 6, i: 8099] avg mini-batch loss: 2.5232945287227633\n",
      "[epoch: 6, i: 8199] avg mini-batch loss: 2.564028398990631\n",
      "[epoch: 6, i: 8299] avg mini-batch loss: 2.5284075224399567\n",
      "[epoch: 6, i: 8399] avg mini-batch loss: 2.5551867949962617\n",
      "[epoch: 6, i: 8499] avg mini-batch loss: 2.5168938887119294\n",
      "[epoch: 6, i: 8599] avg mini-batch loss: 2.5236801612377167\n",
      "[epoch: 6, i: 8699] avg mini-batch loss: 2.4833499896526336\n",
      "[epoch: 6, i: 8799] avg mini-batch loss: 2.5437217915058135\n",
      "[epoch: 6, i: 8899] avg mini-batch loss: 2.5047324192523956\n",
      "[epoch: 6, i: 8999] avg mini-batch loss: 2.485318397283554\n",
      "[epoch: 6, i: 9099] avg mini-batch loss: 2.51354744553566\n",
      "[epoch: 6, i: 9199] avg mini-batch loss: 2.5320688223838808\n",
      "[epoch: 6, i: 9299] avg mini-batch loss: 2.5540972995758056\n",
      "[epoch: 6, i: 9399] avg mini-batch loss: 2.506494061946869\n",
      "[epoch: 6, i: 9499] avg mini-batch loss: 2.5039955878257754\n",
      "[epoch: 6, i: 9599] avg mini-batch loss: 2.495910768508911\n",
      "[epoch: 6, i: 9699] avg mini-batch loss: 2.4791239488124845\n",
      "[epoch: 7, i: 99] avg mini-batch loss: 2.4694851171970367\n",
      "[epoch: 7, i: 199] avg mini-batch loss: 2.4886036467552186\n",
      "[epoch: 7, i: 299] avg mini-batch loss: 2.4968889582157137\n",
      "[epoch: 7, i: 399] avg mini-batch loss: 2.532190662622452\n",
      "[epoch: 7, i: 499] avg mini-batch loss: 2.516035739183426\n",
      "[epoch: 7, i: 599] avg mini-batch loss: 2.525824856758118\n",
      "[epoch: 7, i: 699] avg mini-batch loss: 2.4845354318618775\n",
      "[epoch: 7, i: 799] avg mini-batch loss: 2.476106733083725\n",
      "[epoch: 7, i: 899] avg mini-batch loss: 2.5229168558120727\n",
      "[epoch: 7, i: 999] avg mini-batch loss: 2.503842301368713\n",
      "[epoch: 7, i: 1099] avg mini-batch loss: 2.5722172558307648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 7, i: 1199] avg mini-batch loss: 2.4716108584403993\n",
      "[epoch: 7, i: 1299] avg mini-batch loss: 2.4627025997638703\n",
      "[epoch: 7, i: 1399] avg mini-batch loss: 2.503006423711777\n",
      "[epoch: 7, i: 1499] avg mini-batch loss: 2.4925047826766966\n",
      "[epoch: 7, i: 1599] avg mini-batch loss: 2.4988087260723115\n",
      "[epoch: 7, i: 1699] avg mini-batch loss: 2.5065488505363462\n",
      "[epoch: 7, i: 1799] avg mini-batch loss: 2.477833238840103\n",
      "[epoch: 7, i: 1899] avg mini-batch loss: 2.5123038291931152\n",
      "[epoch: 7, i: 1999] avg mini-batch loss: 2.425411809682846\n",
      "[epoch: 7, i: 2099] avg mini-batch loss: 2.5334390687942503\n",
      "[epoch: 7, i: 2199] avg mini-batch loss: 2.4546426737308504\n",
      "[epoch: 7, i: 2299] avg mini-batch loss: 2.5214697277545928\n",
      "[epoch: 7, i: 2399] avg mini-batch loss: 2.4562521505355837\n",
      "[epoch: 7, i: 2499] avg mini-batch loss: 2.5007671892642973\n",
      "[epoch: 7, i: 2599] avg mini-batch loss: 2.5042924344539643\n",
      "[epoch: 7, i: 2699] avg mini-batch loss: 2.498035391569138\n",
      "[epoch: 7, i: 2799] avg mini-batch loss: 2.55811905503273\n",
      "[epoch: 7, i: 2899] avg mini-batch loss: 2.4472200012207033\n",
      "[epoch: 7, i: 2999] avg mini-batch loss: 2.4954971361160276\n",
      "[epoch: 7, i: 3099] avg mini-batch loss: 2.456008348464966\n",
      "[epoch: 7, i: 3199] avg mini-batch loss: 2.461460874080658\n",
      "[epoch: 7, i: 3299] avg mini-batch loss: 2.4889213502407075\n",
      "[epoch: 7, i: 3399] avg mini-batch loss: 2.4634352910518644\n",
      "[epoch: 7, i: 3499] avg mini-batch loss: 2.467145696878433\n",
      "[epoch: 7, i: 3599] avg mini-batch loss: 2.4771322405338285\n",
      "[epoch: 7, i: 3699] avg mini-batch loss: 2.5084128546714783\n",
      "[epoch: 7, i: 3799] avg mini-batch loss: 2.516129422187805\n",
      "[epoch: 7, i: 3899] avg mini-batch loss: 2.497741276025772\n",
      "[epoch: 7, i: 3999] avg mini-batch loss: 2.462166119813919\n",
      "[epoch: 7, i: 4099] avg mini-batch loss: 2.440492271184921\n",
      "[epoch: 7, i: 4199] avg mini-batch loss: 2.4745177352428436\n",
      "[epoch: 7, i: 4299] avg mini-batch loss: 2.540935961008072\n",
      "[epoch: 7, i: 4399] avg mini-batch loss: 2.483581496477127\n",
      "[epoch: 7, i: 4499] avg mini-batch loss: 2.442991019487381\n",
      "[epoch: 7, i: 4599] avg mini-batch loss: 2.4519774556159972\n",
      "[epoch: 7, i: 4699] avg mini-batch loss: 2.4762642467021942\n",
      "[epoch: 7, i: 4799] avg mini-batch loss: 2.44624853849411\n",
      "[epoch: 7, i: 4899] avg mini-batch loss: 2.4762653517723083\n",
      "[epoch: 7, i: 4999] avg mini-batch loss: 2.4696861350536348\n",
      "[epoch: 7, i: 5099] avg mini-batch loss: 2.4719748377799986\n",
      "[epoch: 7, i: 5199] avg mini-batch loss: 2.4848690021038053\n",
      "[epoch: 7, i: 5299] avg mini-batch loss: 2.46136666059494\n",
      "[epoch: 7, i: 5399] avg mini-batch loss: 2.457567378282547\n",
      "[epoch: 7, i: 5499] avg mini-batch loss: 2.4645281767845155\n",
      "[epoch: 7, i: 5599] avg mini-batch loss: 2.417128790616989\n",
      "[epoch: 7, i: 5699] avg mini-batch loss: 2.401308296918869\n",
      "[epoch: 7, i: 5799] avg mini-batch loss: 2.5006747686862947\n",
      "[epoch: 7, i: 5899] avg mini-batch loss: 2.4883325612545013\n",
      "[epoch: 7, i: 5999] avg mini-batch loss: 2.429236580133438\n",
      "[epoch: 7, i: 6099] avg mini-batch loss: 2.4506930911540987\n",
      "[epoch: 7, i: 6199] avg mini-batch loss: 2.4144937193393705\n",
      "[epoch: 7, i: 6299] avg mini-batch loss: 2.4309037017822264\n",
      "[epoch: 7, i: 6399] avg mini-batch loss: 2.4127230155467987\n",
      "[epoch: 7, i: 6499] avg mini-batch loss: 2.4146433377265932\n",
      "[epoch: 7, i: 6599] avg mini-batch loss: 2.4229476368427276\n",
      "[epoch: 7, i: 6699] avg mini-batch loss: 2.4725354218482973\n",
      "[epoch: 7, i: 6799] avg mini-batch loss: 2.44087859749794\n",
      "[epoch: 7, i: 6899] avg mini-batch loss: 2.468658472299576\n",
      "[epoch: 7, i: 6999] avg mini-batch loss: 2.482100706100464\n",
      "[epoch: 7, i: 7099] avg mini-batch loss: 2.433141862154007\n",
      "[epoch: 7, i: 7199] avg mini-batch loss: 2.415692363977432\n",
      "[epoch: 7, i: 7299] avg mini-batch loss: 2.4249250388145445\n",
      "[epoch: 7, i: 7399] avg mini-batch loss: 2.473548914194107\n",
      "[epoch: 7, i: 7499] avg mini-batch loss: 2.418501728773117\n",
      "[epoch: 7, i: 7599] avg mini-batch loss: 2.4702144157886505\n",
      "[epoch: 7, i: 7699] avg mini-batch loss: 2.392732163667679\n",
      "[epoch: 7, i: 7799] avg mini-batch loss: 2.454338489770889\n",
      "[epoch: 7, i: 7899] avg mini-batch loss: 2.416165270805359\n",
      "[epoch: 7, i: 7999] avg mini-batch loss: 2.42492698431015\n",
      "[epoch: 7, i: 8099] avg mini-batch loss: 2.4426906406879425\n",
      "[epoch: 7, i: 8199] avg mini-batch loss: 2.426768841743469\n",
      "[epoch: 7, i: 8299] avg mini-batch loss: 2.4106382060050966\n",
      "[epoch: 7, i: 8399] avg mini-batch loss: 2.417757170200348\n",
      "[epoch: 7, i: 8499] avg mini-batch loss: 2.388629516363144\n",
      "[epoch: 7, i: 8599] avg mini-batch loss: 2.425143095254898\n",
      "[epoch: 7, i: 8699] avg mini-batch loss: 2.4194580233097076\n",
      "[epoch: 7, i: 8799] avg mini-batch loss: 2.4142921805381774\n",
      "[epoch: 7, i: 8899] avg mini-batch loss: 2.391270182132721\n",
      "[epoch: 7, i: 8999] avg mini-batch loss: 2.4143638288974762\n",
      "[epoch: 7, i: 9099] avg mini-batch loss: 2.428137333393097\n",
      "[epoch: 7, i: 9199] avg mini-batch loss: 2.4634412252902984\n",
      "[epoch: 7, i: 9299] avg mini-batch loss: 2.3865926814079286\n",
      "[epoch: 7, i: 9399] avg mini-batch loss: 2.4077004730701446\n",
      "[epoch: 7, i: 9499] avg mini-batch loss: 2.42304239153862\n",
      "[epoch: 7, i: 9599] avg mini-batch loss: 2.4028680551052095\n",
      "[epoch: 7, i: 9699] avg mini-batch loss: 2.4128768825531006\n",
      "[epoch: 8, i: 99] avg mini-batch loss: 2.4544613099098207\n",
      "[epoch: 8, i: 199] avg mini-batch loss: 2.393813544511795\n",
      "[epoch: 8, i: 299] avg mini-batch loss: 2.3973604035377503\n",
      "[epoch: 8, i: 399] avg mini-batch loss: 2.433790466785431\n",
      "[epoch: 8, i: 499] avg mini-batch loss: 2.4155806696414945\n",
      "[epoch: 8, i: 599] avg mini-batch loss: 2.420709220170975\n",
      "[epoch: 8, i: 699] avg mini-batch loss: 2.345432571172714\n",
      "[epoch: 8, i: 799] avg mini-batch loss: 2.3917808651924135\n",
      "[epoch: 8, i: 899] avg mini-batch loss: 2.4001123237609865\n",
      "[epoch: 8, i: 999] avg mini-batch loss: 2.3685763549804686\n",
      "[epoch: 8, i: 1099] avg mini-batch loss: 2.4487191438674927\n",
      "[epoch: 8, i: 1199] avg mini-batch loss: 2.3463328790664675\n",
      "[epoch: 8, i: 1299] avg mini-batch loss: 2.416190538406372\n",
      "[epoch: 8, i: 1399] avg mini-batch loss: 2.415979609489441\n",
      "[epoch: 8, i: 1499] avg mini-batch loss: 2.4288891422748566\n",
      "[epoch: 8, i: 1599] avg mini-batch loss: 2.4009284460544587\n",
      "[epoch: 8, i: 1699] avg mini-batch loss: 2.385595906972885\n",
      "[epoch: 8, i: 1799] avg mini-batch loss: 2.342707065343857\n",
      "[epoch: 8, i: 1899] avg mini-batch loss: 2.3758010387420656\n",
      "[epoch: 8, i: 1999] avg mini-batch loss: 2.4170621395111085\n",
      "[epoch: 8, i: 2099] avg mini-batch loss: 2.3665247809886933\n",
      "[epoch: 8, i: 2199] avg mini-batch loss: 2.405182181596756\n",
      "[epoch: 8, i: 2299] avg mini-batch loss: 2.4421562480926515\n",
      "[epoch: 8, i: 2399] avg mini-batch loss: 2.4041317987442015\n",
      "[epoch: 8, i: 2499] avg mini-batch loss: 2.406777688264847\n",
      "[epoch: 8, i: 2599] avg mini-batch loss: 2.3334581100940706\n",
      "[epoch: 8, i: 2699] avg mini-batch loss: 2.4161818695068358\n",
      "[epoch: 8, i: 2799] avg mini-batch loss: 2.3869551169872283\n",
      "[epoch: 8, i: 2899] avg mini-batch loss: 2.2956961023807527\n",
      "[epoch: 8, i: 2999] avg mini-batch loss: 2.367399011850357\n",
      "[epoch: 8, i: 3099] avg mini-batch loss: 2.402014353275299\n",
      "[epoch: 8, i: 3199] avg mini-batch loss: 2.405958478450775\n",
      "[epoch: 8, i: 3299] avg mini-batch loss: 2.3403133285045623\n",
      "[epoch: 8, i: 3399] avg mini-batch loss: 2.4267146611213684\n",
      "[epoch: 8, i: 3499] avg mini-batch loss: 2.4011731719970704\n",
      "[epoch: 8, i: 3599] avg mini-batch loss: 2.3779085195064544\n",
      "[epoch: 8, i: 3699] avg mini-batch loss: 2.439615421295166\n",
      "[epoch: 8, i: 3799] avg mini-batch loss: 2.3543650460243226\n",
      "[epoch: 8, i: 3899] avg mini-batch loss: 2.3914494311809538\n",
      "[epoch: 8, i: 3999] avg mini-batch loss: 2.390728352069855\n",
      "[epoch: 8, i: 4099] avg mini-batch loss: 2.3850446820259092\n",
      "[epoch: 8, i: 4199] avg mini-batch loss: 2.414834728240967\n",
      "[epoch: 8, i: 4299] avg mini-batch loss: 2.4193414413928984\n",
      "[epoch: 8, i: 4399] avg mini-batch loss: 2.4155322909355164\n",
      "[epoch: 8, i: 4499] avg mini-batch loss: 2.308810441493988\n",
      "[epoch: 8, i: 4599] avg mini-batch loss: 2.346570748090744\n",
      "[epoch: 8, i: 4699] avg mini-batch loss: 2.329238543510437\n",
      "[epoch: 8, i: 4799] avg mini-batch loss: 2.395128455162048\n",
      "[epoch: 8, i: 4899] avg mini-batch loss: 2.423779846429825\n",
      "[epoch: 8, i: 4999] avg mini-batch loss: 2.376032212972641\n",
      "[epoch: 8, i: 5099] avg mini-batch loss: 2.396568238735199\n",
      "[epoch: 8, i: 5199] avg mini-batch loss: 2.305256109237671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 8, i: 5299] avg mini-batch loss: 2.4319940638542175\n",
      "[epoch: 8, i: 5399] avg mini-batch loss: 2.3878057289123533\n",
      "[epoch: 8, i: 5499] avg mini-batch loss: 2.376921249628067\n",
      "[epoch: 8, i: 5599] avg mini-batch loss: 2.3568664717674257\n",
      "[epoch: 8, i: 5699] avg mini-batch loss: 2.3504626536369324\n",
      "[epoch: 8, i: 5799] avg mini-batch loss: 2.3703112244606017\n",
      "[epoch: 8, i: 5899] avg mini-batch loss: 2.377045508623123\n",
      "[epoch: 8, i: 5999] avg mini-batch loss: 2.3608139789104463\n",
      "[epoch: 8, i: 6099] avg mini-batch loss: 2.355445239543915\n",
      "[epoch: 8, i: 6199] avg mini-batch loss: 2.381480348110199\n",
      "[epoch: 8, i: 6299] avg mini-batch loss: 2.376917806863785\n",
      "[epoch: 8, i: 6399] avg mini-batch loss: 2.294738233089447\n",
      "[epoch: 8, i: 6499] avg mini-batch loss: 2.3485882449150086\n",
      "[epoch: 8, i: 6599] avg mini-batch loss: 2.3172139954566955\n",
      "[epoch: 8, i: 6699] avg mini-batch loss: 2.329444398880005\n",
      "[epoch: 8, i: 6799] avg mini-batch loss: 2.3768014204502106\n",
      "[epoch: 8, i: 6899] avg mini-batch loss: 2.3614051795005797\n",
      "[epoch: 8, i: 6999] avg mini-batch loss: 2.344069097042084\n",
      "[epoch: 8, i: 7099] avg mini-batch loss: 2.286282833814621\n",
      "[epoch: 8, i: 7199] avg mini-batch loss: 2.364965728521347\n",
      "[epoch: 8, i: 7299] avg mini-batch loss: 2.399447308778763\n",
      "[epoch: 8, i: 7399] avg mini-batch loss: 2.3473401260375977\n",
      "[epoch: 8, i: 7499] avg mini-batch loss: 2.3742524862289427\n",
      "[epoch: 8, i: 7599] avg mini-batch loss: 2.366358468532562\n",
      "[epoch: 8, i: 7699] avg mini-batch loss: 2.343772878646851\n",
      "[epoch: 8, i: 7799] avg mini-batch loss: 2.305287781953812\n",
      "[epoch: 8, i: 7899] avg mini-batch loss: 2.3708625626564026\n",
      "[epoch: 8, i: 7999] avg mini-batch loss: 2.335633283853531\n",
      "[epoch: 8, i: 8099] avg mini-batch loss: 2.4178141832351683\n",
      "[epoch: 8, i: 8199] avg mini-batch loss: 2.392705420255661\n",
      "[epoch: 8, i: 8299] avg mini-batch loss: 2.2861649084091185\n",
      "[epoch: 8, i: 8399] avg mini-batch loss: 2.3208584678173065\n",
      "[epoch: 8, i: 8499] avg mini-batch loss: 2.3378721141815184\n",
      "[epoch: 8, i: 8599] avg mini-batch loss: 2.3634807777404787\n",
      "[epoch: 8, i: 8699] avg mini-batch loss: 2.28283115029335\n",
      "[epoch: 8, i: 8799] avg mini-batch loss: 2.3090760850906373\n",
      "[epoch: 8, i: 8899] avg mini-batch loss: 2.394792160987854\n",
      "[epoch: 8, i: 8999] avg mini-batch loss: 2.276935043334961\n",
      "[epoch: 8, i: 9099] avg mini-batch loss: 2.3786405646800994\n",
      "[epoch: 8, i: 9199] avg mini-batch loss: 2.367254959344864\n",
      "[epoch: 8, i: 9299] avg mini-batch loss: 2.3376114678382875\n",
      "[epoch: 8, i: 9399] avg mini-batch loss: 2.323683148622513\n",
      "[epoch: 8, i: 9499] avg mini-batch loss: 2.32534632563591\n",
      "[epoch: 8, i: 9599] avg mini-batch loss: 2.3862010300159455\n",
      "[epoch: 8, i: 9699] avg mini-batch loss: 2.3178725063800814\n",
      "[epoch: 9, i: 99] avg mini-batch loss: 2.270655335187912\n",
      "[epoch: 9, i: 199] avg mini-batch loss: 2.291287214756012\n",
      "[epoch: 9, i: 299] avg mini-batch loss: 2.323178970813751\n",
      "[epoch: 9, i: 399] avg mini-batch loss: 2.339755425453186\n",
      "[epoch: 9, i: 499] avg mini-batch loss: 2.3258835959434507\n",
      "[epoch: 9, i: 599] avg mini-batch loss: 2.298331632614136\n",
      "[epoch: 9, i: 699] avg mini-batch loss: 2.3018078339099883\n",
      "[epoch: 9, i: 799] avg mini-batch loss: 2.290628180503845\n",
      "[epoch: 9, i: 899] avg mini-batch loss: 2.3075491786003113\n",
      "[epoch: 9, i: 999] avg mini-batch loss: 2.3162593281269075\n",
      "[epoch: 9, i: 1099] avg mini-batch loss: 2.291656050682068\n",
      "[epoch: 9, i: 1199] avg mini-batch loss: 2.2595293271541594\n",
      "[epoch: 9, i: 1299] avg mini-batch loss: 2.3247278714179993\n",
      "[epoch: 9, i: 1399] avg mini-batch loss: 2.341780118942261\n",
      "[epoch: 9, i: 1499] avg mini-batch loss: 2.372874892950058\n",
      "[epoch: 9, i: 1599] avg mini-batch loss: 2.3141641318798065\n",
      "[epoch: 9, i: 1699] avg mini-batch loss: 2.264613951444626\n",
      "[epoch: 9, i: 1799] avg mini-batch loss: 2.274059933423996\n",
      "[epoch: 9, i: 1899] avg mini-batch loss: 2.3705822050571443\n",
      "[epoch: 9, i: 1999] avg mini-batch loss: 2.278862007856369\n",
      "[epoch: 9, i: 2099] avg mini-batch loss: 2.2985335147380828\n",
      "[epoch: 9, i: 2199] avg mini-batch loss: 2.34127366065979\n",
      "[epoch: 9, i: 2299] avg mini-batch loss: 2.32425866484642\n",
      "[epoch: 9, i: 2399] avg mini-batch loss: 2.345112690925598\n",
      "[epoch: 9, i: 2499] avg mini-batch loss: 2.2970512878894804\n",
      "[epoch: 9, i: 2599] avg mini-batch loss: 2.2771544635295866\n",
      "[epoch: 9, i: 2699] avg mini-batch loss: 2.3382847249507903\n",
      "[epoch: 9, i: 2799] avg mini-batch loss: 2.288769519329071\n",
      "[epoch: 9, i: 2899] avg mini-batch loss: 2.3273667907714843\n",
      "[epoch: 9, i: 2999] avg mini-batch loss: 2.2747472512722013\n",
      "[epoch: 9, i: 3099] avg mini-batch loss: 2.27902019739151\n",
      "[epoch: 9, i: 3199] avg mini-batch loss: 2.3453695547580717\n",
      "[epoch: 9, i: 3299] avg mini-batch loss: 2.282471978664398\n",
      "[epoch: 9, i: 3399] avg mini-batch loss: 2.2940042769908904\n",
      "[epoch: 9, i: 3499] avg mini-batch loss: 2.237710818052292\n",
      "[epoch: 9, i: 3599] avg mini-batch loss: 2.30813924074173\n",
      "[epoch: 9, i: 3699] avg mini-batch loss: 2.2616143822669983\n",
      "[epoch: 9, i: 3799] avg mini-batch loss: 2.2611533761024476\n",
      "[epoch: 9, i: 3899] avg mini-batch loss: 2.314186395406723\n",
      "[epoch: 9, i: 3999] avg mini-batch loss: 2.3022380197048187\n",
      "[epoch: 9, i: 4099] avg mini-batch loss: 2.3062612891197203\n",
      "[epoch: 9, i: 4199] avg mini-batch loss: 2.2664286029338836\n",
      "[epoch: 9, i: 4299] avg mini-batch loss: 2.300061289072037\n",
      "[epoch: 9, i: 4399] avg mini-batch loss: 2.2895539939403533\n",
      "[epoch: 9, i: 4499] avg mini-batch loss: 2.3042476665973664\n",
      "[epoch: 9, i: 4599] avg mini-batch loss: 2.284495244026184\n",
      "[epoch: 9, i: 4699] avg mini-batch loss: 2.2813006138801573\n",
      "[epoch: 9, i: 4799] avg mini-batch loss: 2.3016340124607084\n",
      "[epoch: 9, i: 4899] avg mini-batch loss: 2.2359051752090453\n",
      "[epoch: 9, i: 4999] avg mini-batch loss: 2.3039967393875123\n",
      "[epoch: 9, i: 5099] avg mini-batch loss: 2.268842761516571\n",
      "[epoch: 9, i: 5199] avg mini-batch loss: 2.2643767189979553\n",
      "[epoch: 9, i: 5299] avg mini-batch loss: 2.3108426666259767\n",
      "[epoch: 9, i: 5399] avg mini-batch loss: 2.328866393566132\n",
      "[epoch: 9, i: 5499] avg mini-batch loss: 2.284069050550461\n",
      "[epoch: 9, i: 5599] avg mini-batch loss: 2.2576973855495455\n",
      "[epoch: 9, i: 5699] avg mini-batch loss: 2.2848785662651063\n",
      "[epoch: 9, i: 5799] avg mini-batch loss: 2.26395943403244\n",
      "[epoch: 9, i: 5899] avg mini-batch loss: 2.286024593114853\n",
      "[epoch: 9, i: 5999] avg mini-batch loss: 2.2773086392879485\n",
      "[epoch: 9, i: 6099] avg mini-batch loss: 2.349652384519577\n",
      "[epoch: 9, i: 6199] avg mini-batch loss: 2.2598713958263397\n",
      "[epoch: 9, i: 6299] avg mini-batch loss: 2.240124577283859\n",
      "[epoch: 9, i: 6399] avg mini-batch loss: 2.2513293063640596\n",
      "[epoch: 9, i: 6499] avg mini-batch loss: 2.2760196685791017\n",
      "[epoch: 9, i: 6599] avg mini-batch loss: 2.3235080885887145\n",
      "[epoch: 9, i: 6699] avg mini-batch loss: 2.2543618655204773\n",
      "[epoch: 9, i: 6799] avg mini-batch loss: 2.2705346858501434\n",
      "[epoch: 9, i: 6899] avg mini-batch loss: 2.281969417333603\n",
      "[epoch: 9, i: 6999] avg mini-batch loss: 2.256665868759155\n",
      "[epoch: 9, i: 7099] avg mini-batch loss: 2.271212079524994\n",
      "[epoch: 9, i: 7199] avg mini-batch loss: 2.2516209292411804\n",
      "[epoch: 9, i: 7299] avg mini-batch loss: 2.271413289308548\n",
      "[epoch: 9, i: 7399] avg mini-batch loss: 2.285706938505173\n",
      "[epoch: 9, i: 7499] avg mini-batch loss: 2.2199046099185944\n",
      "[epoch: 9, i: 7599] avg mini-batch loss: 2.2426412439346315\n",
      "[epoch: 9, i: 7699] avg mini-batch loss: 2.301365696191788\n",
      "[epoch: 9, i: 7799] avg mini-batch loss: 2.250326896905899\n",
      "[epoch: 9, i: 7899] avg mini-batch loss: 2.3135409259796145\n",
      "[epoch: 9, i: 7999] avg mini-batch loss: 2.277448208332062\n",
      "[epoch: 9, i: 8099] avg mini-batch loss: 2.236302284002304\n",
      "[epoch: 9, i: 8199] avg mini-batch loss: 2.289284693002701\n",
      "[epoch: 9, i: 8299] avg mini-batch loss: 2.253476779460907\n",
      "[epoch: 9, i: 8399] avg mini-batch loss: 2.260628904104233\n",
      "[epoch: 9, i: 8499] avg mini-batch loss: 2.2638287341594694\n",
      "[epoch: 9, i: 8599] avg mini-batch loss: 2.2876677942276\n",
      "[epoch: 9, i: 8699] avg mini-batch loss: 2.218228347301483\n",
      "[epoch: 9, i: 8799] avg mini-batch loss: 2.277394758462906\n",
      "[epoch: 9, i: 8899] avg mini-batch loss: 2.2496114802360534\n",
      "[epoch: 9, i: 8999] avg mini-batch loss: 2.2560677480697633\n",
      "[epoch: 9, i: 9099] avg mini-batch loss: 2.2064817607402802\n",
      "[epoch: 9, i: 9199] avg mini-batch loss: 2.254440804719925\n",
      "[epoch: 9, i: 9299] avg mini-batch loss: 2.2289075231552125\n",
      "[epoch: 9, i: 9399] avg mini-batch loss: 2.253821587562561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 9, i: 9499] avg mini-batch loss: 2.213537555932999\n",
      "[epoch: 9, i: 9599] avg mini-batch loss: 2.216568672657013\n",
      "[epoch: 9, i: 9699] avg mini-batch loss: 2.2326023375988004\n",
      "Finished Training.\n"
     ]
    }
   ],
   "source": [
    "# If there are GPUs, choose the first one for computing. Otherwise use CPU.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# If 'cuda:0' is printed, it means GPU is available.\n",
    "\n",
    "# model = models.resnet50(pretrained=False)\n",
    "# state_dict=torch.load(\"../src/models/resnet50-19c8e357.pth\")\n",
    "# model.load_state_dict(state_dict)\n",
    "# model.fc = nn.Linear(2048, 80)\n",
    "\n",
    "model_mlp = nn.Sequential(nn.Sequential(nn.Linear(2048,32),nn.ReLU(),nn.Dropout(p=0.3)),nn.Linear(32,80))\n",
    "model_mlp.to(device)\n",
    "model.to(device)\n",
    "\n",
    "print(f'model parameters sent to {device}')\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss(reduction='mean')\n",
    "opt = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\n",
    "\n",
    "avg_losses = [] # Average losses\n",
    "epochs = num_epoch\n",
    "print_freq = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # print(data)\n",
    "        # Get the inputs.\n",
    "        inputs, _, labels, _ = data\n",
    "        \n",
    "        \n",
    "        \n",
    "#       try:\n",
    "        # Move the inputs to the specified device\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # First get 2048-D embeddings of our input through pre-trained resnet\n",
    "        inputs = model(inputs)\n",
    "        \n",
    "        # Zero the parameter gradients.\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # Forward step.\n",
    "        outputs = model_mlp(inputs) # applying only our mlp to the model\n",
    "        loss = loss_func(outputs, labels)\n",
    "\n",
    "        # Backward step.\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimization step (update the parameters).\n",
    "        opt.step()\n",
    "\n",
    "        # Print statistics.\n",
    "        running_loss += loss.item()\n",
    "#         except RuntimeError:\n",
    "#             print(f'Input shape: {inputs.shape}\\n Label shape: {labels.shape}')\n",
    "#             print(f' Labels: {labels}')\n",
    "#         print(i)\n",
    "        if i % print_freq == print_freq - 1: # prints every several mini-batches.\n",
    "            avg_loss = running_loss / print_freq\n",
    "            print(f'[epoch: {epoch}, i: {i}] avg mini-batch loss: {avg_loss}')\n",
    "            avg_losses.append(avg_loss)\n",
    "            running_loss = 0.0\n",
    "        \n",
    "        del inputs, outputs, labels, loss\n",
    "print('Finished Training.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! ls /datasets/COCO-2017/val2017/ | grep 000000311913"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "970"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plot\n",
    "len(avg_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '/datasets/COCO-2017/val2017'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'resnet50_80_10'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fname = '80_10_resnet50'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = f'../src/models/{fname}.pth'\n",
    "torch.save(model_mlp.state_dict(), PATH)\n",
    "print(f'Model saved at {PATH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at ../src/models/resnet50_80_10.pth\n"
     ]
    }
   ],
   "source": [
    "\n",
    "PATH = f'../src/models/{fname}.pth'\n",
    "torch.save(model.state_dict(), PATH)\n",
    "print(f'Model saved at {PATH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "### After model in cell above is saved, use model parameters w/ fc layer replaced to train GNN again and compare results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(avg_losses).to_csv(f'../data/out/avg_losses_{fname}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=80, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 128, 128]) torch.Size([1, 2]) torch.Size([1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-782bebbe4795>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0medge_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mknn_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mgraphs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m#     print(graphs[i].x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;31m# See note [TorchScript super()]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    414\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    415\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 416\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import Data\n",
    "from torch_cluster import knn_graph\n",
    "graphs=[]\n",
    "for i,(imgs,xy,y) in enumerate(valloader):\n",
    "    print(imgs.shape,xy.shape,y.shape)\n",
    "    edge_index = knn_graph(xy,k=5)\n",
    "    graphs.append(Data(x=model(imgs),pos=xy,edge_index=edge_index,y=y))\n",
    "#     print(graphs[i].x.shape)\n",
    "    if i==10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import DataLoader as TG_DataLoader\n",
    "# https://pytorch.org/docs/stable/data.html#torch.utils.data.Subset\n",
    "\n",
    "graph_dataloader=TG_DataLoader(graphs,batch_size=2,shuffle=True)#[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'F' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-b04ad55f4308>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0medge_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_gcn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/datasets/home/41/641/jdlevy/.conda/envs/torchgeo/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-8b6d3d128f90>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, edge_attr)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mconv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_each\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_edge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_attr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'F' is not defined"
     ]
    }
   ],
   "source": [
    "for G in graph_dataloader:\n",
    "    y=G.y\n",
    "    z=G.x\n",
    "    edge_index=G.edge_index\n",
    "    y_pred=model_gcn(z,edge_index)\n",
    "    print(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-6f409796ed3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# DataLoader is iterable over Dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotations\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mannotations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mannotations\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# select device (whether GPU or CPU)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# DataLoader is iterable over Dataset\n",
    "for imgs, annotations in valloader:\n",
    "    imgs = list(img.to(device) for img in imgs)\n",
    "    annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
    "    print(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset CocoDetection\n",
       "    Number of datapoints: 118287\n",
       "    Root location: /datasets/COCO-2017/train2017\n",
       "    StandardTransform\n",
       "Transform: <function crop_by_bbox at 0x7fe92a5d53b0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset CocoDetection\n",
       "    Number of datapoints: 5000\n",
       "    Root location: /datasets/COCO-2017/val2017\n",
       "    StandardTransform\n",
       "Transform: <function crop_by_bbox at 0x7fe92a5d53b0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gcn=GCNNet(2048,80, [32]*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict=torch.load(\"../src/models/graph_80_5_resnet50.pth\")\n",
    "model_gcn.load_state_dict(state_dict)\n",
    "# model.fc = nn.Linear(2048, 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchgeo",
   "language": "python",
   "name": "torchgeo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
